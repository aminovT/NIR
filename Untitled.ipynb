{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score,classification_report,f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as utils\n",
    "from torch.optim import Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('l1', nn.Linear(784, 200))\n",
    "model.add_module('r1' , nn.ReLU())\n",
    "model.add_module('l2', nn.Linear(200, 200))\n",
    "model.add_module('r2' , nn.ReLU())\n",
    "model.add_module('l3', nn.Linear(200, 10))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(model.l2.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_200 = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(200), torch.eye(200))\n",
    "m_784 = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(784), torch.eye(784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_pr_loss(a , m ):\n",
    "    s = 0 \n",
    "    c = 0\n",
    "    for k in a:\n",
    "\n",
    "        s -= m.log_prob(k)\n",
    "        c += 1\n",
    "    return s/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_pr(a , var):\n",
    "    torch.sum(a**2) / variance**2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(183.9560, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_pr_loss(list(model.l2.weight) , m )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [396/60000 (1%)]\tLoss: 1090.502686 \n",
      "Train Epoch: 1 [796/60000 (1%)]\tLoss: 1088.872192 \n",
      "Train Epoch: 1 [1196/60000 (2%)]\tLoss: 1088.660400 \n",
      "Train Epoch: 1 [1596/60000 (3%)]\tLoss: 1089.216919 \n",
      "Train Epoch: 1 [1996/60000 (3%)]\tLoss: 1089.406128 \n",
      "Train Epoch: 1 [2396/60000 (4%)]\tLoss: 1088.428589 \n",
      "Train Epoch: 1 [2796/60000 (5%)]\tLoss: 1089.685303 \n",
      "Train Epoch: 1 [3196/60000 (5%)]\tLoss: 1088.836548 \n",
      "Train Epoch: 1 [3596/60000 (6%)]\tLoss: 1088.368652 \n",
      "Train Epoch: 1 [3996/60000 (7%)]\tLoss: 1088.723755 \n",
      "Train Epoch: 1 [4396/60000 (7%)]\tLoss: 1088.657227 \n",
      "Train Epoch: 1 [4796/60000 (8%)]\tLoss: 1089.640137 \n",
      "Train Epoch: 1 [5196/60000 (9%)]\tLoss: 1088.463745 \n",
      "Train Epoch: 1 [5596/60000 (9%)]\tLoss: 1088.347168 \n",
      "Train Epoch: 1 [5996/60000 (10%)]\tLoss: 1089.607544 \n",
      "Train Epoch: 1 [6396/60000 (11%)]\tLoss: 1088.440918 \n",
      "Train Epoch: 1 [6796/60000 (11%)]\tLoss: 1088.545410 \n",
      "Train Epoch: 1 [7196/60000 (12%)]\tLoss: 1088.775879 \n",
      "Train Epoch: 1 [7596/60000 (13%)]\tLoss: 1088.563843 \n",
      "Train Epoch: 1 [7996/60000 (13%)]\tLoss: 1088.793091 \n",
      "Train Epoch: 1 [8396/60000 (14%)]\tLoss: 1088.391724 \n",
      "Train Epoch: 1 [8796/60000 (15%)]\tLoss: 1088.690552 \n",
      "Train Epoch: 1 [9196/60000 (15%)]\tLoss: 1089.129150 \n",
      "Train Epoch: 1 [9596/60000 (16%)]\tLoss: 1088.530518 \n",
      "Train Epoch: 1 [9996/60000 (17%)]\tLoss: 1088.652710 \n",
      "Train Epoch: 1 [10396/60000 (17%)]\tLoss: 1088.574341 \n",
      "Train Epoch: 1 [10796/60000 (18%)]\tLoss: 1089.313721 \n",
      "Train Epoch: 1 [11196/60000 (19%)]\tLoss: 1088.732666 \n",
      "Train Epoch: 1 [11596/60000 (19%)]\tLoss: 1088.360229 \n",
      "Train Epoch: 1 [11996/60000 (20%)]\tLoss: 1088.407593 \n",
      "Train Epoch: 1 [12396/60000 (21%)]\tLoss: 1089.304565 \n",
      "Train Epoch: 1 [12796/60000 (21%)]\tLoss: 1089.016479 \n",
      "Train Epoch: 1 [13196/60000 (22%)]\tLoss: 1088.685303 \n",
      "Train Epoch: 1 [13596/60000 (23%)]\tLoss: 1088.674438 \n",
      "Train Epoch: 1 [13996/60000 (23%)]\tLoss: 1088.505737 \n",
      "Train Epoch: 1 [14396/60000 (24%)]\tLoss: 1088.427002 \n",
      "Train Epoch: 1 [14796/60000 (25%)]\tLoss: 1088.846436 \n",
      "Train Epoch: 1 [15196/60000 (25%)]\tLoss: 1089.265259 \n",
      "Train Epoch: 1 [15596/60000 (26%)]\tLoss: 1088.849487 \n",
      "Train Epoch: 1 [15996/60000 (27%)]\tLoss: 1089.201050 \n",
      "Train Epoch: 1 [16396/60000 (27%)]\tLoss: 1088.833496 \n",
      "Train Epoch: 1 [16796/60000 (28%)]\tLoss: 1089.015503 \n",
      "Train Epoch: 1 [17196/60000 (29%)]\tLoss: 1088.398682 \n",
      "Train Epoch: 1 [17596/60000 (29%)]\tLoss: 1088.526001 \n",
      "Train Epoch: 1 [17996/60000 (30%)]\tLoss: 1088.780273 \n",
      "Train Epoch: 1 [18396/60000 (31%)]\tLoss: 1088.935181 \n",
      "Train Epoch: 1 [18796/60000 (31%)]\tLoss: 1089.014160 \n",
      "Train Epoch: 1 [19196/60000 (32%)]\tLoss: 1088.460449 \n",
      "Train Epoch: 1 [19596/60000 (33%)]\tLoss: 1088.703247 \n",
      "Train Epoch: 1 [19996/60000 (33%)]\tLoss: 1089.009766 \n",
      "Train Epoch: 1 [20396/60000 (34%)]\tLoss: 1088.514282 \n",
      "Train Epoch: 1 [20796/60000 (35%)]\tLoss: 1088.433105 \n",
      "Train Epoch: 1 [21196/60000 (35%)]\tLoss: 1088.346191 \n",
      "Train Epoch: 1 [21596/60000 (36%)]\tLoss: 1088.809448 \n",
      "Train Epoch: 1 [21996/60000 (37%)]\tLoss: 1088.402100 \n",
      "Train Epoch: 1 [22396/60000 (37%)]\tLoss: 1088.542358 \n",
      "Train Epoch: 1 [22796/60000 (38%)]\tLoss: 1088.426147 \n",
      "Train Epoch: 1 [23196/60000 (39%)]\tLoss: 1088.519531 \n",
      "Train Epoch: 1 [23596/60000 (39%)]\tLoss: 1088.836792 \n",
      "Train Epoch: 1 [23996/60000 (40%)]\tLoss: 1088.848511 \n",
      "Train Epoch: 1 [24396/60000 (41%)]\tLoss: 1088.342041 \n",
      "Train Epoch: 1 [24796/60000 (41%)]\tLoss: 1088.644653 \n",
      "Train Epoch: 1 [25196/60000 (42%)]\tLoss: 1090.252441 \n",
      "Train Epoch: 1 [25596/60000 (43%)]\tLoss: 1088.740967 \n",
      "Train Epoch: 1 [25996/60000 (43%)]\tLoss: 1088.454468 \n",
      "Train Epoch: 1 [26396/60000 (44%)]\tLoss: 1088.502930 \n",
      "Train Epoch: 1 [26796/60000 (45%)]\tLoss: 1088.999756 \n",
      "Train Epoch: 1 [27196/60000 (45%)]\tLoss: 1088.417236 \n",
      "Train Epoch: 1 [27596/60000 (46%)]\tLoss: 1088.365723 \n",
      "Train Epoch: 1 [27996/60000 (47%)]\tLoss: 1088.418091 \n",
      "Train Epoch: 1 [28396/60000 (47%)]\tLoss: 1088.724854 \n",
      "Train Epoch: 1 [28796/60000 (48%)]\tLoss: 1088.971436 \n",
      "Train Epoch: 1 [29196/60000 (49%)]\tLoss: 1088.388062 \n",
      "Train Epoch: 1 [29596/60000 (49%)]\tLoss: 1088.426758 \n",
      "Train Epoch: 1 [29996/60000 (50%)]\tLoss: 1089.034912 \n",
      "Train Epoch: 1 [30396/60000 (51%)]\tLoss: 1088.580688 \n",
      "Train Epoch: 1 [30796/60000 (51%)]\tLoss: 1088.389282 \n",
      "Train Epoch: 1 [31196/60000 (52%)]\tLoss: 1088.661743 \n",
      "Train Epoch: 1 [31596/60000 (53%)]\tLoss: 1090.243530 \n",
      "Train Epoch: 1 [31996/60000 (53%)]\tLoss: 1089.175537 \n",
      "Train Epoch: 1 [32396/60000 (54%)]\tLoss: 1088.345337 \n",
      "Train Epoch: 1 [32796/60000 (55%)]\tLoss: 1088.464844 \n",
      "Train Epoch: 1 [33196/60000 (55%)]\tLoss: 1088.540649 \n",
      "Train Epoch: 1 [33596/60000 (56%)]\tLoss: 1089.113403 \n",
      "Train Epoch: 1 [33996/60000 (57%)]\tLoss: 1088.356201 \n",
      "Train Epoch: 1 [34396/60000 (57%)]\tLoss: 1088.349365 \n",
      "Train Epoch: 1 [34796/60000 (58%)]\tLoss: 1089.647339 \n",
      "Train Epoch: 1 [35196/60000 (59%)]\tLoss: 1088.456909 \n",
      "Train Epoch: 1 [35596/60000 (59%)]\tLoss: 1088.438477 \n",
      "Train Epoch: 1 [35996/60000 (60%)]\tLoss: 1088.523560 \n",
      "Train Epoch: 1 [36396/60000 (61%)]\tLoss: 1088.567749 \n",
      "Train Epoch: 1 [36796/60000 (61%)]\tLoss: 1088.621582 \n",
      "Train Epoch: 1 [37196/60000 (62%)]\tLoss: 1088.409668 \n",
      "Train Epoch: 1 [37596/60000 (63%)]\tLoss: 1089.363770 \n",
      "Train Epoch: 1 [37996/60000 (63%)]\tLoss: 1088.531250 \n",
      "Train Epoch: 1 [38396/60000 (64%)]\tLoss: 1088.392212 \n",
      "Train Epoch: 1 [38796/60000 (65%)]\tLoss: 1088.386108 \n",
      "Train Epoch: 1 [39196/60000 (65%)]\tLoss: 1088.599243 \n",
      "Train Epoch: 1 [39596/60000 (66%)]\tLoss: 1088.350708 \n",
      "Train Epoch: 1 [39996/60000 (67%)]\tLoss: 1088.343994 \n",
      "Train Epoch: 1 [40396/60000 (67%)]\tLoss: 1088.353149 \n",
      "Train Epoch: 1 [40796/60000 (68%)]\tLoss: 1088.437988 \n",
      "Train Epoch: 1 [41196/60000 (69%)]\tLoss: 1088.341553 \n",
      "Train Epoch: 1 [41596/60000 (69%)]\tLoss: 1088.386597 \n",
      "Train Epoch: 1 [41996/60000 (70%)]\tLoss: 1088.471313 \n",
      "Train Epoch: 1 [42396/60000 (71%)]\tLoss: 1088.687378 \n",
      "Train Epoch: 1 [42796/60000 (71%)]\tLoss: 1088.558105 \n",
      "Train Epoch: 1 [43196/60000 (72%)]\tLoss: 1088.378418 \n",
      "Train Epoch: 1 [43596/60000 (73%)]\tLoss: 1088.369873 \n",
      "Train Epoch: 1 [43996/60000 (73%)]\tLoss: 1088.336548 \n",
      "Train Epoch: 1 [44396/60000 (74%)]\tLoss: 1088.514893 \n",
      "Train Epoch: 1 [44796/60000 (75%)]\tLoss: 1089.495483 \n",
      "Train Epoch: 1 [45196/60000 (75%)]\tLoss: 1088.697510 \n",
      "Train Epoch: 1 [45596/60000 (76%)]\tLoss: 1088.520020 \n",
      "Train Epoch: 1 [45996/60000 (77%)]\tLoss: 1089.278442 \n",
      "Train Epoch: 1 [46396/60000 (77%)]\tLoss: 1088.432617 \n",
      "Train Epoch: 1 [46796/60000 (78%)]\tLoss: 1088.839478 \n",
      "Train Epoch: 1 [47196/60000 (79%)]\tLoss: 1088.375732 \n",
      "Train Epoch: 1 [47596/60000 (79%)]\tLoss: 1088.350586 \n",
      "Train Epoch: 1 [47996/60000 (80%)]\tLoss: 1088.423218 \n",
      "Train Epoch: 1 [48396/60000 (81%)]\tLoss: 1088.982422 \n",
      "Train Epoch: 1 [48796/60000 (81%)]\tLoss: 1088.333252 \n",
      "Train Epoch: 1 [49196/60000 (82%)]\tLoss: 1088.607056 \n",
      "Train Epoch: 1 [49596/60000 (83%)]\tLoss: 1088.322998 \n",
      "Train Epoch: 1 [49996/60000 (83%)]\tLoss: 1088.432129 \n",
      "Train Epoch: 1 [50396/60000 (84%)]\tLoss: 1088.630615 \n",
      "Train Epoch: 1 [50796/60000 (85%)]\tLoss: 1088.838867 \n",
      "Train Epoch: 1 [51196/60000 (85%)]\tLoss: 1088.957153 \n",
      "Train Epoch: 1 [51596/60000 (86%)]\tLoss: 1089.407959 \n",
      "Train Epoch: 1 [51996/60000 (87%)]\tLoss: 1088.627808 \n",
      "Train Epoch: 1 [52396/60000 (87%)]\tLoss: 1088.328369 \n",
      "Train Epoch: 1 [52796/60000 (88%)]\tLoss: 1088.499756 \n",
      "Train Epoch: 1 [53196/60000 (89%)]\tLoss: 1088.479980 \n",
      "Train Epoch: 1 [53596/60000 (89%)]\tLoss: 1088.819580 \n",
      "Train Epoch: 1 [53996/60000 (90%)]\tLoss: 1089.517212 \n",
      "Train Epoch: 1 [54396/60000 (91%)]\tLoss: 1088.731812 \n",
      "Train Epoch: 1 [54796/60000 (91%)]\tLoss: 1088.414673 \n",
      "Train Epoch: 1 [55196/60000 (92%)]\tLoss: 1088.373779 \n",
      "Train Epoch: 1 [55596/60000 (93%)]\tLoss: 1088.364868 \n",
      "Train Epoch: 1 [55996/60000 (93%)]\tLoss: 1088.850342 \n",
      "Train Epoch: 1 [56396/60000 (94%)]\tLoss: 1088.377808 \n",
      "Train Epoch: 1 [56796/60000 (95%)]\tLoss: 1088.413818 \n",
      "Train Epoch: 1 [57196/60000 (95%)]\tLoss: 1088.623779 \n",
      "Train Epoch: 1 [57596/60000 (96%)]\tLoss: 1088.645386 \n",
      "Train Epoch: 1 [57996/60000 (97%)]\tLoss: 1088.819092 \n",
      "Train Epoch: 1 [58396/60000 (97%)]\tLoss: 1088.595459 \n",
      "Train Epoch: 1 [58796/60000 (98%)]\tLoss: 1088.948120 \n",
      "Train Epoch: 1 [59196/60000 (99%)]\tLoss: 1089.310913 \n",
      "Train Epoch: 1 [59596/60000 (99%)]\tLoss: 1088.531128 \n",
      "Train Epoch: 1 [59996/60000 (100%)]\tLoss: 1088.504395 \n",
      "Train Epoch: 2 [396/60000 (1%)]\tLoss: 1089.116699 \n",
      "Train Epoch: 2 [796/60000 (1%)]\tLoss: 1088.458984 \n",
      "Train Epoch: 2 [1196/60000 (2%)]\tLoss: 1088.930176 \n",
      "Train Epoch: 2 [1596/60000 (3%)]\tLoss: 1088.332642 \n",
      "Train Epoch: 2 [1996/60000 (3%)]\tLoss: 1088.346313 \n",
      "Train Epoch: 2 [2396/60000 (4%)]\tLoss: 1088.338867 \n",
      "Train Epoch: 2 [2796/60000 (5%)]\tLoss: 1088.507080 \n",
      "Train Epoch: 2 [3196/60000 (5%)]\tLoss: 1088.327271 \n",
      "Train Epoch: 2 [3596/60000 (6%)]\tLoss: 1088.468994 \n",
      "Train Epoch: 2 [3996/60000 (7%)]\tLoss: 1088.340332 \n",
      "Train Epoch: 2 [4396/60000 (7%)]\tLoss: 1088.415771 \n",
      "Train Epoch: 2 [4796/60000 (8%)]\tLoss: 1088.323853 \n",
      "Train Epoch: 2 [5196/60000 (9%)]\tLoss: 1088.566528 \n",
      "Train Epoch: 2 [5596/60000 (9%)]\tLoss: 1088.986084 \n",
      "Train Epoch: 2 [5996/60000 (10%)]\tLoss: 1088.750244 \n",
      "Train Epoch: 2 [6396/60000 (11%)]\tLoss: 1088.389160 \n",
      "Train Epoch: 2 [6796/60000 (11%)]\tLoss: 1088.467041 \n",
      "Train Epoch: 2 [7196/60000 (12%)]\tLoss: 1088.624512 \n",
      "Train Epoch: 2 [7596/60000 (13%)]\tLoss: 1088.347534 \n",
      "Train Epoch: 2 [7996/60000 (13%)]\tLoss: 1088.706055 \n",
      "Train Epoch: 2 [8396/60000 (14%)]\tLoss: 1088.347778 \n",
      "Train Epoch: 2 [8796/60000 (15%)]\tLoss: 1088.472778 \n",
      "Train Epoch: 2 [9196/60000 (15%)]\tLoss: 1088.552124 \n",
      "Train Epoch: 2 [9596/60000 (16%)]\tLoss: 1089.314575 \n",
      "Train Epoch: 2 [9996/60000 (17%)]\tLoss: 1089.448486 \n",
      "Train Epoch: 2 [10396/60000 (17%)]\tLoss: 1088.360596 \n",
      "Train Epoch: 2 [10796/60000 (18%)]\tLoss: 1088.316650 \n",
      "Train Epoch: 2 [11196/60000 (19%)]\tLoss: 1088.412231 \n",
      "Train Epoch: 2 [11596/60000 (19%)]\tLoss: 1088.324951 \n",
      "Train Epoch: 2 [11996/60000 (20%)]\tLoss: 1088.326172 \n",
      "Train Epoch: 2 [12396/60000 (21%)]\tLoss: 1088.388062 \n",
      "Train Epoch: 2 [12796/60000 (21%)]\tLoss: 1088.793213 \n",
      "Train Epoch: 2 [13196/60000 (22%)]\tLoss: 1088.699463 \n",
      "Train Epoch: 2 [13596/60000 (23%)]\tLoss: 1088.346313 \n",
      "Train Epoch: 2 [13996/60000 (23%)]\tLoss: 1088.363159 \n",
      "Train Epoch: 2 [14396/60000 (24%)]\tLoss: 1088.525757 \n",
      "Train Epoch: 2 [14796/60000 (25%)]\tLoss: 1088.365356 \n",
      "Train Epoch: 2 [15196/60000 (25%)]\tLoss: 1088.486694 \n",
      "Train Epoch: 2 [15596/60000 (26%)]\tLoss: 1088.470459 \n",
      "Train Epoch: 2 [15996/60000 (27%)]\tLoss: 1088.408691 \n",
      "Train Epoch: 2 [16396/60000 (27%)]\tLoss: 1088.385010 \n",
      "Train Epoch: 2 [16796/60000 (28%)]\tLoss: 1088.398682 \n",
      "Train Epoch: 2 [17196/60000 (29%)]\tLoss: 1089.223145 \n",
      "Train Epoch: 2 [17596/60000 (29%)]\tLoss: 1088.869873 \n",
      "Train Epoch: 2 [17996/60000 (30%)]\tLoss: 1088.884033 \n",
      "Train Epoch: 2 [18396/60000 (31%)]\tLoss: 1088.481689 \n",
      "Train Epoch: 2 [18796/60000 (31%)]\tLoss: 1088.324097 \n",
      "Train Epoch: 2 [19196/60000 (32%)]\tLoss: 1088.507446 \n",
      "Train Epoch: 2 [19596/60000 (33%)]\tLoss: 1088.794312 \n",
      "Train Epoch: 2 [19996/60000 (33%)]\tLoss: 1088.335449 \n",
      "Train Epoch: 2 [20396/60000 (34%)]\tLoss: 1088.375488 \n",
      "Train Epoch: 2 [20796/60000 (35%)]\tLoss: 1088.892334 \n",
      "Train Epoch: 2 [21196/60000 (35%)]\tLoss: 1088.430664 \n",
      "Train Epoch: 2 [21596/60000 (36%)]\tLoss: 1088.371948 \n",
      "Train Epoch: 2 [21996/60000 (37%)]\tLoss: 1088.383301 \n",
      "Train Epoch: 2 [22396/60000 (37%)]\tLoss: 1088.375366 \n",
      "Train Epoch: 2 [22796/60000 (38%)]\tLoss: 1088.349854 \n",
      "Train Epoch: 2 [23196/60000 (39%)]\tLoss: 1088.343506 \n",
      "Train Epoch: 2 [23596/60000 (39%)]\tLoss: 1088.563354 \n",
      "Train Epoch: 2 [23996/60000 (40%)]\tLoss: 1088.308472 \n",
      "Train Epoch: 2 [24396/60000 (41%)]\tLoss: 1088.329956 \n",
      "Train Epoch: 2 [24796/60000 (41%)]\tLoss: 1088.579346 \n",
      "Train Epoch: 2 [25196/60000 (42%)]\tLoss: 1088.316162 \n",
      "Train Epoch: 2 [25596/60000 (43%)]\tLoss: 1088.565674 \n",
      "Train Epoch: 2 [25996/60000 (43%)]\tLoss: 1088.511963 \n",
      "Train Epoch: 2 [26396/60000 (44%)]\tLoss: 1088.510864 \n",
      "Train Epoch: 2 [26796/60000 (45%)]\tLoss: 1088.327393 \n",
      "Train Epoch: 2 [27196/60000 (45%)]\tLoss: 1088.391724 \n",
      "Train Epoch: 2 [27596/60000 (46%)]\tLoss: 1089.448975 \n",
      "Train Epoch: 2 [27996/60000 (47%)]\tLoss: 1088.313843 \n",
      "Train Epoch: 2 [28396/60000 (47%)]\tLoss: 1088.317017 \n",
      "Train Epoch: 2 [28796/60000 (48%)]\tLoss: 1088.304199 \n",
      "Train Epoch: 2 [29196/60000 (49%)]\tLoss: 1088.958618 \n",
      "Train Epoch: 2 [29596/60000 (49%)]\tLoss: 1088.525391 \n",
      "Train Epoch: 2 [29996/60000 (50%)]\tLoss: 1088.322876 \n",
      "Train Epoch: 2 [30396/60000 (51%)]\tLoss: 1088.631714 \n",
      "Train Epoch: 2 [30796/60000 (51%)]\tLoss: 1088.381348 \n",
      "Train Epoch: 2 [31196/60000 (52%)]\tLoss: 1089.055542 \n",
      "Train Epoch: 2 [31596/60000 (53%)]\tLoss: 1088.358643 \n",
      "Train Epoch: 2 [31996/60000 (53%)]\tLoss: 1089.086182 \n",
      "Train Epoch: 2 [32396/60000 (54%)]\tLoss: 1088.687256 \n",
      "Train Epoch: 2 [32796/60000 (55%)]\tLoss: 1088.346191 \n",
      "Train Epoch: 2 [33196/60000 (55%)]\tLoss: 1088.416138 \n",
      "Train Epoch: 2 [33596/60000 (56%)]\tLoss: 1088.479736 \n",
      "Train Epoch: 2 [33996/60000 (57%)]\tLoss: 1088.398315 \n",
      "Train Epoch: 2 [34396/60000 (57%)]\tLoss: 1088.293701 \n",
      "Train Epoch: 2 [34796/60000 (58%)]\tLoss: 1088.542969 \n",
      "Train Epoch: 2 [35196/60000 (59%)]\tLoss: 1088.300781 \n",
      "Train Epoch: 2 [35596/60000 (59%)]\tLoss: 1088.459717 \n",
      "Train Epoch: 2 [35996/60000 (60%)]\tLoss: 1088.947266 \n",
      "Train Epoch: 2 [36396/60000 (61%)]\tLoss: 1088.805908 \n",
      "Train Epoch: 2 [36796/60000 (61%)]\tLoss: 1088.447021 \n",
      "Train Epoch: 2 [37196/60000 (62%)]\tLoss: 1088.424561 \n",
      "Train Epoch: 2 [37596/60000 (63%)]\tLoss: 1088.329346 \n",
      "Train Epoch: 2 [37996/60000 (63%)]\tLoss: 1088.406982 \n",
      "Train Epoch: 2 [38396/60000 (64%)]\tLoss: 1088.373047 \n",
      "Train Epoch: 2 [38796/60000 (65%)]\tLoss: 1088.443115 \n",
      "Train Epoch: 2 [39196/60000 (65%)]\tLoss: 1088.946655 \n",
      "Train Epoch: 2 [39596/60000 (66%)]\tLoss: 1088.286499 \n",
      "Train Epoch: 2 [39996/60000 (67%)]\tLoss: 1089.638428 \n",
      "Train Epoch: 2 [40396/60000 (67%)]\tLoss: 1088.451172 \n",
      "Train Epoch: 2 [40796/60000 (68%)]\tLoss: 1088.293335 \n",
      "Train Epoch: 2 [41196/60000 (69%)]\tLoss: 1088.317749 \n",
      "Train Epoch: 2 [41596/60000 (69%)]\tLoss: 1088.300293 \n",
      "Train Epoch: 2 [41996/60000 (70%)]\tLoss: 1088.312012 \n",
      "Train Epoch: 2 [42396/60000 (71%)]\tLoss: 1088.691528 \n",
      "Train Epoch: 2 [42796/60000 (71%)]\tLoss: 1088.457520 \n",
      "Train Epoch: 2 [43196/60000 (72%)]\tLoss: 1088.548706 \n",
      "Train Epoch: 2 [43596/60000 (73%)]\tLoss: 1088.429199 \n",
      "Train Epoch: 2 [43996/60000 (73%)]\tLoss: 1088.659912 \n",
      "Train Epoch: 2 [44396/60000 (74%)]\tLoss: 1088.317139 \n",
      "Train Epoch: 2 [44796/60000 (75%)]\tLoss: 1088.888794 \n",
      "Train Epoch: 2 [45196/60000 (75%)]\tLoss: 1088.538086 \n",
      "Train Epoch: 2 [45596/60000 (76%)]\tLoss: 1088.520264 \n",
      "Train Epoch: 2 [45996/60000 (77%)]\tLoss: 1088.383301 \n",
      "Train Epoch: 2 [46396/60000 (77%)]\tLoss: 1088.363281 \n",
      "Train Epoch: 2 [46796/60000 (78%)]\tLoss: 1088.393799 \n",
      "Train Epoch: 2 [47196/60000 (79%)]\tLoss: 1089.266602 \n",
      "Train Epoch: 2 [47596/60000 (79%)]\tLoss: 1088.845215 \n",
      "Train Epoch: 2 [47996/60000 (80%)]\tLoss: 1088.334351 \n",
      "Train Epoch: 2 [48396/60000 (81%)]\tLoss: 1088.328125 \n",
      "Train Epoch: 2 [48796/60000 (81%)]\tLoss: 1088.363525 \n",
      "Train Epoch: 2 [49196/60000 (82%)]\tLoss: 1088.432373 \n",
      "Train Epoch: 2 [49596/60000 (83%)]\tLoss: 1088.911377 \n",
      "Train Epoch: 2 [49996/60000 (83%)]\tLoss: 1088.481445 \n",
      "Train Epoch: 2 [50396/60000 (84%)]\tLoss: 1088.399902 \n",
      "Train Epoch: 2 [50796/60000 (85%)]\tLoss: 1089.276245 \n",
      "Train Epoch: 2 [51196/60000 (85%)]\tLoss: 1088.428223 \n",
      "Train Epoch: 2 [51596/60000 (86%)]\tLoss: 1088.612427 \n",
      "Train Epoch: 2 [51996/60000 (87%)]\tLoss: 1088.416138 \n",
      "Train Epoch: 2 [52396/60000 (87%)]\tLoss: 1088.340210 \n",
      "Train Epoch: 2 [52796/60000 (88%)]\tLoss: 1088.301147 \n",
      "Train Epoch: 2 [53196/60000 (89%)]\tLoss: 1088.484009 \n",
      "Train Epoch: 2 [53596/60000 (89%)]\tLoss: 1088.435425 \n",
      "Train Epoch: 2 [53996/60000 (90%)]\tLoss: 1089.032837 \n",
      "Train Epoch: 2 [54396/60000 (91%)]\tLoss: 1088.300293 \n",
      "Train Epoch: 2 [54796/60000 (91%)]\tLoss: 1088.505981 \n",
      "Train Epoch: 2 [55196/60000 (92%)]\tLoss: 1088.401611 \n",
      "Train Epoch: 2 [55596/60000 (93%)]\tLoss: 1088.305664 \n",
      "Train Epoch: 2 [55996/60000 (93%)]\tLoss: 1088.367432 \n",
      "Train Epoch: 2 [56396/60000 (94%)]\tLoss: 1089.180908 \n",
      "Train Epoch: 2 [56796/60000 (95%)]\tLoss: 1088.305908 \n",
      "Train Epoch: 2 [57196/60000 (95%)]\tLoss: 1088.325806 \n",
      "Train Epoch: 2 [57596/60000 (96%)]\tLoss: 1088.384888 \n",
      "Train Epoch: 2 [57996/60000 (97%)]\tLoss: 1088.408936 \n",
      "Train Epoch: 2 [58396/60000 (97%)]\tLoss: 1088.416382 \n",
      "Train Epoch: 2 [58796/60000 (98%)]\tLoss: 1088.630981 \n",
      "Train Epoch: 2 [59196/60000 (99%)]\tLoss: 1088.637695 \n",
      "Train Epoch: 2 [59596/60000 (99%)]\tLoss: 1088.337158 \n",
      "Train Epoch: 2 [59996/60000 (100%)]\tLoss: 1088.315186 \n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "model.train()\n",
    "step = 0 \n",
    "loss_history = []\n",
    "for epoch in range(1, 3):\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_0 = criterion(output, target)\n",
    "        loss_1 = call_pr_loss(list(model.l1.weight) , m_784)\n",
    "        loss_2 = call_pr_loss(list(model.l2.weight) , m_200)\n",
    "        loss_3 = call_pr_loss(list(model.l3.weight) , m_200)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = loss_0 + loss_1+ loss_2+ loss_3\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        loss_history.append(loss.item())\n",
    "        if step % 100  == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} '.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.item()))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0583, Accuracy: 56209/60000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in trainloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(trainloader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(trainloader.dataset),\n",
    "    100. * correct / len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (l1): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (r1): ReLU()\n",
       "  (l2): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r2): ReLU()\n",
       "  (l3): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r3): ReLU()\n",
       "  (lf): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('l1', nn.Linear(784, 200))\n",
    "model.add_module('r1', nn.ReLU())\n",
    "model.add_module('l2', nn.Linear(200, 200))\n",
    "model.add_module('r2', nn.ReLU())\n",
    "model.add_module('l3', nn.Linear(200, 200))\n",
    "model.add_module('r3', nn.ReLU())\n",
    "model.add_module('lf', nn.Linear(200, 10))\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [396/60000 (1%)]\tLoss: 1273.165161 \n",
      "Train Epoch: 1 [796/60000 (1%)]\tLoss: 1274.213745 \n",
      "Train Epoch: 1 [1196/60000 (2%)]\tLoss: 1273.304443 \n",
      "Train Epoch: 1 [1596/60000 (3%)]\tLoss: 1273.267944 \n",
      "Train Epoch: 1 [1996/60000 (3%)]\tLoss: 1273.239868 \n",
      "Train Epoch: 1 [2396/60000 (4%)]\tLoss: 1272.884033 \n",
      "Train Epoch: 1 [2796/60000 (5%)]\tLoss: 1272.223389 \n",
      "Train Epoch: 1 [3196/60000 (5%)]\tLoss: 1273.221069 \n",
      "Train Epoch: 1 [3596/60000 (6%)]\tLoss: 1272.995605 \n",
      "Train Epoch: 1 [3996/60000 (7%)]\tLoss: 1272.723267 \n",
      "Train Epoch: 1 [4396/60000 (7%)]\tLoss: 1272.678833 \n",
      "Train Epoch: 1 [4796/60000 (8%)]\tLoss: 1272.388184 \n",
      "Train Epoch: 1 [5196/60000 (9%)]\tLoss: 1272.868408 \n",
      "Train Epoch: 1 [5596/60000 (9%)]\tLoss: 1272.451904 \n",
      "Train Epoch: 1 [5996/60000 (10%)]\tLoss: 1272.338867 \n",
      "Train Epoch: 1 [6396/60000 (11%)]\tLoss: 1272.318115 \n",
      "Train Epoch: 1 [6796/60000 (11%)]\tLoss: 1272.714722 \n",
      "Train Epoch: 1 [7196/60000 (12%)]\tLoss: 1273.524902 \n",
      "Train Epoch: 1 [7596/60000 (13%)]\tLoss: 1273.047974 \n",
      "Train Epoch: 1 [7996/60000 (13%)]\tLoss: 1272.947021 \n",
      "Train Epoch: 1 [8396/60000 (14%)]\tLoss: 1272.298218 \n",
      "Train Epoch: 1 [8796/60000 (15%)]\tLoss: 1272.165527 \n",
      "Train Epoch: 1 [9196/60000 (15%)]\tLoss: 1273.108154 \n",
      "Train Epoch: 1 [9596/60000 (16%)]\tLoss: 1272.449463 \n",
      "Train Epoch: 1 [9996/60000 (17%)]\tLoss: 1272.540527 \n",
      "Train Epoch: 1 [10396/60000 (17%)]\tLoss: 1272.190063 \n",
      "Train Epoch: 1 [10796/60000 (18%)]\tLoss: 1272.888550 \n",
      "Train Epoch: 1 [11196/60000 (19%)]\tLoss: 1273.660522 \n",
      "Train Epoch: 1 [11596/60000 (19%)]\tLoss: 1272.730591 \n",
      "Train Epoch: 1 [11996/60000 (20%)]\tLoss: 1274.678589 \n",
      "Train Epoch: 1 [12396/60000 (21%)]\tLoss: 1272.150635 \n",
      "Train Epoch: 1 [12796/60000 (21%)]\tLoss: 1272.171997 \n",
      "Train Epoch: 1 [13196/60000 (22%)]\tLoss: 1272.399658 \n",
      "Train Epoch: 1 [13596/60000 (23%)]\tLoss: 1272.215332 \n",
      "Train Epoch: 1 [13996/60000 (23%)]\tLoss: 1272.466064 \n",
      "Train Epoch: 1 [14396/60000 (24%)]\tLoss: 1272.194824 \n",
      "Train Epoch: 1 [14796/60000 (25%)]\tLoss: 1272.501831 \n",
      "Train Epoch: 1 [15196/60000 (25%)]\tLoss: 1272.199829 \n",
      "Train Epoch: 1 [15596/60000 (26%)]\tLoss: 1272.337036 \n",
      "Train Epoch: 1 [15996/60000 (27%)]\tLoss: 1272.698730 \n",
      "Train Epoch: 1 [16396/60000 (27%)]\tLoss: 1272.908325 \n",
      "Train Epoch: 1 [16796/60000 (28%)]\tLoss: 1272.473389 \n",
      "Train Epoch: 1 [17196/60000 (29%)]\tLoss: 1272.453491 \n",
      "Train Epoch: 1 [17596/60000 (29%)]\tLoss: 1272.466431 \n",
      "Train Epoch: 1 [17996/60000 (30%)]\tLoss: 1272.718872 \n",
      "Train Epoch: 1 [18396/60000 (31%)]\tLoss: 1272.290894 \n",
      "Train Epoch: 1 [18796/60000 (31%)]\tLoss: 1272.422974 \n",
      "Train Epoch: 1 [19196/60000 (32%)]\tLoss: 1272.138306 \n",
      "Train Epoch: 1 [19596/60000 (33%)]\tLoss: 1272.296997 \n",
      "Train Epoch: 1 [19996/60000 (33%)]\tLoss: 1272.641357 \n",
      "Train Epoch: 1 [20396/60000 (34%)]\tLoss: 1272.301636 \n",
      "Train Epoch: 1 [20796/60000 (35%)]\tLoss: 1272.740967 \n",
      "Train Epoch: 1 [21196/60000 (35%)]\tLoss: 1272.863770 \n",
      "Train Epoch: 1 [21596/60000 (36%)]\tLoss: 1272.241577 \n",
      "Train Epoch: 1 [21996/60000 (37%)]\tLoss: 1272.459839 \n",
      "Train Epoch: 1 [22396/60000 (37%)]\tLoss: 1272.353027 \n",
      "Train Epoch: 1 [22796/60000 (38%)]\tLoss: 1272.367188 \n",
      "Train Epoch: 1 [23196/60000 (39%)]\tLoss: 1272.192749 \n",
      "Train Epoch: 1 [23596/60000 (39%)]\tLoss: 1274.244263 \n",
      "Train Epoch: 1 [23996/60000 (40%)]\tLoss: 1272.605103 \n",
      "Train Epoch: 1 [24396/60000 (41%)]\tLoss: 1272.159668 \n",
      "Train Epoch: 1 [24796/60000 (41%)]\tLoss: 1272.262207 \n",
      "Train Epoch: 1 [25196/60000 (42%)]\tLoss: 1272.403076 \n",
      "Train Epoch: 1 [25596/60000 (43%)]\tLoss: 1272.714111 \n",
      "Train Epoch: 1 [25996/60000 (43%)]\tLoss: 1272.180054 \n",
      "Train Epoch: 1 [26396/60000 (44%)]\tLoss: 1272.185425 \n",
      "Train Epoch: 1 [26796/60000 (45%)]\tLoss: 1272.223389 \n",
      "Train Epoch: 1 [27196/60000 (45%)]\tLoss: 1272.182739 \n",
      "Train Epoch: 1 [27596/60000 (46%)]\tLoss: 1272.693481 \n",
      "Train Epoch: 1 [27996/60000 (47%)]\tLoss: 1272.253906 \n",
      "Train Epoch: 1 [28396/60000 (47%)]\tLoss: 1272.220825 \n",
      "Train Epoch: 1 [28796/60000 (48%)]\tLoss: 1272.216431 \n",
      "Train Epoch: 1 [29196/60000 (49%)]\tLoss: 1272.398804 \n",
      "Train Epoch: 1 [29596/60000 (49%)]\tLoss: 1272.580811 \n",
      "Train Epoch: 1 [29996/60000 (50%)]\tLoss: 1272.343384 \n",
      "Train Epoch: 1 [30396/60000 (51%)]\tLoss: 1272.652832 \n",
      "Train Epoch: 1 [30796/60000 (51%)]\tLoss: 1272.777588 \n",
      "Train Epoch: 1 [31196/60000 (52%)]\tLoss: 1272.185669 \n",
      "Train Epoch: 1 [31596/60000 (53%)]\tLoss: 1272.182373 \n",
      "Train Epoch: 1 [31996/60000 (53%)]\tLoss: 1272.350220 \n",
      "Train Epoch: 1 [32396/60000 (54%)]\tLoss: 1272.170776 \n",
      "Train Epoch: 1 [32796/60000 (55%)]\tLoss: 1272.144043 \n",
      "Train Epoch: 1 [33196/60000 (55%)]\tLoss: 1273.396362 \n",
      "Train Epoch: 1 [33596/60000 (56%)]\tLoss: 1272.142334 \n",
      "Train Epoch: 1 [33996/60000 (57%)]\tLoss: 1272.469360 \n",
      "Train Epoch: 1 [34396/60000 (57%)]\tLoss: 1272.312378 \n",
      "Train Epoch: 1 [34796/60000 (58%)]\tLoss: 1272.771606 \n",
      "Train Epoch: 1 [35196/60000 (59%)]\tLoss: 1272.612305 \n",
      "Train Epoch: 1 [35596/60000 (59%)]\tLoss: 1272.645630 \n",
      "Train Epoch: 1 [35996/60000 (60%)]\tLoss: 1272.691284 \n",
      "Train Epoch: 1 [36396/60000 (61%)]\tLoss: 1273.163208 \n",
      "Train Epoch: 1 [36796/60000 (61%)]\tLoss: 1272.358521 \n",
      "Train Epoch: 1 [37196/60000 (62%)]\tLoss: 1272.254883 \n",
      "Train Epoch: 1 [37596/60000 (63%)]\tLoss: 1272.270142 \n",
      "Train Epoch: 1 [37996/60000 (63%)]\tLoss: 1272.276978 \n",
      "Train Epoch: 1 [38396/60000 (64%)]\tLoss: 1272.434082 \n",
      "Train Epoch: 1 [38796/60000 (65%)]\tLoss: 1272.162842 \n",
      "Train Epoch: 1 [39196/60000 (65%)]\tLoss: 1272.324707 \n",
      "Train Epoch: 1 [39596/60000 (66%)]\tLoss: 1272.141846 \n",
      "Train Epoch: 1 [39996/60000 (67%)]\tLoss: 1272.282837 \n",
      "Train Epoch: 1 [40396/60000 (67%)]\tLoss: 1272.136963 \n",
      "Train Epoch: 1 [40796/60000 (68%)]\tLoss: 1272.194458 \n",
      "Train Epoch: 1 [41196/60000 (69%)]\tLoss: 1272.438232 \n",
      "Train Epoch: 1 [41596/60000 (69%)]\tLoss: 1272.263428 \n",
      "Train Epoch: 1 [41996/60000 (70%)]\tLoss: 1273.478394 \n",
      "Train Epoch: 1 [42396/60000 (71%)]\tLoss: 1272.933228 \n",
      "Train Epoch: 1 [42796/60000 (71%)]\tLoss: 1272.423340 \n",
      "Train Epoch: 1 [43196/60000 (72%)]\tLoss: 1272.679932 \n",
      "Train Epoch: 1 [43596/60000 (73%)]\tLoss: 1272.135132 \n",
      "Train Epoch: 1 [43996/60000 (73%)]\tLoss: 1272.126831 \n",
      "Train Epoch: 1 [44396/60000 (74%)]\tLoss: 1272.398193 \n",
      "Train Epoch: 1 [44796/60000 (75%)]\tLoss: 1272.439331 \n",
      "Train Epoch: 1 [45196/60000 (75%)]\tLoss: 1272.515869 \n",
      "Train Epoch: 1 [45596/60000 (76%)]\tLoss: 1272.599731 \n",
      "Train Epoch: 1 [45996/60000 (77%)]\tLoss: 1272.133057 \n",
      "Train Epoch: 1 [46396/60000 (77%)]\tLoss: 1272.174438 \n",
      "Train Epoch: 1 [46796/60000 (78%)]\tLoss: 1272.478882 \n",
      "Train Epoch: 1 [47196/60000 (79%)]\tLoss: 1273.613647 \n",
      "Train Epoch: 1 [47596/60000 (79%)]\tLoss: 1272.389648 \n",
      "Train Epoch: 1 [47996/60000 (80%)]\tLoss: 1272.210205 \n",
      "Train Epoch: 1 [48396/60000 (81%)]\tLoss: 1272.407227 \n",
      "Train Epoch: 1 [48796/60000 (81%)]\tLoss: 1272.447021 \n",
      "Train Epoch: 1 [49196/60000 (82%)]\tLoss: 1272.416748 \n",
      "Train Epoch: 1 [49596/60000 (83%)]\tLoss: 1272.196533 \n",
      "Train Epoch: 1 [49996/60000 (83%)]\tLoss: 1272.773804 \n",
      "Train Epoch: 1 [50396/60000 (84%)]\tLoss: 1272.131958 \n",
      "Train Epoch: 1 [50796/60000 (85%)]\tLoss: 1272.160767 \n",
      "Train Epoch: 1 [51196/60000 (85%)]\tLoss: 1272.141846 \n",
      "Train Epoch: 1 [51596/60000 (86%)]\tLoss: 1272.191406 \n",
      "Train Epoch: 1 [51996/60000 (87%)]\tLoss: 1272.328125 \n",
      "Train Epoch: 1 [52396/60000 (87%)]\tLoss: 1272.162476 \n",
      "Train Epoch: 1 [52796/60000 (88%)]\tLoss: 1272.183716 \n",
      "Train Epoch: 1 [53196/60000 (89%)]\tLoss: 1272.224854 \n",
      "Train Epoch: 1 [53596/60000 (89%)]\tLoss: 1272.149170 \n",
      "Train Epoch: 1 [53996/60000 (90%)]\tLoss: 1272.132446 \n",
      "Train Epoch: 1 [54396/60000 (91%)]\tLoss: 1272.204468 \n",
      "Train Epoch: 1 [54796/60000 (91%)]\tLoss: 1272.285278 \n",
      "Train Epoch: 1 [55196/60000 (92%)]\tLoss: 1272.247559 \n",
      "Train Epoch: 1 [55596/60000 (93%)]\tLoss: 1272.442627 \n",
      "Train Epoch: 1 [55996/60000 (93%)]\tLoss: 1272.209717 \n",
      "Train Epoch: 1 [56396/60000 (94%)]\tLoss: 1272.478271 \n",
      "Train Epoch: 1 [56796/60000 (95%)]\tLoss: 1272.156860 \n",
      "Train Epoch: 1 [57196/60000 (95%)]\tLoss: 1272.649536 \n",
      "Train Epoch: 1 [57596/60000 (96%)]\tLoss: 1273.396484 \n",
      "Train Epoch: 1 [57996/60000 (97%)]\tLoss: 1272.281128 \n",
      "Train Epoch: 1 [58396/60000 (97%)]\tLoss: 1273.084717 \n",
      "Train Epoch: 1 [58796/60000 (98%)]\tLoss: 1272.218262 \n",
      "Train Epoch: 1 [59196/60000 (99%)]\tLoss: 1272.161865 \n",
      "Train Epoch: 1 [59596/60000 (99%)]\tLoss: 1272.152832 \n",
      "Train Epoch: 1 [59996/60000 (100%)]\tLoss: 1272.145020 \n",
      "Train Epoch: 2 [396/60000 (1%)]\tLoss: 1272.145142 \n",
      "Train Epoch: 2 [796/60000 (1%)]\tLoss: 1272.951660 \n",
      "Train Epoch: 2 [1196/60000 (2%)]\tLoss: 1272.130127 \n",
      "Train Epoch: 2 [1596/60000 (3%)]\tLoss: 1272.133911 \n",
      "Train Epoch: 2 [1996/60000 (3%)]\tLoss: 1272.760620 \n",
      "Train Epoch: 2 [2396/60000 (4%)]\tLoss: 1272.322876 \n",
      "Train Epoch: 2 [2796/60000 (5%)]\tLoss: 1272.204468 \n",
      "Train Epoch: 2 [3196/60000 (5%)]\tLoss: 1272.122681 \n",
      "Train Epoch: 2 [3596/60000 (6%)]\tLoss: 1272.113281 \n",
      "Train Epoch: 2 [3996/60000 (7%)]\tLoss: 1272.167480 \n",
      "Train Epoch: 2 [4396/60000 (7%)]\tLoss: 1272.274048 \n",
      "Train Epoch: 2 [4796/60000 (8%)]\tLoss: 1272.240601 \n",
      "Train Epoch: 2 [5196/60000 (9%)]\tLoss: 1272.185181 \n",
      "Train Epoch: 2 [5596/60000 (9%)]\tLoss: 1272.224609 \n",
      "Train Epoch: 2 [5996/60000 (10%)]\tLoss: 1272.131104 \n",
      "Train Epoch: 2 [6396/60000 (11%)]\tLoss: 1272.126465 \n",
      "Train Epoch: 2 [6796/60000 (11%)]\tLoss: 1272.112183 \n",
      "Train Epoch: 2 [7196/60000 (12%)]\tLoss: 1272.182129 \n",
      "Train Epoch: 2 [7596/60000 (13%)]\tLoss: 1272.107300 \n",
      "Train Epoch: 2 [7996/60000 (13%)]\tLoss: 1272.133667 \n",
      "Train Epoch: 2 [8396/60000 (14%)]\tLoss: 1272.142456 \n",
      "Train Epoch: 2 [8796/60000 (15%)]\tLoss: 1272.143433 \n",
      "Train Epoch: 2 [9196/60000 (15%)]\tLoss: 1272.385376 \n",
      "Train Epoch: 2 [9596/60000 (16%)]\tLoss: 1272.134766 \n",
      "Train Epoch: 2 [9996/60000 (17%)]\tLoss: 1272.197876 \n",
      "Train Epoch: 2 [10396/60000 (17%)]\tLoss: 1272.505005 \n",
      "Train Epoch: 2 [10796/60000 (18%)]\tLoss: 1272.179199 \n",
      "Train Epoch: 2 [11196/60000 (19%)]\tLoss: 1272.176392 \n",
      "Train Epoch: 2 [11596/60000 (19%)]\tLoss: 1272.174194 \n",
      "Train Epoch: 2 [11996/60000 (20%)]\tLoss: 1272.225952 \n",
      "Train Epoch: 2 [12396/60000 (21%)]\tLoss: 1272.147095 \n",
      "Train Epoch: 2 [12796/60000 (21%)]\tLoss: 1272.544556 \n",
      "Train Epoch: 2 [13196/60000 (22%)]\tLoss: 1272.148438 \n",
      "Train Epoch: 2 [13596/60000 (23%)]\tLoss: 1272.267456 \n",
      "Train Epoch: 2 [13996/60000 (23%)]\tLoss: 1272.245972 \n",
      "Train Epoch: 2 [14396/60000 (24%)]\tLoss: 1272.127808 \n",
      "Train Epoch: 2 [14796/60000 (25%)]\tLoss: 1272.280762 \n",
      "Train Epoch: 2 [15196/60000 (25%)]\tLoss: 1272.258911 \n",
      "Train Epoch: 2 [15596/60000 (26%)]\tLoss: 1273.119873 \n",
      "Train Epoch: 2 [15996/60000 (27%)]\tLoss: 1272.100098 \n",
      "Train Epoch: 2 [16396/60000 (27%)]\tLoss: 1272.305664 \n",
      "Train Epoch: 2 [16796/60000 (28%)]\tLoss: 1272.571777 \n",
      "Train Epoch: 2 [17196/60000 (29%)]\tLoss: 1272.148438 \n",
      "Train Epoch: 2 [17596/60000 (29%)]\tLoss: 1272.206787 \n",
      "Train Epoch: 2 [17996/60000 (30%)]\tLoss: 1272.125854 \n",
      "Train Epoch: 2 [18396/60000 (31%)]\tLoss: 1272.172119 \n",
      "Train Epoch: 2 [18796/60000 (31%)]\tLoss: 1272.154297 \n",
      "Train Epoch: 2 [19196/60000 (32%)]\tLoss: 1273.116821 \n",
      "Train Epoch: 2 [19596/60000 (33%)]\tLoss: 1274.019043 \n",
      "Train Epoch: 2 [19996/60000 (33%)]\tLoss: 1272.152466 \n",
      "Train Epoch: 2 [20396/60000 (34%)]\tLoss: 1272.114014 \n",
      "Train Epoch: 2 [20796/60000 (35%)]\tLoss: 1272.143799 \n",
      "Train Epoch: 2 [21196/60000 (35%)]\tLoss: 1272.136475 \n",
      "Train Epoch: 2 [21596/60000 (36%)]\tLoss: 1272.513184 \n",
      "Train Epoch: 2 [21996/60000 (37%)]\tLoss: 1272.222534 \n",
      "Train Epoch: 2 [22396/60000 (37%)]\tLoss: 1272.168457 \n",
      "Train Epoch: 2 [22796/60000 (38%)]\tLoss: 1272.114502 \n",
      "Train Epoch: 2 [23196/60000 (39%)]\tLoss: 1272.218506 \n",
      "Train Epoch: 2 [23596/60000 (39%)]\tLoss: 1272.098145 \n",
      "Train Epoch: 2 [23996/60000 (40%)]\tLoss: 1272.512939 \n",
      "Train Epoch: 2 [24396/60000 (41%)]\tLoss: 1273.481201 \n",
      "Train Epoch: 2 [24796/60000 (41%)]\tLoss: 1272.636108 \n",
      "Train Epoch: 2 [25196/60000 (42%)]\tLoss: 1272.122192 \n",
      "Train Epoch: 2 [25596/60000 (43%)]\tLoss: 1272.107178 \n",
      "Train Epoch: 2 [25996/60000 (43%)]\tLoss: 1272.135132 \n",
      "Train Epoch: 2 [26396/60000 (44%)]\tLoss: 1272.103638 \n",
      "Train Epoch: 2 [26796/60000 (45%)]\tLoss: 1272.119873 \n",
      "Train Epoch: 2 [27196/60000 (45%)]\tLoss: 1272.096924 \n",
      "Train Epoch: 2 [27596/60000 (46%)]\tLoss: 1273.081909 \n",
      "Train Epoch: 2 [27996/60000 (47%)]\tLoss: 1272.143677 \n",
      "Train Epoch: 2 [28396/60000 (47%)]\tLoss: 1272.195801 \n",
      "Train Epoch: 2 [28796/60000 (48%)]\tLoss: 1272.226318 \n",
      "Train Epoch: 2 [29196/60000 (49%)]\tLoss: 1272.117920 \n",
      "Train Epoch: 2 [29596/60000 (49%)]\tLoss: 1272.237793 \n",
      "Train Epoch: 2 [29996/60000 (50%)]\tLoss: 1272.348389 \n",
      "Train Epoch: 2 [30396/60000 (51%)]\tLoss: 1273.067261 \n",
      "Train Epoch: 2 [30796/60000 (51%)]\tLoss: 1272.194092 \n",
      "Train Epoch: 2 [31196/60000 (52%)]\tLoss: 1272.127563 \n",
      "Train Epoch: 2 [31596/60000 (53%)]\tLoss: 1272.148315 \n",
      "Train Epoch: 2 [31996/60000 (53%)]\tLoss: 1272.111938 \n",
      "Train Epoch: 2 [32396/60000 (54%)]\tLoss: 1275.678589 \n",
      "Train Epoch: 2 [32796/60000 (55%)]\tLoss: 1272.090698 \n",
      "Train Epoch: 2 [33196/60000 (55%)]\tLoss: 1272.349365 \n",
      "Train Epoch: 2 [33596/60000 (56%)]\tLoss: 1272.914795 \n",
      "Train Epoch: 2 [33996/60000 (57%)]\tLoss: 1272.499878 \n",
      "Train Epoch: 2 [34396/60000 (57%)]\tLoss: 1272.094238 \n",
      "Train Epoch: 2 [34796/60000 (58%)]\tLoss: 1272.099121 \n",
      "Train Epoch: 2 [35196/60000 (59%)]\tLoss: 1272.150757 \n",
      "Train Epoch: 2 [35596/60000 (59%)]\tLoss: 1272.109863 \n",
      "Train Epoch: 2 [35996/60000 (60%)]\tLoss: 1272.111816 \n",
      "Train Epoch: 2 [36396/60000 (61%)]\tLoss: 1272.155151 \n",
      "Train Epoch: 2 [36796/60000 (61%)]\tLoss: 1272.801392 \n",
      "Train Epoch: 2 [37196/60000 (62%)]\tLoss: 1272.520752 \n",
      "Train Epoch: 2 [37596/60000 (63%)]\tLoss: 1272.351685 \n",
      "Train Epoch: 2 [37996/60000 (63%)]\tLoss: 1272.106812 \n",
      "Train Epoch: 2 [38396/60000 (64%)]\tLoss: 1272.109985 \n",
      "Train Epoch: 2 [38796/60000 (65%)]\tLoss: 1272.140869 \n",
      "Train Epoch: 2 [39196/60000 (65%)]\tLoss: 1272.229004 \n",
      "Train Epoch: 2 [39596/60000 (66%)]\tLoss: 1272.123169 \n",
      "Train Epoch: 2 [39996/60000 (67%)]\tLoss: 1272.118774 \n",
      "Train Epoch: 2 [40396/60000 (67%)]\tLoss: 1272.151978 \n",
      "Train Epoch: 2 [40796/60000 (68%)]\tLoss: 1272.147705 \n",
      "Train Epoch: 2 [41196/60000 (69%)]\tLoss: 1272.128906 \n",
      "Train Epoch: 2 [41596/60000 (69%)]\tLoss: 1272.099487 \n",
      "Train Epoch: 2 [41996/60000 (70%)]\tLoss: 1272.089233 \n",
      "Train Epoch: 2 [42396/60000 (71%)]\tLoss: 1272.093628 \n",
      "Train Epoch: 2 [42796/60000 (71%)]\tLoss: 1272.445435 \n",
      "Train Epoch: 2 [43196/60000 (72%)]\tLoss: 1273.714966 \n",
      "Train Epoch: 2 [43596/60000 (73%)]\tLoss: 1272.111084 \n",
      "Train Epoch: 2 [43996/60000 (73%)]\tLoss: 1272.375366 \n",
      "Train Epoch: 2 [44396/60000 (74%)]\tLoss: 1272.247925 \n",
      "Train Epoch: 2 [44796/60000 (75%)]\tLoss: 1272.086670 \n",
      "Train Epoch: 2 [45196/60000 (75%)]\tLoss: 1272.120850 \n",
      "Train Epoch: 2 [45596/60000 (76%)]\tLoss: 1272.579834 \n",
      "Train Epoch: 2 [45996/60000 (77%)]\tLoss: 1272.135010 \n",
      "Train Epoch: 2 [46396/60000 (77%)]\tLoss: 1272.251221 \n",
      "Train Epoch: 2 [46796/60000 (78%)]\tLoss: 1272.087402 \n",
      "Train Epoch: 2 [47196/60000 (79%)]\tLoss: 1272.365356 \n",
      "Train Epoch: 2 [47596/60000 (79%)]\tLoss: 1272.111328 \n",
      "Train Epoch: 2 [47996/60000 (80%)]\tLoss: 1272.115967 \n",
      "Train Epoch: 2 [48396/60000 (81%)]\tLoss: 1272.108887 \n",
      "Train Epoch: 2 [48796/60000 (81%)]\tLoss: 1272.131470 \n",
      "Train Epoch: 2 [49196/60000 (82%)]\tLoss: 1272.115112 \n",
      "Train Epoch: 2 [49596/60000 (83%)]\tLoss: 1273.036499 \n",
      "Train Epoch: 2 [49996/60000 (83%)]\tLoss: 1272.096680 \n",
      "Train Epoch: 2 [50396/60000 (84%)]\tLoss: 1272.089355 \n",
      "Train Epoch: 2 [50796/60000 (85%)]\tLoss: 1272.471680 \n",
      "Train Epoch: 2 [51196/60000 (85%)]\tLoss: 1272.114258 \n",
      "Train Epoch: 2 [51596/60000 (86%)]\tLoss: 1272.243164 \n",
      "Train Epoch: 2 [51996/60000 (87%)]\tLoss: 1272.160889 \n",
      "Train Epoch: 2 [52396/60000 (87%)]\tLoss: 1272.114746 \n",
      "Train Epoch: 2 [52796/60000 (88%)]\tLoss: 1272.140015 \n",
      "Train Epoch: 2 [53196/60000 (89%)]\tLoss: 1272.114258 \n",
      "Train Epoch: 2 [53596/60000 (89%)]\tLoss: 1272.091064 \n",
      "Train Epoch: 2 [53996/60000 (90%)]\tLoss: 1272.120728 \n",
      "Train Epoch: 2 [54396/60000 (91%)]\tLoss: 1272.200317 \n",
      "Train Epoch: 2 [54796/60000 (91%)]\tLoss: 1272.140625 \n",
      "Train Epoch: 2 [55196/60000 (92%)]\tLoss: 1272.092651 \n",
      "Train Epoch: 2 [55596/60000 (93%)]\tLoss: 1272.246094 \n",
      "Train Epoch: 2 [55996/60000 (93%)]\tLoss: 1272.210327 \n",
      "Train Epoch: 2 [56396/60000 (94%)]\tLoss: 1272.120850 \n",
      "Train Epoch: 2 [56796/60000 (95%)]\tLoss: 1272.365601 \n",
      "Train Epoch: 2 [57196/60000 (95%)]\tLoss: 1272.097168 \n",
      "Train Epoch: 2 [57596/60000 (96%)]\tLoss: 1272.549072 \n",
      "Train Epoch: 2 [57996/60000 (97%)]\tLoss: 1272.127808 \n",
      "Train Epoch: 2 [58396/60000 (97%)]\tLoss: 1272.153076 \n",
      "Train Epoch: 2 [58796/60000 (98%)]\tLoss: 1272.192871 \n",
      "Train Epoch: 2 [59196/60000 (99%)]\tLoss: 1272.102661 \n",
      "Train Epoch: 2 [59596/60000 (99%)]\tLoss: 1272.120728 \n",
      "Train Epoch: 2 [59996/60000 (100%)]\tLoss: 1272.113403 \n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "model.train()\n",
    "step = 0 \n",
    "loss_history = []\n",
    "for epoch in range(1, 3):\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_0 = criterion(output, target)\n",
    "        loss_1 = call_pr_loss(list(model.l1.weight) , m_784)\n",
    "        loss_2 = call_pr_loss(list(model.l2.weight) , m_200)\n",
    "        loss_3 = call_pr_loss(list(model.l3.weight) , m_200)\n",
    "        loss_f = call_pr_loss(list(model.lf.weight) , m_200)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = loss_0 + loss_1+ loss_2+ loss_3 + loss_f\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        loss_history.append(loss.item())\n",
    "        if step % 100  == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} '.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.item()))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0508, Accuracy: 56605/60000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in trainloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(trainloader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(trainloader.dataset),\n",
    "    100. * correct / len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (l1): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (r1): ReLU()\n",
       "  (l2): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r2): ReLU()\n",
       "  (l3): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r3): ReLU()\n",
       "  (l4): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r4): ReLU()\n",
       "  (lf): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('l1', nn.Linear(784, 200))\n",
    "model.add_module('r1', nn.ReLU())\n",
    "model.add_module('l2', nn.Linear(200, 200))\n",
    "model.add_module('r2', nn.ReLU())\n",
    "model.add_module('l3', nn.Linear(200, 200))\n",
    "model.add_module('r3', nn.ReLU())\n",
    "model.add_module('l4', nn.Linear(200, 200))\n",
    "model.add_module('r4', nn.ReLU())\n",
    "model.add_module('lf', nn.Linear(200, 10))\n",
    "\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [396/60000 (1%)]\tLoss: 1457.793701 \n",
      "Train Epoch: 1 [796/60000 (1%)]\tLoss: 1456.788940 \n",
      "Train Epoch: 1 [1196/60000 (2%)]\tLoss: 1456.826904 \n",
      "Train Epoch: 1 [1596/60000 (3%)]\tLoss: 1457.522339 \n",
      "Train Epoch: 1 [1996/60000 (3%)]\tLoss: 1456.639404 \n",
      "Train Epoch: 1 [2396/60000 (4%)]\tLoss: 1457.189331 \n",
      "Train Epoch: 1 [2796/60000 (5%)]\tLoss: 1457.361206 \n",
      "Train Epoch: 1 [3196/60000 (5%)]\tLoss: 1456.545288 \n",
      "Train Epoch: 1 [3596/60000 (6%)]\tLoss: 1457.025513 \n",
      "Train Epoch: 1 [3996/60000 (7%)]\tLoss: 1456.697388 \n",
      "Train Epoch: 1 [4396/60000 (7%)]\tLoss: 1456.103271 \n",
      "Train Epoch: 1 [4796/60000 (8%)]\tLoss: 1456.893555 \n",
      "Train Epoch: 1 [5196/60000 (9%)]\tLoss: 1457.766357 \n",
      "Train Epoch: 1 [5596/60000 (9%)]\tLoss: 1456.602661 \n",
      "Train Epoch: 1 [5996/60000 (10%)]\tLoss: 1456.678345 \n",
      "Train Epoch: 1 [6396/60000 (11%)]\tLoss: 1456.217041 \n",
      "Train Epoch: 1 [6796/60000 (11%)]\tLoss: 1456.156494 \n",
      "Train Epoch: 1 [7196/60000 (12%)]\tLoss: 1456.489746 \n",
      "Train Epoch: 1 [7596/60000 (13%)]\tLoss: 1457.818726 \n",
      "Train Epoch: 1 [7996/60000 (13%)]\tLoss: 1456.094116 \n",
      "Train Epoch: 1 [8396/60000 (14%)]\tLoss: 1456.678711 \n",
      "Train Epoch: 1 [8796/60000 (15%)]\tLoss: 1456.448364 \n",
      "Train Epoch: 1 [9196/60000 (15%)]\tLoss: 1456.612793 \n",
      "Train Epoch: 1 [9596/60000 (16%)]\tLoss: 1456.593628 \n",
      "Train Epoch: 1 [9996/60000 (17%)]\tLoss: 1456.023560 \n",
      "Train Epoch: 1 [10396/60000 (17%)]\tLoss: 1456.616455 \n",
      "Train Epoch: 1 [10796/60000 (18%)]\tLoss: 1456.953003 \n",
      "Train Epoch: 1 [11196/60000 (19%)]\tLoss: 1457.298706 \n",
      "Train Epoch: 1 [11596/60000 (19%)]\tLoss: 1457.545654 \n",
      "Train Epoch: 1 [11996/60000 (20%)]\tLoss: 1455.958130 \n",
      "Train Epoch: 1 [12396/60000 (21%)]\tLoss: 1456.244507 \n",
      "Train Epoch: 1 [12796/60000 (21%)]\tLoss: 1457.247070 \n",
      "Train Epoch: 1 [13196/60000 (22%)]\tLoss: 1456.260742 \n",
      "Train Epoch: 1 [13596/60000 (23%)]\tLoss: 1455.993164 \n",
      "Train Epoch: 1 [13996/60000 (23%)]\tLoss: 1455.969849 \n",
      "Train Epoch: 1 [14396/60000 (24%)]\tLoss: 1457.208740 \n",
      "Train Epoch: 1 [14796/60000 (25%)]\tLoss: 1456.046265 \n",
      "Train Epoch: 1 [15196/60000 (25%)]\tLoss: 1455.943848 \n",
      "Train Epoch: 1 [15596/60000 (26%)]\tLoss: 1456.933594 \n",
      "Train Epoch: 1 [15996/60000 (27%)]\tLoss: 1456.373291 \n",
      "Train Epoch: 1 [16396/60000 (27%)]\tLoss: 1456.313721 \n",
      "Train Epoch: 1 [16796/60000 (28%)]\tLoss: 1456.201538 \n",
      "Train Epoch: 1 [17196/60000 (29%)]\tLoss: 1456.022705 \n",
      "Train Epoch: 1 [17596/60000 (29%)]\tLoss: 1456.441650 \n",
      "Train Epoch: 1 [17996/60000 (30%)]\tLoss: 1457.006348 \n",
      "Train Epoch: 1 [18396/60000 (31%)]\tLoss: 1456.073608 \n",
      "Train Epoch: 1 [18796/60000 (31%)]\tLoss: 1455.925537 \n",
      "Train Epoch: 1 [19196/60000 (32%)]\tLoss: 1456.109497 \n",
      "Train Epoch: 1 [19596/60000 (33%)]\tLoss: 1455.998169 \n",
      "Train Epoch: 1 [19996/60000 (33%)]\tLoss: 1455.924438 \n",
      "Train Epoch: 1 [20396/60000 (34%)]\tLoss: 1456.063599 \n",
      "Train Epoch: 1 [20796/60000 (35%)]\tLoss: 1457.316406 \n",
      "Train Epoch: 1 [21196/60000 (35%)]\tLoss: 1457.231934 \n",
      "Train Epoch: 1 [21596/60000 (36%)]\tLoss: 1455.942749 \n",
      "Train Epoch: 1 [21996/60000 (37%)]\tLoss: 1455.978027 \n",
      "Train Epoch: 1 [22396/60000 (37%)]\tLoss: 1456.074097 \n",
      "Train Epoch: 1 [22796/60000 (38%)]\tLoss: 1456.721680 \n",
      "Train Epoch: 1 [23196/60000 (39%)]\tLoss: 1455.958740 \n",
      "Train Epoch: 1 [23596/60000 (39%)]\tLoss: 1456.170532 \n",
      "Train Epoch: 1 [23996/60000 (40%)]\tLoss: 1456.425415 \n",
      "Train Epoch: 1 [24396/60000 (41%)]\tLoss: 1456.168579 \n",
      "Train Epoch: 1 [24796/60000 (41%)]\tLoss: 1455.957031 \n",
      "Train Epoch: 1 [25196/60000 (42%)]\tLoss: 1456.270752 \n",
      "Train Epoch: 1 [25596/60000 (43%)]\tLoss: 1457.465454 \n",
      "Train Epoch: 1 [25996/60000 (43%)]\tLoss: 1455.953735 \n",
      "Train Epoch: 1 [26396/60000 (44%)]\tLoss: 1456.240967 \n",
      "Train Epoch: 1 [26796/60000 (45%)]\tLoss: 1456.546509 \n",
      "Train Epoch: 1 [27196/60000 (45%)]\tLoss: 1456.067993 \n",
      "Train Epoch: 1 [27596/60000 (46%)]\tLoss: 1456.294312 \n",
      "Train Epoch: 1 [27996/60000 (47%)]\tLoss: 1456.062012 \n",
      "Train Epoch: 1 [28396/60000 (47%)]\tLoss: 1456.458618 \n",
      "Train Epoch: 1 [28796/60000 (48%)]\tLoss: 1456.030151 \n",
      "Train Epoch: 1 [29196/60000 (49%)]\tLoss: 1456.160889 \n",
      "Train Epoch: 1 [29596/60000 (49%)]\tLoss: 1455.910156 \n",
      "Train Epoch: 1 [29996/60000 (50%)]\tLoss: 1456.210571 \n",
      "Train Epoch: 1 [30396/60000 (51%)]\tLoss: 1455.957520 \n",
      "Train Epoch: 1 [30796/60000 (51%)]\tLoss: 1455.924316 \n",
      "Train Epoch: 1 [31196/60000 (52%)]\tLoss: 1456.336426 \n",
      "Train Epoch: 1 [31596/60000 (53%)]\tLoss: 1456.048706 \n",
      "Train Epoch: 1 [31996/60000 (53%)]\tLoss: 1456.034790 \n",
      "Train Epoch: 1 [32396/60000 (54%)]\tLoss: 1456.017822 \n",
      "Train Epoch: 1 [32796/60000 (55%)]\tLoss: 1456.001465 \n",
      "Train Epoch: 1 [33196/60000 (55%)]\tLoss: 1457.087891 \n",
      "Train Epoch: 1 [33596/60000 (56%)]\tLoss: 1456.525635 \n",
      "Train Epoch: 1 [33996/60000 (57%)]\tLoss: 1456.659058 \n",
      "Train Epoch: 1 [34396/60000 (57%)]\tLoss: 1455.940796 \n",
      "Train Epoch: 1 [34796/60000 (58%)]\tLoss: 1456.140991 \n",
      "Train Epoch: 1 [35196/60000 (59%)]\tLoss: 1456.227295 \n",
      "Train Epoch: 1 [35596/60000 (59%)]\tLoss: 1456.284668 \n",
      "Train Epoch: 1 [35996/60000 (60%)]\tLoss: 1456.098267 \n",
      "Train Epoch: 1 [36396/60000 (61%)]\tLoss: 1456.139526 \n",
      "Train Epoch: 1 [36796/60000 (61%)]\tLoss: 1456.233032 \n",
      "Train Epoch: 1 [37196/60000 (62%)]\tLoss: 1456.112427 \n",
      "Train Epoch: 1 [37596/60000 (63%)]\tLoss: 1456.137451 \n",
      "Train Epoch: 1 [37996/60000 (63%)]\tLoss: 1456.995117 \n",
      "Train Epoch: 1 [38396/60000 (64%)]\tLoss: 1455.933228 \n",
      "Train Epoch: 1 [38796/60000 (65%)]\tLoss: 1457.007812 \n",
      "Train Epoch: 1 [39196/60000 (65%)]\tLoss: 1456.353638 \n",
      "Train Epoch: 1 [39596/60000 (66%)]\tLoss: 1456.080444 \n",
      "Train Epoch: 1 [39996/60000 (67%)]\tLoss: 1455.943359 \n",
      "Train Epoch: 1 [40396/60000 (67%)]\tLoss: 1456.713623 \n",
      "Train Epoch: 1 [40796/60000 (68%)]\tLoss: 1457.171875 \n",
      "Train Epoch: 1 [41196/60000 (69%)]\tLoss: 1456.139526 \n",
      "Train Epoch: 1 [41596/60000 (69%)]\tLoss: 1455.978149 \n",
      "Train Epoch: 1 [41996/60000 (70%)]\tLoss: 1456.173340 \n",
      "Train Epoch: 1 [42396/60000 (71%)]\tLoss: 1455.960693 \n",
      "Train Epoch: 1 [42796/60000 (71%)]\tLoss: 1455.945068 \n",
      "Train Epoch: 1 [43196/60000 (72%)]\tLoss: 1457.098022 \n",
      "Train Epoch: 1 [43596/60000 (73%)]\tLoss: 1456.996826 \n",
      "Train Epoch: 1 [43996/60000 (73%)]\tLoss: 1455.977539 \n",
      "Train Epoch: 1 [44396/60000 (74%)]\tLoss: 1456.866211 \n",
      "Train Epoch: 1 [44796/60000 (75%)]\tLoss: 1456.147461 \n",
      "Train Epoch: 1 [45196/60000 (75%)]\tLoss: 1456.041870 \n",
      "Train Epoch: 1 [45596/60000 (76%)]\tLoss: 1456.415283 \n",
      "Train Epoch: 1 [45996/60000 (77%)]\tLoss: 1456.215942 \n",
      "Train Epoch: 1 [46396/60000 (77%)]\tLoss: 1455.984619 \n",
      "Train Epoch: 1 [46796/60000 (78%)]\tLoss: 1455.957520 \n",
      "Train Epoch: 1 [47196/60000 (79%)]\tLoss: 1455.966431 \n",
      "Train Epoch: 1 [47596/60000 (79%)]\tLoss: 1456.114258 \n",
      "Train Epoch: 1 [47996/60000 (80%)]\tLoss: 1456.036621 \n",
      "Train Epoch: 1 [48396/60000 (81%)]\tLoss: 1456.814087 \n",
      "Train Epoch: 1 [48796/60000 (81%)]\tLoss: 1456.105835 \n",
      "Train Epoch: 1 [49196/60000 (82%)]\tLoss: 1456.040405 \n",
      "Train Epoch: 1 [49596/60000 (83%)]\tLoss: 1455.938721 \n",
      "Train Epoch: 1 [49996/60000 (83%)]\tLoss: 1455.988037 \n",
      "Train Epoch: 1 [50396/60000 (84%)]\tLoss: 1457.383057 \n",
      "Train Epoch: 1 [50796/60000 (85%)]\tLoss: 1456.479736 \n",
      "Train Epoch: 1 [51196/60000 (85%)]\tLoss: 1457.019775 \n",
      "Train Epoch: 1 [51596/60000 (86%)]\tLoss: 1456.688965 \n",
      "Train Epoch: 1 [51996/60000 (87%)]\tLoss: 1455.989502 \n",
      "Train Epoch: 1 [52396/60000 (87%)]\tLoss: 1456.461182 \n",
      "Train Epoch: 1 [52796/60000 (88%)]\tLoss: 1456.233521 \n",
      "Train Epoch: 1 [53196/60000 (89%)]\tLoss: 1456.446289 \n",
      "Train Epoch: 1 [53596/60000 (89%)]\tLoss: 1455.969727 \n",
      "Train Epoch: 1 [53996/60000 (90%)]\tLoss: 1456.016113 \n",
      "Train Epoch: 1 [54396/60000 (91%)]\tLoss: 1456.354736 \n",
      "Train Epoch: 1 [54796/60000 (91%)]\tLoss: 1456.072388 \n",
      "Train Epoch: 1 [55196/60000 (92%)]\tLoss: 1455.938232 \n",
      "Train Epoch: 1 [55596/60000 (93%)]\tLoss: 1456.248535 \n",
      "Train Epoch: 1 [55996/60000 (93%)]\tLoss: 1455.962524 \n",
      "Train Epoch: 1 [56396/60000 (94%)]\tLoss: 1455.941528 \n",
      "Train Epoch: 1 [56796/60000 (95%)]\tLoss: 1455.947021 \n",
      "Train Epoch: 1 [57196/60000 (95%)]\tLoss: 1456.354004 \n",
      "Train Epoch: 1 [57596/60000 (96%)]\tLoss: 1455.948608 \n",
      "Train Epoch: 1 [57996/60000 (97%)]\tLoss: 1456.016235 \n",
      "Train Epoch: 1 [58396/60000 (97%)]\tLoss: 1455.957153 \n",
      "Train Epoch: 1 [58796/60000 (98%)]\tLoss: 1455.937378 \n",
      "Train Epoch: 1 [59196/60000 (99%)]\tLoss: 1456.188721 \n",
      "Train Epoch: 1 [59596/60000 (99%)]\tLoss: 1456.195312 \n",
      "Train Epoch: 1 [59996/60000 (100%)]\tLoss: 1456.138672 \n",
      "Train Epoch: 2 [396/60000 (1%)]\tLoss: 1456.007568 \n",
      "Train Epoch: 2 [796/60000 (1%)]\tLoss: 1455.959717 \n",
      "Train Epoch: 2 [1196/60000 (2%)]\tLoss: 1456.069946 \n",
      "Train Epoch: 2 [1596/60000 (3%)]\tLoss: 1456.179932 \n",
      "Train Epoch: 2 [1996/60000 (3%)]\tLoss: 1455.935303 \n",
      "Train Epoch: 2 [2396/60000 (4%)]\tLoss: 1455.963379 \n",
      "Train Epoch: 2 [2796/60000 (5%)]\tLoss: 1456.213379 \n",
      "Train Epoch: 2 [3196/60000 (5%)]\tLoss: 1456.233398 \n",
      "Train Epoch: 2 [3596/60000 (6%)]\tLoss: 1455.988770 \n",
      "Train Epoch: 2 [3996/60000 (7%)]\tLoss: 1455.917969 \n",
      "Train Epoch: 2 [4396/60000 (7%)]\tLoss: 1455.912476 \n",
      "Train Epoch: 2 [4796/60000 (8%)]\tLoss: 1455.966675 \n",
      "Train Epoch: 2 [5196/60000 (9%)]\tLoss: 1455.924438 \n",
      "Train Epoch: 2 [5596/60000 (9%)]\tLoss: 1455.956177 \n",
      "Train Epoch: 2 [5996/60000 (10%)]\tLoss: 1456.063110 \n",
      "Train Epoch: 2 [6396/60000 (11%)]\tLoss: 1455.946167 \n",
      "Train Epoch: 2 [6796/60000 (11%)]\tLoss: 1456.033081 \n",
      "Train Epoch: 2 [7196/60000 (12%)]\tLoss: 1455.917847 \n",
      "Train Epoch: 2 [7596/60000 (13%)]\tLoss: 1455.963135 \n",
      "Train Epoch: 2 [7996/60000 (13%)]\tLoss: 1455.943237 \n",
      "Train Epoch: 2 [8396/60000 (14%)]\tLoss: 1455.997070 \n",
      "Train Epoch: 2 [8796/60000 (15%)]\tLoss: 1455.922729 \n",
      "Train Epoch: 2 [9196/60000 (15%)]\tLoss: 1456.128174 \n",
      "Train Epoch: 2 [9596/60000 (16%)]\tLoss: 1456.505981 \n",
      "Train Epoch: 2 [9996/60000 (17%)]\tLoss: 1456.850830 \n",
      "Train Epoch: 2 [10396/60000 (17%)]\tLoss: 1456.731201 \n",
      "Train Epoch: 2 [10796/60000 (18%)]\tLoss: 1456.046997 \n",
      "Train Epoch: 2 [11196/60000 (19%)]\tLoss: 1455.916138 \n",
      "Train Epoch: 2 [11596/60000 (19%)]\tLoss: 1455.909668 \n",
      "Train Epoch: 2 [11996/60000 (20%)]\tLoss: 1455.961426 \n",
      "Train Epoch: 2 [12396/60000 (21%)]\tLoss: 1456.688477 \n",
      "Train Epoch: 2 [12796/60000 (21%)]\tLoss: 1455.907593 \n",
      "Train Epoch: 2 [13196/60000 (22%)]\tLoss: 1455.989990 \n",
      "Train Epoch: 2 [13596/60000 (23%)]\tLoss: 1455.927490 \n",
      "Train Epoch: 2 [13996/60000 (23%)]\tLoss: 1456.775635 \n",
      "Train Epoch: 2 [14396/60000 (24%)]\tLoss: 1455.993652 \n",
      "Train Epoch: 2 [14796/60000 (25%)]\tLoss: 1456.221924 \n",
      "Train Epoch: 2 [15196/60000 (25%)]\tLoss: 1456.036011 \n",
      "Train Epoch: 2 [15596/60000 (26%)]\tLoss: 1455.900024 \n",
      "Train Epoch: 2 [15996/60000 (27%)]\tLoss: 1455.949219 \n",
      "Train Epoch: 2 [16396/60000 (27%)]\tLoss: 1455.917847 \n",
      "Train Epoch: 2 [16796/60000 (28%)]\tLoss: 1456.070190 \n",
      "Train Epoch: 2 [17196/60000 (29%)]\tLoss: 1455.908569 \n",
      "Train Epoch: 2 [17596/60000 (29%)]\tLoss: 1455.905151 \n",
      "Train Epoch: 2 [17996/60000 (30%)]\tLoss: 1455.947632 \n",
      "Train Epoch: 2 [18396/60000 (31%)]\tLoss: 1456.265503 \n",
      "Train Epoch: 2 [18796/60000 (31%)]\tLoss: 1455.906982 \n",
      "Train Epoch: 2 [19196/60000 (32%)]\tLoss: 1455.912598 \n",
      "Train Epoch: 2 [19596/60000 (33%)]\tLoss: 1456.051758 \n",
      "Train Epoch: 2 [19996/60000 (33%)]\tLoss: 1455.901001 \n",
      "Train Epoch: 2 [20396/60000 (34%)]\tLoss: 1455.962524 \n",
      "Train Epoch: 2 [20796/60000 (35%)]\tLoss: 1456.477539 \n",
      "Train Epoch: 2 [21196/60000 (35%)]\tLoss: 1455.911377 \n",
      "Train Epoch: 2 [21596/60000 (36%)]\tLoss: 1456.632202 \n",
      "Train Epoch: 2 [21996/60000 (37%)]\tLoss: 1455.915527 \n",
      "Train Epoch: 2 [22396/60000 (37%)]\tLoss: 1455.931274 \n",
      "Train Epoch: 2 [22796/60000 (38%)]\tLoss: 1455.952881 \n",
      "Train Epoch: 2 [23196/60000 (39%)]\tLoss: 1455.915771 \n",
      "Train Epoch: 2 [23596/60000 (39%)]\tLoss: 1456.326904 \n",
      "Train Epoch: 2 [23996/60000 (40%)]\tLoss: 1456.091797 \n",
      "Train Epoch: 2 [24396/60000 (41%)]\tLoss: 1455.943604 \n",
      "Train Epoch: 2 [24796/60000 (41%)]\tLoss: 1455.954468 \n",
      "Train Epoch: 2 [25196/60000 (42%)]\tLoss: 1455.941040 \n",
      "Train Epoch: 2 [25596/60000 (43%)]\tLoss: 1456.618652 \n",
      "Train Epoch: 2 [25996/60000 (43%)]\tLoss: 1457.255005 \n",
      "Train Epoch: 2 [26396/60000 (44%)]\tLoss: 1455.889648 \n",
      "Train Epoch: 2 [26796/60000 (45%)]\tLoss: 1456.169678 \n",
      "Train Epoch: 2 [27196/60000 (45%)]\tLoss: 1456.082153 \n",
      "Train Epoch: 2 [27596/60000 (46%)]\tLoss: 1455.933716 \n",
      "Train Epoch: 2 [27996/60000 (47%)]\tLoss: 1456.127686 \n",
      "Train Epoch: 2 [28396/60000 (47%)]\tLoss: 1455.908936 \n",
      "Train Epoch: 2 [28796/60000 (48%)]\tLoss: 1455.893677 \n",
      "Train Epoch: 2 [29196/60000 (49%)]\tLoss: 1456.501587 \n",
      "Train Epoch: 2 [29596/60000 (49%)]\tLoss: 1455.907959 \n",
      "Train Epoch: 2 [29996/60000 (50%)]\tLoss: 1456.164917 \n",
      "Train Epoch: 2 [30396/60000 (51%)]\tLoss: 1456.753540 \n",
      "Train Epoch: 2 [30796/60000 (51%)]\tLoss: 1455.938721 \n",
      "Train Epoch: 2 [31196/60000 (52%)]\tLoss: 1455.956055 \n",
      "Train Epoch: 2 [31596/60000 (53%)]\tLoss: 1456.003174 \n",
      "Train Epoch: 2 [31996/60000 (53%)]\tLoss: 1455.959473 \n",
      "Train Epoch: 2 [32396/60000 (54%)]\tLoss: 1455.950073 \n",
      "Train Epoch: 2 [32796/60000 (55%)]\tLoss: 1455.955566 \n",
      "Train Epoch: 2 [33196/60000 (55%)]\tLoss: 1456.055298 \n",
      "Train Epoch: 2 [33596/60000 (56%)]\tLoss: 1455.938477 \n",
      "Train Epoch: 2 [33996/60000 (57%)]\tLoss: 1455.900269 \n",
      "Train Epoch: 2 [34396/60000 (57%)]\tLoss: 1456.051392 \n",
      "Train Epoch: 2 [34796/60000 (58%)]\tLoss: 1455.904175 \n",
      "Train Epoch: 2 [35196/60000 (59%)]\tLoss: 1456.052612 \n",
      "Train Epoch: 2 [35596/60000 (59%)]\tLoss: 1455.897339 \n",
      "Train Epoch: 2 [35996/60000 (60%)]\tLoss: 1455.887329 \n",
      "Train Epoch: 2 [36396/60000 (61%)]\tLoss: 1455.915527 \n",
      "Train Epoch: 2 [36796/60000 (61%)]\tLoss: 1455.911499 \n",
      "Train Epoch: 2 [37196/60000 (62%)]\tLoss: 1455.984375 \n",
      "Train Epoch: 2 [37596/60000 (63%)]\tLoss: 1455.890747 \n",
      "Train Epoch: 2 [37996/60000 (63%)]\tLoss: 1456.201172 \n",
      "Train Epoch: 2 [38396/60000 (64%)]\tLoss: 1456.086914 \n",
      "Train Epoch: 2 [38796/60000 (65%)]\tLoss: 1456.468262 \n",
      "Train Epoch: 2 [39196/60000 (65%)]\tLoss: 1455.888672 \n",
      "Train Epoch: 2 [39596/60000 (66%)]\tLoss: 1455.966797 \n",
      "Train Epoch: 2 [39996/60000 (67%)]\tLoss: 1455.931030 \n",
      "Train Epoch: 2 [40396/60000 (67%)]\tLoss: 1455.946167 \n",
      "Train Epoch: 2 [40796/60000 (68%)]\tLoss: 1455.956543 \n",
      "Train Epoch: 2 [41196/60000 (69%)]\tLoss: 1455.984009 \n",
      "Train Epoch: 2 [41596/60000 (69%)]\tLoss: 1455.952759 \n",
      "Train Epoch: 2 [41996/60000 (70%)]\tLoss: 1455.918823 \n",
      "Train Epoch: 2 [42396/60000 (71%)]\tLoss: 1456.862671 \n",
      "Train Epoch: 2 [42796/60000 (71%)]\tLoss: 1456.210693 \n",
      "Train Epoch: 2 [43196/60000 (72%)]\tLoss: 1455.943970 \n",
      "Train Epoch: 2 [43596/60000 (73%)]\tLoss: 1455.942017 \n",
      "Train Epoch: 2 [43996/60000 (73%)]\tLoss: 1455.992065 \n",
      "Train Epoch: 2 [44396/60000 (74%)]\tLoss: 1456.011719 \n",
      "Train Epoch: 2 [44796/60000 (75%)]\tLoss: 1455.987061 \n",
      "Train Epoch: 2 [45196/60000 (75%)]\tLoss: 1456.556763 \n",
      "Train Epoch: 2 [45596/60000 (76%)]\tLoss: 1455.984619 \n",
      "Train Epoch: 2 [45996/60000 (77%)]\tLoss: 1455.901611 \n",
      "Train Epoch: 2 [46396/60000 (77%)]\tLoss: 1455.921997 \n",
      "Train Epoch: 2 [46796/60000 (78%)]\tLoss: 1456.556885 \n",
      "Train Epoch: 2 [47196/60000 (79%)]\tLoss: 1455.971680 \n",
      "Train Epoch: 2 [47596/60000 (79%)]\tLoss: 1455.906738 \n",
      "Train Epoch: 2 [47996/60000 (80%)]\tLoss: 1455.887939 \n",
      "Train Epoch: 2 [48396/60000 (81%)]\tLoss: 1456.007690 \n",
      "Train Epoch: 2 [48796/60000 (81%)]\tLoss: 1455.972534 \n",
      "Train Epoch: 2 [49196/60000 (82%)]\tLoss: 1455.908813 \n",
      "Train Epoch: 2 [49596/60000 (83%)]\tLoss: 1455.955688 \n",
      "Train Epoch: 2 [49996/60000 (83%)]\tLoss: 1455.903564 \n",
      "Train Epoch: 2 [50396/60000 (84%)]\tLoss: 1456.475464 \n",
      "Train Epoch: 2 [50796/60000 (85%)]\tLoss: 1455.890869 \n",
      "Train Epoch: 2 [51196/60000 (85%)]\tLoss: 1455.906372 \n",
      "Train Epoch: 2 [51596/60000 (86%)]\tLoss: 1456.007568 \n",
      "Train Epoch: 2 [51996/60000 (87%)]\tLoss: 1456.044434 \n",
      "Train Epoch: 2 [52396/60000 (87%)]\tLoss: 1455.939453 \n",
      "Train Epoch: 2 [52796/60000 (88%)]\tLoss: 1455.888550 \n",
      "Train Epoch: 2 [53196/60000 (89%)]\tLoss: 1455.959106 \n",
      "Train Epoch: 2 [53596/60000 (89%)]\tLoss: 1455.941162 \n",
      "Train Epoch: 2 [53996/60000 (90%)]\tLoss: 1456.302856 \n",
      "Train Epoch: 2 [54396/60000 (91%)]\tLoss: 1455.883911 \n",
      "Train Epoch: 2 [54796/60000 (91%)]\tLoss: 1456.036255 \n",
      "Train Epoch: 2 [55196/60000 (92%)]\tLoss: 1455.878906 \n",
      "Train Epoch: 2 [55596/60000 (93%)]\tLoss: 1455.955200 \n",
      "Train Epoch: 2 [55996/60000 (93%)]\tLoss: 1456.078369 \n",
      "Train Epoch: 2 [56396/60000 (94%)]\tLoss: 1455.875488 \n",
      "Train Epoch: 2 [56796/60000 (95%)]\tLoss: 1455.909058 \n",
      "Train Epoch: 2 [57196/60000 (95%)]\tLoss: 1455.905029 \n",
      "Train Epoch: 2 [57596/60000 (96%)]\tLoss: 1455.908081 \n",
      "Train Epoch: 2 [57996/60000 (97%)]\tLoss: 1455.933716 \n",
      "Train Epoch: 2 [58396/60000 (97%)]\tLoss: 1455.910400 \n",
      "Train Epoch: 2 [58796/60000 (98%)]\tLoss: 1455.913330 \n",
      "Train Epoch: 2 [59196/60000 (99%)]\tLoss: 1455.878174 \n",
      "Train Epoch: 2 [59596/60000 (99%)]\tLoss: 1456.008789 \n",
      "Train Epoch: 2 [59996/60000 (100%)]\tLoss: 1457.063354 \n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "model.train()\n",
    "step = 0 \n",
    "loss_history = []\n",
    "for epoch in range(1, 3):\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_0 = criterion(output, target)\n",
    "        loss_1 = call_pr_loss(list(model.l1.weight) , m_784)\n",
    "        loss_2 = call_pr_loss(list(model.l2.weight) , m_200)\n",
    "        loss_3 = call_pr_loss(list(model.l3.weight) , m_200)\n",
    "        loss_4 = call_pr_loss(list(model.l4.weight) , m_200)\n",
    "        loss_f = call_pr_loss(list(model.lf.weight) , m_200)\n",
    "        \n",
    "        \n",
    "        loss = loss_0 + loss_1+ loss_2+ loss_3 + loss_4 + loss_f\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        loss_history.append(loss.item())\n",
    "        if step % 100  == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} '.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.item()))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0486, Accuracy: 56613/60000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in trainloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(trainloader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(trainloader.dataset),\n",
    "    100. * correct / len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (l1): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (r1): ReLU()\n",
       "  (l2): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r2): ReLU()\n",
       "  (l3): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r3): ReLU()\n",
       "  (l4): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r4): ReLU()\n",
       "  (l5): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r5): ReLU()\n",
       "  (lf): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('l1', nn.Linear(784, 200))\n",
    "model.add_module('r1', nn.ReLU())\n",
    "model.add_module('l2', nn.Linear(200, 200))\n",
    "model.add_module('r2', nn.ReLU())\n",
    "model.add_module('l3', nn.Linear(200, 200))\n",
    "model.add_module('r3', nn.ReLU())\n",
    "model.add_module('l4', nn.Linear(200, 200))\n",
    "model.add_module('r4', nn.ReLU())\n",
    "model.add_module('l5', nn.Linear(200, 200))\n",
    "model.add_module('r5', nn.ReLU())\n",
    "model.add_module('lf', nn.Linear(200, 10))\n",
    "\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [396/60000 (1%)]\tLoss: 1641.770630 \n",
      "Train Epoch: 1 [796/60000 (1%)]\tLoss: 1641.718018 \n",
      "Train Epoch: 1 [1196/60000 (2%)]\tLoss: 1641.682983 \n",
      "Train Epoch: 1 [1596/60000 (3%)]\tLoss: 1641.681641 \n",
      "Train Epoch: 1 [1996/60000 (3%)]\tLoss: 1641.663574 \n",
      "Train Epoch: 1 [2396/60000 (4%)]\tLoss: 1641.703247 \n",
      "Train Epoch: 1 [2796/60000 (5%)]\tLoss: 1641.671387 \n",
      "Train Epoch: 1 [3196/60000 (5%)]\tLoss: 1641.657837 \n",
      "Train Epoch: 1 [3596/60000 (6%)]\tLoss: 1641.690552 \n",
      "Train Epoch: 1 [3996/60000 (7%)]\tLoss: 1641.679077 \n",
      "Train Epoch: 1 [4396/60000 (7%)]\tLoss: 1641.666748 \n",
      "Train Epoch: 1 [4796/60000 (8%)]\tLoss: 1641.730957 \n",
      "Train Epoch: 1 [5196/60000 (9%)]\tLoss: 1641.704102 \n",
      "Train Epoch: 1 [5596/60000 (9%)]\tLoss: 1641.680176 \n",
      "Train Epoch: 1 [5996/60000 (10%)]\tLoss: 1641.678467 \n",
      "Train Epoch: 1 [6396/60000 (11%)]\tLoss: 1641.697754 \n",
      "Train Epoch: 1 [6796/60000 (11%)]\tLoss: 1641.684204 \n",
      "Train Epoch: 1 [7196/60000 (12%)]\tLoss: 1641.707031 \n",
      "Train Epoch: 1 [7596/60000 (13%)]\tLoss: 1641.670654 \n",
      "Train Epoch: 1 [7996/60000 (13%)]\tLoss: 1641.649658 \n",
      "Train Epoch: 1 [8396/60000 (14%)]\tLoss: 1641.698975 \n",
      "Train Epoch: 1 [8796/60000 (15%)]\tLoss: 1641.644775 \n",
      "Train Epoch: 1 [9196/60000 (15%)]\tLoss: 1641.628662 \n",
      "Train Epoch: 1 [9596/60000 (16%)]\tLoss: 1641.648438 \n",
      "Train Epoch: 1 [9996/60000 (17%)]\tLoss: 1641.647949 \n",
      "Train Epoch: 1 [10396/60000 (17%)]\tLoss: 1641.728271 \n",
      "Train Epoch: 1 [10796/60000 (18%)]\tLoss: 1641.638428 \n",
      "Train Epoch: 1 [11196/60000 (19%)]\tLoss: 1641.680420 \n",
      "Train Epoch: 1 [11596/60000 (19%)]\tLoss: 1641.662842 \n",
      "Train Epoch: 1 [11996/60000 (20%)]\tLoss: 1641.655273 \n",
      "Train Epoch: 1 [12396/60000 (21%)]\tLoss: 1641.697021 \n",
      "Train Epoch: 1 [12796/60000 (21%)]\tLoss: 1641.686279 \n",
      "Train Epoch: 1 [13196/60000 (22%)]\tLoss: 1641.692383 \n",
      "Train Epoch: 1 [13596/60000 (23%)]\tLoss: 1641.721924 \n",
      "Train Epoch: 1 [13996/60000 (23%)]\tLoss: 1641.685547 \n",
      "Train Epoch: 1 [14396/60000 (24%)]\tLoss: 1641.725830 \n",
      "Train Epoch: 1 [14796/60000 (25%)]\tLoss: 1641.741211 \n",
      "Train Epoch: 1 [15196/60000 (25%)]\tLoss: 1641.725830 \n",
      "Train Epoch: 1 [15596/60000 (26%)]\tLoss: 1641.721680 \n",
      "Train Epoch: 1 [15996/60000 (27%)]\tLoss: 1641.596924 \n",
      "Train Epoch: 1 [16396/60000 (27%)]\tLoss: 1641.741455 \n",
      "Train Epoch: 1 [16796/60000 (28%)]\tLoss: 1641.688965 \n",
      "Train Epoch: 1 [17196/60000 (29%)]\tLoss: 1641.701904 \n",
      "Train Epoch: 1 [17596/60000 (29%)]\tLoss: 1641.701416 \n",
      "Train Epoch: 1 [17996/60000 (30%)]\tLoss: 1641.705566 \n",
      "Train Epoch: 1 [18396/60000 (31%)]\tLoss: 1641.700195 \n",
      "Train Epoch: 1 [18796/60000 (31%)]\tLoss: 1641.673096 \n",
      "Train Epoch: 1 [19196/60000 (32%)]\tLoss: 1641.686279 \n",
      "Train Epoch: 1 [19596/60000 (33%)]\tLoss: 1641.706055 \n",
      "Train Epoch: 1 [19996/60000 (33%)]\tLoss: 1641.691895 \n",
      "Train Epoch: 1 [20396/60000 (34%)]\tLoss: 1641.673584 \n",
      "Train Epoch: 1 [20796/60000 (35%)]\tLoss: 1641.681641 \n",
      "Train Epoch: 1 [21196/60000 (35%)]\tLoss: 1641.696777 \n",
      "Train Epoch: 1 [21596/60000 (36%)]\tLoss: 1641.686523 \n",
      "Train Epoch: 1 [21996/60000 (37%)]\tLoss: 1641.685791 \n",
      "Train Epoch: 1 [22396/60000 (37%)]\tLoss: 1641.753906 \n",
      "Train Epoch: 1 [22796/60000 (38%)]\tLoss: 1641.617920 \n",
      "Train Epoch: 1 [23196/60000 (39%)]\tLoss: 1641.723145 \n",
      "Train Epoch: 1 [23596/60000 (39%)]\tLoss: 1641.678955 \n",
      "Train Epoch: 1 [23996/60000 (40%)]\tLoss: 1641.592529 \n",
      "Train Epoch: 1 [24396/60000 (41%)]\tLoss: 1641.704102 \n",
      "Train Epoch: 1 [24796/60000 (41%)]\tLoss: 1641.677490 \n",
      "Train Epoch: 1 [25196/60000 (42%)]\tLoss: 1641.647461 \n",
      "Train Epoch: 1 [25596/60000 (43%)]\tLoss: 1641.732666 \n",
      "Train Epoch: 1 [25996/60000 (43%)]\tLoss: 1641.688965 \n",
      "Train Epoch: 1 [26396/60000 (44%)]\tLoss: 1641.734131 \n",
      "Train Epoch: 1 [26796/60000 (45%)]\tLoss: 1641.697510 \n",
      "Train Epoch: 1 [27196/60000 (45%)]\tLoss: 1641.639893 \n",
      "Train Epoch: 1 [27596/60000 (46%)]\tLoss: 1641.686279 \n",
      "Train Epoch: 1 [27996/60000 (47%)]\tLoss: 1641.665771 \n",
      "Train Epoch: 1 [28396/60000 (47%)]\tLoss: 1641.609863 \n",
      "Train Epoch: 1 [28796/60000 (48%)]\tLoss: 1641.693604 \n",
      "Train Epoch: 1 [29196/60000 (49%)]\tLoss: 1641.706543 \n",
      "Train Epoch: 1 [29596/60000 (49%)]\tLoss: 1641.733643 \n",
      "Train Epoch: 1 [29996/60000 (50%)]\tLoss: 1641.625488 \n",
      "Train Epoch: 1 [30396/60000 (51%)]\tLoss: 1641.737061 \n",
      "Train Epoch: 1 [30796/60000 (51%)]\tLoss: 1641.670654 \n",
      "Train Epoch: 1 [31196/60000 (52%)]\tLoss: 1641.681885 \n",
      "Train Epoch: 1 [31596/60000 (53%)]\tLoss: 1641.690430 \n",
      "Train Epoch: 1 [31996/60000 (53%)]\tLoss: 1641.728516 \n",
      "Train Epoch: 1 [32396/60000 (54%)]\tLoss: 1641.684814 \n",
      "Train Epoch: 1 [32796/60000 (55%)]\tLoss: 1641.689697 \n",
      "Train Epoch: 1 [33196/60000 (55%)]\tLoss: 1641.735840 \n",
      "Train Epoch: 1 [33596/60000 (56%)]\tLoss: 1641.686279 \n",
      "Train Epoch: 1 [33996/60000 (57%)]\tLoss: 1641.685059 \n",
      "Train Epoch: 1 [34396/60000 (57%)]\tLoss: 1641.673096 \n",
      "Train Epoch: 1 [34796/60000 (58%)]\tLoss: 1641.646240 \n",
      "Train Epoch: 1 [35196/60000 (59%)]\tLoss: 1641.714111 \n",
      "Train Epoch: 1 [35596/60000 (59%)]\tLoss: 1641.702393 \n",
      "Train Epoch: 1 [35996/60000 (60%)]\tLoss: 1641.681885 \n",
      "Train Epoch: 1 [36396/60000 (61%)]\tLoss: 1641.644287 \n",
      "Train Epoch: 1 [36796/60000 (61%)]\tLoss: 1641.689209 \n",
      "Train Epoch: 1 [37196/60000 (62%)]\tLoss: 1641.705078 \n",
      "Train Epoch: 1 [37596/60000 (63%)]\tLoss: 1641.638916 \n",
      "Train Epoch: 1 [37996/60000 (63%)]\tLoss: 1641.698975 \n",
      "Train Epoch: 1 [38396/60000 (64%)]\tLoss: 1641.705811 \n",
      "Train Epoch: 1 [38796/60000 (65%)]\tLoss: 1641.646973 \n",
      "Train Epoch: 1 [39196/60000 (65%)]\tLoss: 1641.646973 \n",
      "Train Epoch: 1 [39596/60000 (66%)]\tLoss: 1641.681396 \n",
      "Train Epoch: 1 [39996/60000 (67%)]\tLoss: 1641.707520 \n",
      "Train Epoch: 1 [40396/60000 (67%)]\tLoss: 1641.626953 \n",
      "Train Epoch: 1 [40796/60000 (68%)]\tLoss: 1641.661865 \n",
      "Train Epoch: 1 [41196/60000 (69%)]\tLoss: 1641.654785 \n",
      "Train Epoch: 1 [41596/60000 (69%)]\tLoss: 1641.714844 \n",
      "Train Epoch: 1 [41996/60000 (70%)]\tLoss: 1641.632568 \n",
      "Train Epoch: 1 [42396/60000 (71%)]\tLoss: 1641.730469 \n",
      "Train Epoch: 1 [42796/60000 (71%)]\tLoss: 1641.704834 \n",
      "Train Epoch: 1 [43196/60000 (72%)]\tLoss: 1641.685791 \n",
      "Train Epoch: 1 [43596/60000 (73%)]\tLoss: 1641.638916 \n",
      "Train Epoch: 1 [43996/60000 (73%)]\tLoss: 1641.677734 \n",
      "Train Epoch: 1 [44396/60000 (74%)]\tLoss: 1641.680420 \n",
      "Train Epoch: 1 [44796/60000 (75%)]\tLoss: 1641.692627 \n",
      "Train Epoch: 1 [45196/60000 (75%)]\tLoss: 1641.677002 \n",
      "Train Epoch: 1 [45596/60000 (76%)]\tLoss: 1641.654297 \n",
      "Train Epoch: 1 [45996/60000 (77%)]\tLoss: 1641.690186 \n",
      "Train Epoch: 1 [46396/60000 (77%)]\tLoss: 1641.692627 \n",
      "Train Epoch: 1 [46796/60000 (78%)]\tLoss: 1641.699463 \n",
      "Train Epoch: 1 [47196/60000 (79%)]\tLoss: 1641.687012 \n",
      "Train Epoch: 1 [47596/60000 (79%)]\tLoss: 1641.675781 \n",
      "Train Epoch: 1 [47996/60000 (80%)]\tLoss: 1641.673340 \n",
      "Train Epoch: 1 [48396/60000 (81%)]\tLoss: 1641.676025 \n",
      "Train Epoch: 1 [48796/60000 (81%)]\tLoss: 1641.730225 \n",
      "Train Epoch: 1 [49196/60000 (82%)]\tLoss: 1641.704834 \n",
      "Train Epoch: 1 [49596/60000 (83%)]\tLoss: 1641.692627 \n",
      "Train Epoch: 1 [49996/60000 (83%)]\tLoss: 1641.678467 \n",
      "Train Epoch: 1 [50396/60000 (84%)]\tLoss: 1641.719971 \n",
      "Train Epoch: 1 [50796/60000 (85%)]\tLoss: 1641.668457 \n",
      "Train Epoch: 1 [51196/60000 (85%)]\tLoss: 1641.669922 \n",
      "Train Epoch: 1 [51596/60000 (86%)]\tLoss: 1641.685791 \n",
      "Train Epoch: 1 [51996/60000 (87%)]\tLoss: 1641.670898 \n",
      "Train Epoch: 1 [52396/60000 (87%)]\tLoss: 1641.723877 \n",
      "Train Epoch: 1 [52796/60000 (88%)]\tLoss: 1641.658691 \n",
      "Train Epoch: 1 [53196/60000 (89%)]\tLoss: 1641.649658 \n",
      "Train Epoch: 1 [53596/60000 (89%)]\tLoss: 1641.698975 \n",
      "Train Epoch: 1 [53996/60000 (90%)]\tLoss: 1641.678223 \n",
      "Train Epoch: 1 [54396/60000 (91%)]\tLoss: 1641.675049 \n",
      "Train Epoch: 1 [54796/60000 (91%)]\tLoss: 1641.649658 \n",
      "Train Epoch: 1 [55196/60000 (92%)]\tLoss: 1641.658691 \n",
      "Train Epoch: 1 [55596/60000 (93%)]\tLoss: 1641.704590 \n",
      "Train Epoch: 1 [55996/60000 (93%)]\tLoss: 1641.707764 \n",
      "Train Epoch: 1 [56396/60000 (94%)]\tLoss: 1641.723633 \n",
      "Train Epoch: 1 [56796/60000 (95%)]\tLoss: 1641.718506 \n",
      "Train Epoch: 1 [57196/60000 (95%)]\tLoss: 1641.668701 \n",
      "Train Epoch: 1 [57596/60000 (96%)]\tLoss: 1641.631104 \n",
      "Train Epoch: 1 [57996/60000 (97%)]\tLoss: 1641.625732 \n",
      "Train Epoch: 1 [58396/60000 (97%)]\tLoss: 1641.700684 \n",
      "Train Epoch: 1 [58796/60000 (98%)]\tLoss: 1641.694580 \n",
      "Train Epoch: 1 [59196/60000 (99%)]\tLoss: 1641.641846 \n",
      "Train Epoch: 1 [59596/60000 (99%)]\tLoss: 1641.721924 \n",
      "Train Epoch: 1 [59996/60000 (100%)]\tLoss: 1641.645996 \n",
      "Train Epoch: 2 [396/60000 (1%)]\tLoss: 1641.670654 \n",
      "Train Epoch: 2 [796/60000 (1%)]\tLoss: 1641.620850 \n",
      "Train Epoch: 2 [1196/60000 (2%)]\tLoss: 1641.679688 \n",
      "Train Epoch: 2 [1596/60000 (3%)]\tLoss: 1641.722412 \n",
      "Train Epoch: 2 [1996/60000 (3%)]\tLoss: 1641.702393 \n",
      "Train Epoch: 2 [2396/60000 (4%)]\tLoss: 1641.687744 \n",
      "Train Epoch: 2 [2796/60000 (5%)]\tLoss: 1641.689941 \n",
      "Train Epoch: 2 [3196/60000 (5%)]\tLoss: 1641.722656 \n",
      "Train Epoch: 2 [3596/60000 (6%)]\tLoss: 1641.700195 \n",
      "Train Epoch: 2 [3996/60000 (7%)]\tLoss: 1641.694580 \n",
      "Train Epoch: 2 [4396/60000 (7%)]\tLoss: 1641.701172 \n",
      "Train Epoch: 2 [4796/60000 (8%)]\tLoss: 1641.743408 \n",
      "Train Epoch: 2 [5196/60000 (9%)]\tLoss: 1641.669434 \n",
      "Train Epoch: 2 [5596/60000 (9%)]\tLoss: 1641.691895 \n",
      "Train Epoch: 2 [5996/60000 (10%)]\tLoss: 1641.724121 \n",
      "Train Epoch: 2 [6396/60000 (11%)]\tLoss: 1641.679199 \n",
      "Train Epoch: 2 [6796/60000 (11%)]\tLoss: 1641.657227 \n",
      "Train Epoch: 2 [7196/60000 (12%)]\tLoss: 1641.694092 \n",
      "Train Epoch: 2 [7596/60000 (13%)]\tLoss: 1641.676514 \n",
      "Train Epoch: 2 [7996/60000 (13%)]\tLoss: 1641.672607 \n",
      "Train Epoch: 2 [8396/60000 (14%)]\tLoss: 1641.754883 \n",
      "Train Epoch: 2 [8796/60000 (15%)]\tLoss: 1641.724365 \n",
      "Train Epoch: 2 [9196/60000 (15%)]\tLoss: 1641.693604 \n",
      "Train Epoch: 2 [9596/60000 (16%)]\tLoss: 1641.671387 \n",
      "Train Epoch: 2 [9996/60000 (17%)]\tLoss: 1641.667969 \n",
      "Train Epoch: 2 [10396/60000 (17%)]\tLoss: 1641.696289 \n",
      "Train Epoch: 2 [10796/60000 (18%)]\tLoss: 1641.702148 \n",
      "Train Epoch: 2 [11196/60000 (19%)]\tLoss: 1641.696289 \n",
      "Train Epoch: 2 [11596/60000 (19%)]\tLoss: 1641.708008 \n",
      "Train Epoch: 2 [11996/60000 (20%)]\tLoss: 1641.697021 \n",
      "Train Epoch: 2 [12396/60000 (21%)]\tLoss: 1641.685547 \n",
      "Train Epoch: 2 [12796/60000 (21%)]\tLoss: 1641.697266 \n",
      "Train Epoch: 2 [13196/60000 (22%)]\tLoss: 1641.760254 \n",
      "Train Epoch: 2 [13596/60000 (23%)]\tLoss: 1641.720215 \n",
      "Train Epoch: 2 [13996/60000 (23%)]\tLoss: 1641.721680 \n",
      "Train Epoch: 2 [14396/60000 (24%)]\tLoss: 1641.698975 \n",
      "Train Epoch: 2 [14796/60000 (25%)]\tLoss: 1641.592285 \n",
      "Train Epoch: 2 [15196/60000 (25%)]\tLoss: 1641.666016 \n",
      "Train Epoch: 2 [15596/60000 (26%)]\tLoss: 1641.666260 \n",
      "Train Epoch: 2 [15996/60000 (27%)]\tLoss: 1641.678223 \n",
      "Train Epoch: 2 [16396/60000 (27%)]\tLoss: 1641.665771 \n",
      "Train Epoch: 2 [16796/60000 (28%)]\tLoss: 1641.722412 \n",
      "Train Epoch: 2 [17196/60000 (29%)]\tLoss: 1641.679932 \n",
      "Train Epoch: 2 [17596/60000 (29%)]\tLoss: 1641.645264 \n",
      "Train Epoch: 2 [17996/60000 (30%)]\tLoss: 1641.641113 \n",
      "Train Epoch: 2 [18396/60000 (31%)]\tLoss: 1641.646973 \n",
      "Train Epoch: 2 [18796/60000 (31%)]\tLoss: 1641.704346 \n",
      "Train Epoch: 2 [19196/60000 (32%)]\tLoss: 1641.678711 \n",
      "Train Epoch: 2 [19596/60000 (33%)]\tLoss: 1641.708496 \n",
      "Train Epoch: 2 [19996/60000 (33%)]\tLoss: 1641.601807 \n",
      "Train Epoch: 2 [20396/60000 (34%)]\tLoss: 1641.652832 \n",
      "Train Epoch: 2 [20796/60000 (35%)]\tLoss: 1641.678467 \n",
      "Train Epoch: 2 [21196/60000 (35%)]\tLoss: 1641.685791 \n",
      "Train Epoch: 2 [21596/60000 (36%)]\tLoss: 1641.699951 \n",
      "Train Epoch: 2 [21996/60000 (37%)]\tLoss: 1641.699951 \n",
      "Train Epoch: 2 [22396/60000 (37%)]\tLoss: 1641.707275 \n",
      "Train Epoch: 2 [22796/60000 (38%)]\tLoss: 1641.735107 \n",
      "Train Epoch: 2 [23196/60000 (39%)]\tLoss: 1641.657715 \n",
      "Train Epoch: 2 [23596/60000 (39%)]\tLoss: 1641.659912 \n",
      "Train Epoch: 2 [23996/60000 (40%)]\tLoss: 1641.676758 \n",
      "Train Epoch: 2 [24396/60000 (41%)]\tLoss: 1641.705322 \n",
      "Train Epoch: 2 [24796/60000 (41%)]\tLoss: 1641.684570 \n",
      "Train Epoch: 2 [25196/60000 (42%)]\tLoss: 1641.696777 \n",
      "Train Epoch: 2 [25596/60000 (43%)]\tLoss: 1641.677979 \n",
      "Train Epoch: 2 [25996/60000 (43%)]\tLoss: 1641.663330 \n",
      "Train Epoch: 2 [26396/60000 (44%)]\tLoss: 1641.688965 \n",
      "Train Epoch: 2 [26796/60000 (45%)]\tLoss: 1641.733398 \n",
      "Train Epoch: 2 [27196/60000 (45%)]\tLoss: 1641.742920 \n",
      "Train Epoch: 2 [27596/60000 (46%)]\tLoss: 1641.673828 \n",
      "Train Epoch: 2 [27996/60000 (47%)]\tLoss: 1641.685791 \n",
      "Train Epoch: 2 [28396/60000 (47%)]\tLoss: 1641.691650 \n",
      "Train Epoch: 2 [28796/60000 (48%)]\tLoss: 1641.668457 \n",
      "Train Epoch: 2 [29196/60000 (49%)]\tLoss: 1641.703369 \n",
      "Train Epoch: 2 [29596/60000 (49%)]\tLoss: 1641.647949 \n",
      "Train Epoch: 2 [29996/60000 (50%)]\tLoss: 1641.767334 \n",
      "Train Epoch: 2 [30396/60000 (51%)]\tLoss: 1641.668457 \n",
      "Train Epoch: 2 [30796/60000 (51%)]\tLoss: 1641.672119 \n",
      "Train Epoch: 2 [31196/60000 (52%)]\tLoss: 1641.693848 \n",
      "Train Epoch: 2 [31596/60000 (53%)]\tLoss: 1641.703125 \n",
      "Train Epoch: 2 [31996/60000 (53%)]\tLoss: 1641.669189 \n",
      "Train Epoch: 2 [32396/60000 (54%)]\tLoss: 1641.663086 \n",
      "Train Epoch: 2 [32796/60000 (55%)]\tLoss: 1641.721436 \n",
      "Train Epoch: 2 [33196/60000 (55%)]\tLoss: 1641.739746 \n",
      "Train Epoch: 2 [33596/60000 (56%)]\tLoss: 1641.702148 \n",
      "Train Epoch: 2 [33996/60000 (57%)]\tLoss: 1641.694092 \n",
      "Train Epoch: 2 [34396/60000 (57%)]\tLoss: 1641.639160 \n",
      "Train Epoch: 2 [34796/60000 (58%)]\tLoss: 1641.692627 \n",
      "Train Epoch: 2 [35196/60000 (59%)]\tLoss: 1641.680176 \n",
      "Train Epoch: 2 [35596/60000 (59%)]\tLoss: 1641.651855 \n",
      "Train Epoch: 2 [35996/60000 (60%)]\tLoss: 1641.689453 \n",
      "Train Epoch: 2 [36396/60000 (61%)]\tLoss: 1641.683594 \n",
      "Train Epoch: 2 [36796/60000 (61%)]\tLoss: 1641.713623 \n",
      "Train Epoch: 2 [37196/60000 (62%)]\tLoss: 1641.696289 \n",
      "Train Epoch: 2 [37596/60000 (63%)]\tLoss: 1641.732666 \n",
      "Train Epoch: 2 [37996/60000 (63%)]\tLoss: 1641.685059 \n",
      "Train Epoch: 2 [38396/60000 (64%)]\tLoss: 1641.727783 \n",
      "Train Epoch: 2 [38796/60000 (65%)]\tLoss: 1641.718018 \n",
      "Train Epoch: 2 [39196/60000 (65%)]\tLoss: 1641.714600 \n",
      "Train Epoch: 2 [39596/60000 (66%)]\tLoss: 1641.662842 \n",
      "Train Epoch: 2 [39996/60000 (67%)]\tLoss: 1641.698242 \n",
      "Train Epoch: 2 [40396/60000 (67%)]\tLoss: 1641.684326 \n",
      "Train Epoch: 2 [40796/60000 (68%)]\tLoss: 1641.658447 \n",
      "Train Epoch: 2 [41196/60000 (69%)]\tLoss: 1641.746094 \n",
      "Train Epoch: 2 [41596/60000 (69%)]\tLoss: 1641.670410 \n",
      "Train Epoch: 2 [41996/60000 (70%)]\tLoss: 1641.686279 \n",
      "Train Epoch: 2 [42396/60000 (71%)]\tLoss: 1641.696533 \n",
      "Train Epoch: 2 [42796/60000 (71%)]\tLoss: 1641.699707 \n",
      "Train Epoch: 2 [43196/60000 (72%)]\tLoss: 1641.653320 \n",
      "Train Epoch: 2 [43596/60000 (73%)]\tLoss: 1641.692871 \n",
      "Train Epoch: 2 [43996/60000 (73%)]\tLoss: 1641.643066 \n",
      "Train Epoch: 2 [44396/60000 (74%)]\tLoss: 1641.727783 \n",
      "Train Epoch: 2 [44796/60000 (75%)]\tLoss: 1641.686768 \n",
      "Train Epoch: 2 [45196/60000 (75%)]\tLoss: 1641.623047 \n",
      "Train Epoch: 2 [45596/60000 (76%)]\tLoss: 1641.726318 \n",
      "Train Epoch: 2 [45996/60000 (77%)]\tLoss: 1641.664795 \n",
      "Train Epoch: 2 [46396/60000 (77%)]\tLoss: 1641.679932 \n",
      "Train Epoch: 2 [46796/60000 (78%)]\tLoss: 1641.719238 \n",
      "Train Epoch: 2 [47196/60000 (79%)]\tLoss: 1641.698486 \n",
      "Train Epoch: 2 [47596/60000 (79%)]\tLoss: 1641.721191 \n",
      "Train Epoch: 2 [47996/60000 (80%)]\tLoss: 1641.692627 \n",
      "Train Epoch: 2 [48396/60000 (81%)]\tLoss: 1641.663330 \n",
      "Train Epoch: 2 [48796/60000 (81%)]\tLoss: 1641.689453 \n",
      "Train Epoch: 2 [49196/60000 (82%)]\tLoss: 1641.698486 \n",
      "Train Epoch: 2 [49596/60000 (83%)]\tLoss: 1641.673096 \n",
      "Train Epoch: 2 [49996/60000 (83%)]\tLoss: 1641.693115 \n",
      "Train Epoch: 2 [50396/60000 (84%)]\tLoss: 1641.695312 \n",
      "Train Epoch: 2 [50796/60000 (85%)]\tLoss: 1641.623291 \n",
      "Train Epoch: 2 [51196/60000 (85%)]\tLoss: 1641.727295 \n",
      "Train Epoch: 2 [51596/60000 (86%)]\tLoss: 1641.694824 \n",
      "Train Epoch: 2 [51996/60000 (87%)]\tLoss: 1641.696289 \n",
      "Train Epoch: 2 [52396/60000 (87%)]\tLoss: 1641.659180 \n",
      "Train Epoch: 2 [52796/60000 (88%)]\tLoss: 1641.680176 \n",
      "Train Epoch: 2 [53196/60000 (89%)]\tLoss: 1641.730957 \n",
      "Train Epoch: 2 [53596/60000 (89%)]\tLoss: 1641.705322 \n",
      "Train Epoch: 2 [53996/60000 (90%)]\tLoss: 1641.707031 \n",
      "Train Epoch: 2 [54396/60000 (91%)]\tLoss: 1641.680176 \n",
      "Train Epoch: 2 [54796/60000 (91%)]\tLoss: 1641.710693 \n",
      "Train Epoch: 2 [55196/60000 (92%)]\tLoss: 1641.695312 \n",
      "Train Epoch: 2 [55596/60000 (93%)]\tLoss: 1641.667480 \n",
      "Train Epoch: 2 [55996/60000 (93%)]\tLoss: 1641.697998 \n",
      "Train Epoch: 2 [56396/60000 (94%)]\tLoss: 1641.649902 \n",
      "Train Epoch: 2 [56796/60000 (95%)]\tLoss: 1641.727783 \n",
      "Train Epoch: 2 [57196/60000 (95%)]\tLoss: 1641.699707 \n",
      "Train Epoch: 2 [57596/60000 (96%)]\tLoss: 1641.686768 \n",
      "Train Epoch: 2 [57996/60000 (97%)]\tLoss: 1641.684570 \n",
      "Train Epoch: 2 [58396/60000 (97%)]\tLoss: 1641.691162 \n",
      "Train Epoch: 2 [58796/60000 (98%)]\tLoss: 1641.705078 \n",
      "Train Epoch: 2 [59196/60000 (99%)]\tLoss: 1641.649902 \n",
      "Train Epoch: 2 [59596/60000 (99%)]\tLoss: 1641.633789 \n",
      "Train Epoch: 2 [59996/60000 (100%)]\tLoss: 1641.681641 \n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "model.train()\n",
    "step = 0 \n",
    "loss_history = []\n",
    "for epoch in range(1, 3):\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_0 = criterion(output, target)\n",
    "        loss_1 = call_pr_loss(list(model.l1.weight) , m_784)\n",
    "        loss_2 = call_pr_loss(list(model.l2.weight) , m_200)\n",
    "        loss_3 = call_pr_loss(list(model.l3.weight) , m_200)\n",
    "        loss_4 = call_pr_loss(list(model.l4.weight) , m_200)\n",
    "        loss_5 = call_pr_loss(list(model.l5.weight) , m_200)\n",
    "        loss_f = call_pr_loss(list(model.lf.weight) , m_200)\n",
    "        \n",
    "        \n",
    "        loss = loss_0 + loss_1+ loss_2+ loss_3 + loss_4 + loss_f + loss_5\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        loss_history.append(loss.item())\n",
    "        if step % 100  == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} '.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.item()))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5753, Accuracy: 6742/60000 (11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in trainloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(trainloader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(trainloader.dataset),\n",
    "    100. * correct / len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (l1): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (r1): ReLU()\n",
       "  (l2): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r2): ReLU()\n",
       "  (l3): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r3): ReLU()\n",
       "  (l4): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r4): ReLU()\n",
       "  (l5): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r5): ReLU()\n",
       "  (l6): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r6): ReLU()\n",
       "  (lf): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('l1', nn.Linear(784, 200))\n",
    "model.add_module('r1', nn.ReLU())\n",
    "model.add_module('l2', nn.Linear(200, 200))\n",
    "model.add_module('r2', nn.ReLU())\n",
    "model.add_module('l3', nn.Linear(200, 200))\n",
    "model.add_module('r3', nn.ReLU())\n",
    "model.add_module('l4', nn.Linear(200, 200))\n",
    "model.add_module('r4', nn.ReLU())\n",
    "model.add_module('l5', nn.Linear(200, 200))\n",
    "model.add_module('r5', nn.ReLU())\n",
    "model.add_module('l6', nn.Linear(200, 200))\n",
    "model.add_module('r6', nn.ReLU())\n",
    "model.add_module('lf', nn.Linear(200, 10))\n",
    "\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [396/60000 (1%)]\tLoss: 1825.517334 \n",
      "Train Epoch: 1 [796/60000 (1%)]\tLoss: 1825.477783 \n",
      "Train Epoch: 1 [1196/60000 (2%)]\tLoss: 1825.501465 \n",
      "Train Epoch: 1 [1596/60000 (3%)]\tLoss: 1825.468872 \n",
      "Train Epoch: 1 [1996/60000 (3%)]\tLoss: 1825.463379 \n",
      "Train Epoch: 1 [2396/60000 (4%)]\tLoss: 1825.479004 \n",
      "Train Epoch: 1 [2796/60000 (5%)]\tLoss: 1825.467773 \n",
      "Train Epoch: 1 [3196/60000 (5%)]\tLoss: 1825.453491 \n",
      "Train Epoch: 1 [3596/60000 (6%)]\tLoss: 1825.478882 \n",
      "Train Epoch: 1 [3996/60000 (7%)]\tLoss: 1825.516479 \n",
      "Train Epoch: 1 [4396/60000 (7%)]\tLoss: 1825.463135 \n",
      "Train Epoch: 1 [4796/60000 (8%)]\tLoss: 1825.494873 \n",
      "Train Epoch: 1 [5196/60000 (9%)]\tLoss: 1825.444092 \n",
      "Train Epoch: 1 [5596/60000 (9%)]\tLoss: 1825.460205 \n",
      "Train Epoch: 1 [5996/60000 (10%)]\tLoss: 1825.455566 \n",
      "Train Epoch: 1 [6396/60000 (11%)]\tLoss: 1825.462402 \n",
      "Train Epoch: 1 [6796/60000 (11%)]\tLoss: 1825.471313 \n",
      "Train Epoch: 1 [7196/60000 (12%)]\tLoss: 1825.474976 \n",
      "Train Epoch: 1 [7596/60000 (13%)]\tLoss: 1825.469971 \n",
      "Train Epoch: 1 [7996/60000 (13%)]\tLoss: 1825.470459 \n",
      "Train Epoch: 1 [8396/60000 (14%)]\tLoss: 1825.494141 \n",
      "Train Epoch: 1 [8796/60000 (15%)]\tLoss: 1825.453125 \n",
      "Train Epoch: 1 [9196/60000 (15%)]\tLoss: 1825.508179 \n",
      "Train Epoch: 1 [9596/60000 (16%)]\tLoss: 1825.443848 \n",
      "Train Epoch: 1 [9996/60000 (17%)]\tLoss: 1825.457397 \n",
      "Train Epoch: 1 [10396/60000 (17%)]\tLoss: 1825.448853 \n",
      "Train Epoch: 1 [10796/60000 (18%)]\tLoss: 1825.480713 \n",
      "Train Epoch: 1 [11196/60000 (19%)]\tLoss: 1825.522705 \n",
      "Train Epoch: 1 [11596/60000 (19%)]\tLoss: 1825.479248 \n",
      "Train Epoch: 1 [11996/60000 (20%)]\tLoss: 1825.483887 \n",
      "Train Epoch: 1 [12396/60000 (21%)]\tLoss: 1825.480469 \n",
      "Train Epoch: 1 [12796/60000 (21%)]\tLoss: 1825.480225 \n",
      "Train Epoch: 1 [13196/60000 (22%)]\tLoss: 1825.482422 \n",
      "Train Epoch: 1 [13596/60000 (23%)]\tLoss: 1825.436523 \n",
      "Train Epoch: 1 [13996/60000 (23%)]\tLoss: 1825.508057 \n",
      "Train Epoch: 1 [14396/60000 (24%)]\tLoss: 1825.470215 \n",
      "Train Epoch: 1 [14796/60000 (25%)]\tLoss: 1825.488037 \n",
      "Train Epoch: 1 [15196/60000 (25%)]\tLoss: 1825.441650 \n",
      "Train Epoch: 1 [15596/60000 (26%)]\tLoss: 1825.468018 \n",
      "Train Epoch: 1 [15996/60000 (27%)]\tLoss: 1825.420654 \n",
      "Train Epoch: 1 [16396/60000 (27%)]\tLoss: 1825.520508 \n",
      "Train Epoch: 1 [16796/60000 (28%)]\tLoss: 1825.476318 \n",
      "Train Epoch: 1 [17196/60000 (29%)]\tLoss: 1825.438965 \n",
      "Train Epoch: 1 [17596/60000 (29%)]\tLoss: 1825.430664 \n",
      "Train Epoch: 1 [17996/60000 (30%)]\tLoss: 1825.476074 \n",
      "Train Epoch: 1 [18396/60000 (31%)]\tLoss: 1825.459961 \n",
      "Train Epoch: 1 [18796/60000 (31%)]\tLoss: 1825.429688 \n",
      "Train Epoch: 1 [19196/60000 (32%)]\tLoss: 1825.462646 \n",
      "Train Epoch: 1 [19596/60000 (33%)]\tLoss: 1825.460938 \n",
      "Train Epoch: 1 [19996/60000 (33%)]\tLoss: 1825.482178 \n",
      "Train Epoch: 1 [20396/60000 (34%)]\tLoss: 1825.466797 \n",
      "Train Epoch: 1 [20796/60000 (35%)]\tLoss: 1825.496826 \n",
      "Train Epoch: 1 [21196/60000 (35%)]\tLoss: 1825.464355 \n",
      "Train Epoch: 1 [21596/60000 (36%)]\tLoss: 1825.460693 \n",
      "Train Epoch: 1 [21996/60000 (37%)]\tLoss: 1825.568115 \n",
      "Train Epoch: 1 [22396/60000 (37%)]\tLoss: 1825.463379 \n",
      "Train Epoch: 1 [22796/60000 (38%)]\tLoss: 1825.443359 \n",
      "Train Epoch: 1 [23196/60000 (39%)]\tLoss: 1825.466309 \n",
      "Train Epoch: 1 [23596/60000 (39%)]\tLoss: 1825.443115 \n",
      "Train Epoch: 1 [23996/60000 (40%)]\tLoss: 1825.460938 \n",
      "Train Epoch: 1 [24396/60000 (41%)]\tLoss: 1825.415039 \n",
      "Train Epoch: 1 [24796/60000 (41%)]\tLoss: 1825.516357 \n",
      "Train Epoch: 1 [25196/60000 (42%)]\tLoss: 1825.498291 \n",
      "Train Epoch: 1 [25596/60000 (43%)]\tLoss: 1825.479980 \n",
      "Train Epoch: 1 [25996/60000 (43%)]\tLoss: 1825.500732 \n",
      "Train Epoch: 1 [26396/60000 (44%)]\tLoss: 1825.464111 \n",
      "Train Epoch: 1 [26796/60000 (45%)]\tLoss: 1825.466309 \n",
      "Train Epoch: 1 [27196/60000 (45%)]\tLoss: 1825.460449 \n",
      "Train Epoch: 1 [27596/60000 (46%)]\tLoss: 1825.433594 \n",
      "Train Epoch: 1 [27996/60000 (47%)]\tLoss: 1825.432617 \n",
      "Train Epoch: 1 [28396/60000 (47%)]\tLoss: 1825.455322 \n",
      "Train Epoch: 1 [28796/60000 (48%)]\tLoss: 1825.489502 \n",
      "Train Epoch: 1 [29196/60000 (49%)]\tLoss: 1825.481689 \n",
      "Train Epoch: 1 [29596/60000 (49%)]\tLoss: 1825.493652 \n",
      "Train Epoch: 1 [29996/60000 (50%)]\tLoss: 1825.480957 \n",
      "Train Epoch: 1 [30396/60000 (51%)]\tLoss: 1825.447510 \n",
      "Train Epoch: 1 [30796/60000 (51%)]\tLoss: 1825.492432 \n",
      "Train Epoch: 1 [31196/60000 (52%)]\tLoss: 1825.512939 \n",
      "Train Epoch: 1 [31596/60000 (53%)]\tLoss: 1825.464355 \n",
      "Train Epoch: 1 [31996/60000 (53%)]\tLoss: 1825.488770 \n",
      "Train Epoch: 1 [32396/60000 (54%)]\tLoss: 1825.451660 \n",
      "Train Epoch: 1 [32796/60000 (55%)]\tLoss: 1825.507568 \n",
      "Train Epoch: 1 [33196/60000 (55%)]\tLoss: 1825.462891 \n",
      "Train Epoch: 1 [33596/60000 (56%)]\tLoss: 1825.477051 \n",
      "Train Epoch: 1 [33996/60000 (57%)]\tLoss: 1825.493408 \n",
      "Train Epoch: 1 [34396/60000 (57%)]\tLoss: 1825.444336 \n",
      "Train Epoch: 1 [34796/60000 (58%)]\tLoss: 1825.479980 \n",
      "Train Epoch: 1 [35196/60000 (59%)]\tLoss: 1825.515869 \n",
      "Train Epoch: 1 [35596/60000 (59%)]\tLoss: 1825.448242 \n",
      "Train Epoch: 1 [35996/60000 (60%)]\tLoss: 1825.496094 \n",
      "Train Epoch: 1 [36396/60000 (61%)]\tLoss: 1825.492676 \n",
      "Train Epoch: 1 [36796/60000 (61%)]\tLoss: 1825.437744 \n",
      "Train Epoch: 1 [37196/60000 (62%)]\tLoss: 1825.484619 \n",
      "Train Epoch: 1 [37596/60000 (63%)]\tLoss: 1825.463623 \n",
      "Train Epoch: 1 [37996/60000 (63%)]\tLoss: 1825.453369 \n",
      "Train Epoch: 1 [38396/60000 (64%)]\tLoss: 1825.430908 \n",
      "Train Epoch: 1 [38796/60000 (65%)]\tLoss: 1825.485840 \n",
      "Train Epoch: 1 [39196/60000 (65%)]\tLoss: 1825.487305 \n",
      "Train Epoch: 1 [39596/60000 (66%)]\tLoss: 1825.441650 \n",
      "Train Epoch: 1 [39996/60000 (67%)]\tLoss: 1825.473633 \n",
      "Train Epoch: 1 [40396/60000 (67%)]\tLoss: 1825.461914 \n",
      "Train Epoch: 1 [40796/60000 (68%)]\tLoss: 1825.481445 \n",
      "Train Epoch: 1 [41196/60000 (69%)]\tLoss: 1825.425537 \n",
      "Train Epoch: 1 [41596/60000 (69%)]\tLoss: 1825.464844 \n",
      "Train Epoch: 1 [41996/60000 (70%)]\tLoss: 1825.439941 \n",
      "Train Epoch: 1 [42396/60000 (71%)]\tLoss: 1825.468506 \n",
      "Train Epoch: 1 [42796/60000 (71%)]\tLoss: 1825.457275 \n",
      "Train Epoch: 1 [43196/60000 (72%)]\tLoss: 1825.515137 \n",
      "Train Epoch: 1 [43596/60000 (73%)]\tLoss: 1825.454346 \n",
      "Train Epoch: 1 [43996/60000 (73%)]\tLoss: 1825.483154 \n",
      "Train Epoch: 1 [44396/60000 (74%)]\tLoss: 1825.462402 \n",
      "Train Epoch: 1 [44796/60000 (75%)]\tLoss: 1825.483398 \n",
      "Train Epoch: 1 [45196/60000 (75%)]\tLoss: 1825.485840 \n",
      "Train Epoch: 1 [45596/60000 (76%)]\tLoss: 1825.468506 \n",
      "Train Epoch: 1 [45996/60000 (77%)]\tLoss: 1825.489746 \n",
      "Train Epoch: 1 [46396/60000 (77%)]\tLoss: 1825.502930 \n",
      "Train Epoch: 1 [46796/60000 (78%)]\tLoss: 1825.480957 \n",
      "Train Epoch: 1 [47196/60000 (79%)]\tLoss: 1825.485840 \n",
      "Train Epoch: 1 [47596/60000 (79%)]\tLoss: 1825.494873 \n",
      "Train Epoch: 1 [47996/60000 (80%)]\tLoss: 1825.444336 \n",
      "Train Epoch: 1 [48396/60000 (81%)]\tLoss: 1825.459229 \n",
      "Train Epoch: 1 [48796/60000 (81%)]\tLoss: 1825.467041 \n",
      "Train Epoch: 1 [49196/60000 (82%)]\tLoss: 1825.471436 \n",
      "Train Epoch: 1 [49596/60000 (83%)]\tLoss: 1825.494873 \n",
      "Train Epoch: 1 [49996/60000 (83%)]\tLoss: 1825.446533 \n",
      "Train Epoch: 1 [50396/60000 (84%)]\tLoss: 1825.467285 \n",
      "Train Epoch: 1 [50796/60000 (85%)]\tLoss: 1825.410889 \n",
      "Train Epoch: 1 [51196/60000 (85%)]\tLoss: 1825.482178 \n",
      "Train Epoch: 1 [51596/60000 (86%)]\tLoss: 1825.506592 \n",
      "Train Epoch: 1 [51996/60000 (87%)]\tLoss: 1825.476074 \n",
      "Train Epoch: 1 [52396/60000 (87%)]\tLoss: 1825.474854 \n",
      "Train Epoch: 1 [52796/60000 (88%)]\tLoss: 1825.505859 \n",
      "Train Epoch: 1 [53196/60000 (89%)]\tLoss: 1825.468506 \n",
      "Train Epoch: 1 [53596/60000 (89%)]\tLoss: 1825.524902 \n",
      "Train Epoch: 1 [53996/60000 (90%)]\tLoss: 1825.468018 \n",
      "Train Epoch: 1 [54396/60000 (91%)]\tLoss: 1825.490234 \n",
      "Train Epoch: 1 [54796/60000 (91%)]\tLoss: 1825.472412 \n",
      "Train Epoch: 1 [55196/60000 (92%)]\tLoss: 1825.514893 \n",
      "Train Epoch: 1 [55596/60000 (93%)]\tLoss: 1825.497803 \n",
      "Train Epoch: 1 [55996/60000 (93%)]\tLoss: 1825.446045 \n",
      "Train Epoch: 1 [56396/60000 (94%)]\tLoss: 1825.506592 \n",
      "Train Epoch: 1 [56796/60000 (95%)]\tLoss: 1825.480957 \n",
      "Train Epoch: 1 [57196/60000 (95%)]\tLoss: 1825.442383 \n",
      "Train Epoch: 1 [57596/60000 (96%)]\tLoss: 1825.490479 \n",
      "Train Epoch: 1 [57996/60000 (97%)]\tLoss: 1825.493652 \n",
      "Train Epoch: 1 [58396/60000 (97%)]\tLoss: 1825.502686 \n",
      "Train Epoch: 1 [58796/60000 (98%)]\tLoss: 1825.461182 \n",
      "Train Epoch: 1 [59196/60000 (99%)]\tLoss: 1825.458252 \n",
      "Train Epoch: 1 [59596/60000 (99%)]\tLoss: 1825.459473 \n",
      "Train Epoch: 1 [59996/60000 (100%)]\tLoss: 1825.497803 \n",
      "Train Epoch: 2 [396/60000 (1%)]\tLoss: 1825.508301 \n",
      "Train Epoch: 2 [796/60000 (1%)]\tLoss: 1825.448486 \n",
      "Train Epoch: 2 [1196/60000 (2%)]\tLoss: 1825.455322 \n",
      "Train Epoch: 2 [1596/60000 (3%)]\tLoss: 1825.500977 \n",
      "Train Epoch: 2 [1996/60000 (3%)]\tLoss: 1825.449219 \n",
      "Train Epoch: 2 [2396/60000 (4%)]\tLoss: 1825.473877 \n",
      "Train Epoch: 2 [2796/60000 (5%)]\tLoss: 1825.484619 \n",
      "Train Epoch: 2 [3196/60000 (5%)]\tLoss: 1825.494141 \n",
      "Train Epoch: 2 [3596/60000 (6%)]\tLoss: 1825.477295 \n",
      "Train Epoch: 2 [3996/60000 (7%)]\tLoss: 1825.492432 \n",
      "Train Epoch: 2 [4396/60000 (7%)]\tLoss: 1825.475586 \n",
      "Train Epoch: 2 [4796/60000 (8%)]\tLoss: 1825.495361 \n",
      "Train Epoch: 2 [5196/60000 (9%)]\tLoss: 1825.457520 \n",
      "Train Epoch: 2 [5596/60000 (9%)]\tLoss: 1825.495361 \n",
      "Train Epoch: 2 [5996/60000 (10%)]\tLoss: 1825.471924 \n",
      "Train Epoch: 2 [6396/60000 (11%)]\tLoss: 1825.479248 \n",
      "Train Epoch: 2 [6796/60000 (11%)]\tLoss: 1825.505127 \n",
      "Train Epoch: 2 [7196/60000 (12%)]\tLoss: 1825.479980 \n",
      "Train Epoch: 2 [7596/60000 (13%)]\tLoss: 1825.484863 \n",
      "Train Epoch: 2 [7996/60000 (13%)]\tLoss: 1825.409668 \n",
      "Train Epoch: 2 [8396/60000 (14%)]\tLoss: 1825.452393 \n",
      "Train Epoch: 2 [8796/60000 (15%)]\tLoss: 1825.510986 \n",
      "Train Epoch: 2 [9196/60000 (15%)]\tLoss: 1825.465332 \n",
      "Train Epoch: 2 [9596/60000 (16%)]\tLoss: 1825.494141 \n",
      "Train Epoch: 2 [9996/60000 (17%)]\tLoss: 1825.480957 \n",
      "Train Epoch: 2 [10396/60000 (17%)]\tLoss: 1825.408936 \n",
      "Train Epoch: 2 [10796/60000 (18%)]\tLoss: 1825.498047 \n",
      "Train Epoch: 2 [11196/60000 (19%)]\tLoss: 1825.469482 \n",
      "Train Epoch: 2 [11596/60000 (19%)]\tLoss: 1825.450928 \n",
      "Train Epoch: 2 [11996/60000 (20%)]\tLoss: 1825.505615 \n",
      "Train Epoch: 2 [12396/60000 (21%)]\tLoss: 1825.486572 \n",
      "Train Epoch: 2 [12796/60000 (21%)]\tLoss: 1825.487305 \n",
      "Train Epoch: 2 [13196/60000 (22%)]\tLoss: 1825.465576 \n",
      "Train Epoch: 2 [13596/60000 (23%)]\tLoss: 1825.466064 \n",
      "Train Epoch: 2 [13996/60000 (23%)]\tLoss: 1825.508545 \n",
      "Train Epoch: 2 [14396/60000 (24%)]\tLoss: 1825.462646 \n",
      "Train Epoch: 2 [14796/60000 (25%)]\tLoss: 1825.503418 \n",
      "Train Epoch: 2 [15196/60000 (25%)]\tLoss: 1825.499268 \n",
      "Train Epoch: 2 [15596/60000 (26%)]\tLoss: 1825.451172 \n",
      "Train Epoch: 2 [15996/60000 (27%)]\tLoss: 1825.408447 \n",
      "Train Epoch: 2 [16396/60000 (27%)]\tLoss: 1825.456299 \n",
      "Train Epoch: 2 [16796/60000 (28%)]\tLoss: 1825.502441 \n",
      "Train Epoch: 2 [17196/60000 (29%)]\tLoss: 1825.496094 \n",
      "Train Epoch: 2 [17596/60000 (29%)]\tLoss: 1825.486816 \n",
      "Train Epoch: 2 [17996/60000 (30%)]\tLoss: 1825.491699 \n",
      "Train Epoch: 2 [18396/60000 (31%)]\tLoss: 1825.474609 \n",
      "Train Epoch: 2 [18796/60000 (31%)]\tLoss: 1825.486084 \n",
      "Train Epoch: 2 [19196/60000 (32%)]\tLoss: 1825.481689 \n",
      "Train Epoch: 2 [19596/60000 (33%)]\tLoss: 1825.450195 \n",
      "Train Epoch: 2 [19996/60000 (33%)]\tLoss: 1825.501465 \n",
      "Train Epoch: 2 [20396/60000 (34%)]\tLoss: 1825.476807 \n",
      "Train Epoch: 2 [20796/60000 (35%)]\tLoss: 1825.437256 \n",
      "Train Epoch: 2 [21196/60000 (35%)]\tLoss: 1825.453613 \n",
      "Train Epoch: 2 [21596/60000 (36%)]\tLoss: 1825.478027 \n",
      "Train Epoch: 2 [21996/60000 (37%)]\tLoss: 1825.495117 \n",
      "Train Epoch: 2 [22396/60000 (37%)]\tLoss: 1825.475830 \n",
      "Train Epoch: 2 [22796/60000 (38%)]\tLoss: 1825.480957 \n",
      "Train Epoch: 2 [23196/60000 (39%)]\tLoss: 1825.465088 \n",
      "Train Epoch: 2 [23596/60000 (39%)]\tLoss: 1825.500732 \n",
      "Train Epoch: 2 [23996/60000 (40%)]\tLoss: 1825.493896 \n",
      "Train Epoch: 2 [24396/60000 (41%)]\tLoss: 1825.498047 \n",
      "Train Epoch: 2 [24796/60000 (41%)]\tLoss: 1825.459961 \n",
      "Train Epoch: 2 [25196/60000 (42%)]\tLoss: 1825.479492 \n",
      "Train Epoch: 2 [25596/60000 (43%)]\tLoss: 1825.463379 \n",
      "Train Epoch: 2 [25996/60000 (43%)]\tLoss: 1825.479004 \n",
      "Train Epoch: 2 [26396/60000 (44%)]\tLoss: 1825.508301 \n",
      "Train Epoch: 2 [26796/60000 (45%)]\tLoss: 1825.500244 \n",
      "Train Epoch: 2 [27196/60000 (45%)]\tLoss: 1825.482178 \n",
      "Train Epoch: 2 [27596/60000 (46%)]\tLoss: 1825.512207 \n",
      "Train Epoch: 2 [27996/60000 (47%)]\tLoss: 1825.441650 \n",
      "Train Epoch: 2 [28396/60000 (47%)]\tLoss: 1825.500000 \n",
      "Train Epoch: 2 [28796/60000 (48%)]\tLoss: 1825.440674 \n",
      "Train Epoch: 2 [29196/60000 (49%)]\tLoss: 1825.440674 \n",
      "Train Epoch: 2 [29596/60000 (49%)]\tLoss: 1825.493652 \n",
      "Train Epoch: 2 [29996/60000 (50%)]\tLoss: 1825.502197 \n",
      "Train Epoch: 2 [30396/60000 (51%)]\tLoss: 1825.477783 \n",
      "Train Epoch: 2 [30796/60000 (51%)]\tLoss: 1825.488770 \n",
      "Train Epoch: 2 [31196/60000 (52%)]\tLoss: 1825.483154 \n",
      "Train Epoch: 2 [31596/60000 (53%)]\tLoss: 1825.499756 \n",
      "Train Epoch: 2 [31996/60000 (53%)]\tLoss: 1825.497803 \n",
      "Train Epoch: 2 [32396/60000 (54%)]\tLoss: 1825.464600 \n",
      "Train Epoch: 2 [32796/60000 (55%)]\tLoss: 1825.498291 \n",
      "Train Epoch: 2 [33196/60000 (55%)]\tLoss: 1825.496338 \n",
      "Train Epoch: 2 [33596/60000 (56%)]\tLoss: 1825.482178 \n",
      "Train Epoch: 2 [33996/60000 (57%)]\tLoss: 1825.433350 \n",
      "Train Epoch: 2 [34396/60000 (57%)]\tLoss: 1825.461670 \n",
      "Train Epoch: 2 [34796/60000 (58%)]\tLoss: 1825.482666 \n",
      "Train Epoch: 2 [35196/60000 (59%)]\tLoss: 1825.475830 \n",
      "Train Epoch: 2 [35596/60000 (59%)]\tLoss: 1825.506348 \n",
      "Train Epoch: 2 [35996/60000 (60%)]\tLoss: 1825.518799 \n",
      "Train Epoch: 2 [36396/60000 (61%)]\tLoss: 1825.484375 \n",
      "Train Epoch: 2 [36796/60000 (61%)]\tLoss: 1825.495361 \n",
      "Train Epoch: 2 [37196/60000 (62%)]\tLoss: 1825.499268 \n",
      "Train Epoch: 2 [37596/60000 (63%)]\tLoss: 1825.506104 \n",
      "Train Epoch: 2 [37996/60000 (63%)]\tLoss: 1825.505371 \n",
      "Train Epoch: 2 [38396/60000 (64%)]\tLoss: 1825.468262 \n",
      "Train Epoch: 2 [38796/60000 (65%)]\tLoss: 1825.458984 \n",
      "Train Epoch: 2 [39196/60000 (65%)]\tLoss: 1825.450684 \n",
      "Train Epoch: 2 [39596/60000 (66%)]\tLoss: 1825.502686 \n",
      "Train Epoch: 2 [39996/60000 (67%)]\tLoss: 1825.484619 \n",
      "Train Epoch: 2 [40396/60000 (67%)]\tLoss: 1825.481934 \n",
      "Train Epoch: 2 [40796/60000 (68%)]\tLoss: 1825.425049 \n",
      "Train Epoch: 2 [41196/60000 (69%)]\tLoss: 1825.474121 \n",
      "Train Epoch: 2 [41596/60000 (69%)]\tLoss: 1825.492432 \n",
      "Train Epoch: 2 [41996/60000 (70%)]\tLoss: 1825.511963 \n",
      "Train Epoch: 2 [42396/60000 (71%)]\tLoss: 1825.485352 \n",
      "Train Epoch: 2 [42796/60000 (71%)]\tLoss: 1825.454102 \n",
      "Train Epoch: 2 [43196/60000 (72%)]\tLoss: 1825.503662 \n",
      "Train Epoch: 2 [43596/60000 (73%)]\tLoss: 1825.452148 \n",
      "Train Epoch: 2 [43996/60000 (73%)]\tLoss: 1825.487061 \n",
      "Train Epoch: 2 [44396/60000 (74%)]\tLoss: 1825.491943 \n",
      "Train Epoch: 2 [44796/60000 (75%)]\tLoss: 1825.401855 \n",
      "Train Epoch: 2 [45196/60000 (75%)]\tLoss: 1825.487793 \n",
      "Train Epoch: 2 [45596/60000 (76%)]\tLoss: 1825.484131 \n",
      "Train Epoch: 2 [45996/60000 (77%)]\tLoss: 1825.490234 \n",
      "Train Epoch: 2 [46396/60000 (77%)]\tLoss: 1825.476318 \n",
      "Train Epoch: 2 [46796/60000 (78%)]\tLoss: 1825.461182 \n",
      "Train Epoch: 2 [47196/60000 (79%)]\tLoss: 1825.503662 \n",
      "Train Epoch: 2 [47596/60000 (79%)]\tLoss: 1825.483398 \n",
      "Train Epoch: 2 [47996/60000 (80%)]\tLoss: 1825.482422 \n",
      "Train Epoch: 2 [48396/60000 (81%)]\tLoss: 1825.450684 \n",
      "Train Epoch: 2 [48796/60000 (81%)]\tLoss: 1825.471680 \n",
      "Train Epoch: 2 [49196/60000 (82%)]\tLoss: 1825.418945 \n",
      "Train Epoch: 2 [49596/60000 (83%)]\tLoss: 1825.495361 \n",
      "Train Epoch: 2 [49996/60000 (83%)]\tLoss: 1825.499268 \n",
      "Train Epoch: 2 [50396/60000 (84%)]\tLoss: 1825.486328 \n",
      "Train Epoch: 2 [50796/60000 (85%)]\tLoss: 1825.442871 \n",
      "Train Epoch: 2 [51196/60000 (85%)]\tLoss: 1825.487549 \n",
      "Train Epoch: 2 [51596/60000 (86%)]\tLoss: 1825.440430 \n",
      "Train Epoch: 2 [51996/60000 (87%)]\tLoss: 1825.488281 \n",
      "Train Epoch: 2 [52396/60000 (87%)]\tLoss: 1825.386475 \n",
      "Train Epoch: 2 [52796/60000 (88%)]\tLoss: 1825.489258 \n",
      "Train Epoch: 2 [53196/60000 (89%)]\tLoss: 1825.500732 \n",
      "Train Epoch: 2 [53596/60000 (89%)]\tLoss: 1825.469727 \n",
      "Train Epoch: 2 [53996/60000 (90%)]\tLoss: 1825.453125 \n",
      "Train Epoch: 2 [54396/60000 (91%)]\tLoss: 1825.461426 \n",
      "Train Epoch: 2 [54796/60000 (91%)]\tLoss: 1825.446045 \n",
      "Train Epoch: 2 [55196/60000 (92%)]\tLoss: 1825.492676 \n",
      "Train Epoch: 2 [55596/60000 (93%)]\tLoss: 1825.471436 \n",
      "Train Epoch: 2 [55996/60000 (93%)]\tLoss: 1825.524902 \n",
      "Train Epoch: 2 [56396/60000 (94%)]\tLoss: 1825.458252 \n",
      "Train Epoch: 2 [56796/60000 (95%)]\tLoss: 1825.499268 \n",
      "Train Epoch: 2 [57196/60000 (95%)]\tLoss: 1825.460449 \n",
      "Train Epoch: 2 [57596/60000 (96%)]\tLoss: 1825.485840 \n",
      "Train Epoch: 2 [57996/60000 (97%)]\tLoss: 1825.477539 \n",
      "Train Epoch: 2 [58396/60000 (97%)]\tLoss: 1825.473633 \n",
      "Train Epoch: 2 [58796/60000 (98%)]\tLoss: 1825.476807 \n",
      "Train Epoch: 2 [59196/60000 (99%)]\tLoss: 1825.451416 \n",
      "Train Epoch: 2 [59596/60000 (99%)]\tLoss: 1825.479736 \n",
      "Train Epoch: 2 [59996/60000 (100%)]\tLoss: 1825.505371 \n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "model.train()\n",
    "step = 0 \n",
    "loss_history = []\n",
    "for epoch in range(1, 3):\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_0 = criterion(output, target)\n",
    "        loss_1 = call_pr_loss(list(model.l1.weight) , m_784)\n",
    "        loss_2 = call_pr_loss(list(model.l2.weight) , m_200)\n",
    "        loss_3 = call_pr_loss(list(model.l3.weight) , m_200)\n",
    "        loss_4 = call_pr_loss(list(model.l4.weight) , m_200)\n",
    "        loss_5 = call_pr_loss(list(model.l5.weight) , m_200)\n",
    "        loss_6 = call_pr_loss(list(model.l6.weight) , m_200)\n",
    "        loss_f = call_pr_loss(list(model.lf.weight) , m_200)\n",
    "        \n",
    "        \n",
    "        loss = loss_0 + loss_1+ loss_2+ loss_3 + loss_4 + loss_f + loss_5 + loss_6\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        loss_history.append(loss.item())\n",
    "        if step % 100  == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} '.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.item()))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5753, Accuracy: 6742/60000 (11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in trainloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(trainloader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(trainloader.dataset),\n",
    "    100. * correct / len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (l1): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (r1): ReLU()\n",
       "  (l2): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r2): ReLU()\n",
       "  (l3): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r3): ReLU()\n",
       "  (l4): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r4): ReLU()\n",
       "  (l5): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r5): ReLU()\n",
       "  (l6): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r6): ReLU()\n",
       "  (l7): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r7): ReLU()\n",
       "  (lf): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('l1', nn.Linear(784, 200))\n",
    "model.add_module('r1', nn.ReLU())\n",
    "model.add_module('l2', nn.Linear(200, 200))\n",
    "model.add_module('r2', nn.ReLU())\n",
    "model.add_module('l3', nn.Linear(200, 200))\n",
    "model.add_module('r3', nn.ReLU())\n",
    "model.add_module('l4', nn.Linear(200, 200))\n",
    "model.add_module('r4', nn.ReLU())\n",
    "model.add_module('l5', nn.Linear(200, 200))\n",
    "model.add_module('r5', nn.ReLU())\n",
    "model.add_module('l6', nn.Linear(200, 200))\n",
    "model.add_module('r6', nn.ReLU())\n",
    "model.add_module('l7', nn.Linear(200, 200))\n",
    "model.add_module('r7', nn.ReLU())\n",
    "model.add_module('lf', nn.Linear(200, 10))\n",
    "\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [396/60000 (1%)]\tLoss: 2009.316895 \n",
      "Train Epoch: 1 [796/60000 (1%)]\tLoss: 2009.271973 \n",
      "Train Epoch: 1 [1196/60000 (2%)]\tLoss: 2009.296387 \n",
      "Train Epoch: 1 [1596/60000 (3%)]\tLoss: 2009.255493 \n",
      "Train Epoch: 1 [1996/60000 (3%)]\tLoss: 2009.305908 \n",
      "Train Epoch: 1 [2396/60000 (4%)]\tLoss: 2009.235474 \n",
      "Train Epoch: 1 [2796/60000 (5%)]\tLoss: 2009.284180 \n",
      "Train Epoch: 1 [3196/60000 (5%)]\tLoss: 2009.280273 \n",
      "Train Epoch: 1 [3596/60000 (6%)]\tLoss: 2009.282471 \n",
      "Train Epoch: 1 [3996/60000 (7%)]\tLoss: 2009.263428 \n",
      "Train Epoch: 1 [4396/60000 (7%)]\tLoss: 2009.218018 \n",
      "Train Epoch: 1 [4796/60000 (8%)]\tLoss: 2009.296387 \n",
      "Train Epoch: 1 [5196/60000 (9%)]\tLoss: 2009.278076 \n",
      "Train Epoch: 1 [5596/60000 (9%)]\tLoss: 2009.234009 \n",
      "Train Epoch: 1 [5996/60000 (10%)]\tLoss: 2009.213867 \n",
      "Train Epoch: 1 [6396/60000 (11%)]\tLoss: 2009.253540 \n",
      "Train Epoch: 1 [6796/60000 (11%)]\tLoss: 2009.281006 \n",
      "Train Epoch: 1 [7196/60000 (12%)]\tLoss: 2009.251587 \n",
      "Train Epoch: 1 [7596/60000 (13%)]\tLoss: 2009.247803 \n",
      "Train Epoch: 1 [7996/60000 (13%)]\tLoss: 2009.264160 \n",
      "Train Epoch: 1 [8396/60000 (14%)]\tLoss: 2009.249878 \n",
      "Train Epoch: 1 [8796/60000 (15%)]\tLoss: 2009.292114 \n",
      "Train Epoch: 1 [9196/60000 (15%)]\tLoss: 2009.345703 \n",
      "Train Epoch: 1 [9596/60000 (16%)]\tLoss: 2009.259521 \n",
      "Train Epoch: 1 [9996/60000 (17%)]\tLoss: 2009.246094 \n",
      "Train Epoch: 1 [10396/60000 (17%)]\tLoss: 2009.267090 \n",
      "Train Epoch: 1 [10796/60000 (18%)]\tLoss: 2009.232422 \n",
      "Train Epoch: 1 [11196/60000 (19%)]\tLoss: 2009.232544 \n",
      "Train Epoch: 1 [11596/60000 (19%)]\tLoss: 2009.278564 \n",
      "Train Epoch: 1 [11996/60000 (20%)]\tLoss: 2009.249512 \n",
      "Train Epoch: 1 [12396/60000 (21%)]\tLoss: 2009.271240 \n",
      "Train Epoch: 1 [12796/60000 (21%)]\tLoss: 2009.269531 \n",
      "Train Epoch: 1 [13196/60000 (22%)]\tLoss: 2009.232666 \n",
      "Train Epoch: 1 [13596/60000 (23%)]\tLoss: 2009.236572 \n",
      "Train Epoch: 1 [13996/60000 (23%)]\tLoss: 2009.284668 \n",
      "Train Epoch: 1 [14396/60000 (24%)]\tLoss: 2009.249756 \n",
      "Train Epoch: 1 [14796/60000 (25%)]\tLoss: 2009.270508 \n",
      "Train Epoch: 1 [15196/60000 (25%)]\tLoss: 2009.300171 \n",
      "Train Epoch: 1 [15596/60000 (26%)]\tLoss: 2009.249023 \n",
      "Train Epoch: 1 [15996/60000 (27%)]\tLoss: 2009.269043 \n",
      "Train Epoch: 1 [16396/60000 (27%)]\tLoss: 2009.304932 \n",
      "Train Epoch: 1 [16796/60000 (28%)]\tLoss: 2009.265869 \n",
      "Train Epoch: 1 [17196/60000 (29%)]\tLoss: 2009.248535 \n",
      "Train Epoch: 1 [17596/60000 (29%)]\tLoss: 2009.274170 \n",
      "Train Epoch: 1 [17996/60000 (30%)]\tLoss: 2009.214844 \n",
      "Train Epoch: 1 [18396/60000 (31%)]\tLoss: 2009.305420 \n",
      "Train Epoch: 1 [18796/60000 (31%)]\tLoss: 2009.233398 \n",
      "Train Epoch: 1 [19196/60000 (32%)]\tLoss: 2009.317627 \n",
      "Train Epoch: 1 [19596/60000 (33%)]\tLoss: 2009.236084 \n",
      "Train Epoch: 1 [19996/60000 (33%)]\tLoss: 2009.203613 \n",
      "Train Epoch: 1 [20396/60000 (34%)]\tLoss: 2009.245605 \n",
      "Train Epoch: 1 [20796/60000 (35%)]\tLoss: 2009.270020 \n",
      "Train Epoch: 1 [21196/60000 (35%)]\tLoss: 2009.287842 \n",
      "Train Epoch: 1 [21596/60000 (36%)]\tLoss: 2009.270020 \n",
      "Train Epoch: 1 [21996/60000 (37%)]\tLoss: 2009.203613 \n",
      "Train Epoch: 1 [22396/60000 (37%)]\tLoss: 2009.254883 \n",
      "Train Epoch: 1 [22796/60000 (38%)]\tLoss: 2009.264893 \n",
      "Train Epoch: 1 [23196/60000 (39%)]\tLoss: 2009.188965 \n",
      "Train Epoch: 1 [23596/60000 (39%)]\tLoss: 2009.301758 \n",
      "Train Epoch: 1 [23996/60000 (40%)]\tLoss: 2009.264404 \n",
      "Train Epoch: 1 [24396/60000 (41%)]\tLoss: 2009.175049 \n",
      "Train Epoch: 1 [24796/60000 (41%)]\tLoss: 2009.259521 \n",
      "Train Epoch: 1 [25196/60000 (42%)]\tLoss: 2009.250977 \n",
      "Train Epoch: 1 [25596/60000 (43%)]\tLoss: 2009.339355 \n",
      "Train Epoch: 1 [25996/60000 (43%)]\tLoss: 2009.289795 \n",
      "Train Epoch: 1 [26396/60000 (44%)]\tLoss: 2009.292480 \n",
      "Train Epoch: 1 [26796/60000 (45%)]\tLoss: 2009.308105 \n",
      "Train Epoch: 1 [27196/60000 (45%)]\tLoss: 2009.234131 \n",
      "Train Epoch: 1 [27596/60000 (46%)]\tLoss: 2009.230713 \n",
      "Train Epoch: 1 [27996/60000 (47%)]\tLoss: 2009.239990 \n",
      "Train Epoch: 1 [28396/60000 (47%)]\tLoss: 2009.241211 \n",
      "Train Epoch: 1 [28796/60000 (48%)]\tLoss: 2009.227783 \n",
      "Train Epoch: 1 [29196/60000 (49%)]\tLoss: 2009.283691 \n",
      "Train Epoch: 1 [29596/60000 (49%)]\tLoss: 2009.257324 \n",
      "Train Epoch: 1 [29996/60000 (50%)]\tLoss: 2009.255859 \n",
      "Train Epoch: 1 [30396/60000 (51%)]\tLoss: 2009.226074 \n",
      "Train Epoch: 1 [30796/60000 (51%)]\tLoss: 2009.280518 \n",
      "Train Epoch: 1 [31196/60000 (52%)]\tLoss: 2009.299561 \n",
      "Train Epoch: 1 [31596/60000 (53%)]\tLoss: 2009.260010 \n",
      "Train Epoch: 1 [31996/60000 (53%)]\tLoss: 2009.251709 \n",
      "Train Epoch: 1 [32396/60000 (54%)]\tLoss: 2009.199951 \n",
      "Train Epoch: 1 [32796/60000 (55%)]\tLoss: 2009.269775 \n",
      "Train Epoch: 1 [33196/60000 (55%)]\tLoss: 2009.278809 \n",
      "Train Epoch: 1 [33596/60000 (56%)]\tLoss: 2009.270996 \n",
      "Train Epoch: 1 [33996/60000 (57%)]\tLoss: 2009.248291 \n",
      "Train Epoch: 1 [34396/60000 (57%)]\tLoss: 2009.250244 \n",
      "Train Epoch: 1 [34796/60000 (58%)]\tLoss: 2009.270996 \n",
      "Train Epoch: 1 [35196/60000 (59%)]\tLoss: 2009.260742 \n",
      "Train Epoch: 1 [35596/60000 (59%)]\tLoss: 2009.233887 \n",
      "Train Epoch: 1 [35996/60000 (60%)]\tLoss: 2009.247803 \n",
      "Train Epoch: 1 [36396/60000 (61%)]\tLoss: 2009.300293 \n",
      "Train Epoch: 1 [36796/60000 (61%)]\tLoss: 2009.269531 \n",
      "Train Epoch: 1 [37196/60000 (62%)]\tLoss: 2009.246338 \n",
      "Train Epoch: 1 [37596/60000 (63%)]\tLoss: 2009.269775 \n",
      "Train Epoch: 1 [37996/60000 (63%)]\tLoss: 2009.307129 \n",
      "Train Epoch: 1 [38396/60000 (64%)]\tLoss: 2009.225342 \n",
      "Train Epoch: 1 [38796/60000 (65%)]\tLoss: 2009.223877 \n",
      "Train Epoch: 1 [39196/60000 (65%)]\tLoss: 2009.287842 \n",
      "Train Epoch: 1 [39596/60000 (66%)]\tLoss: 2009.249756 \n",
      "Train Epoch: 1 [39996/60000 (67%)]\tLoss: 2009.329590 \n",
      "Train Epoch: 1 [40396/60000 (67%)]\tLoss: 2009.252441 \n",
      "Train Epoch: 1 [40796/60000 (68%)]\tLoss: 2009.298584 \n",
      "Train Epoch: 1 [41196/60000 (69%)]\tLoss: 2009.263428 \n",
      "Train Epoch: 1 [41596/60000 (69%)]\tLoss: 2009.149902 \n",
      "Train Epoch: 1 [41996/60000 (70%)]\tLoss: 2009.276367 \n",
      "Train Epoch: 1 [42396/60000 (71%)]\tLoss: 2009.292480 \n",
      "Train Epoch: 1 [42796/60000 (71%)]\tLoss: 2009.250244 \n",
      "Train Epoch: 1 [43196/60000 (72%)]\tLoss: 2009.271484 \n",
      "Train Epoch: 1 [43596/60000 (73%)]\tLoss: 2009.218994 \n",
      "Train Epoch: 1 [43996/60000 (73%)]\tLoss: 2009.237305 \n",
      "Train Epoch: 1 [44396/60000 (74%)]\tLoss: 2009.267090 \n",
      "Train Epoch: 1 [44796/60000 (75%)]\tLoss: 2009.302490 \n",
      "Train Epoch: 1 [45196/60000 (75%)]\tLoss: 2009.242432 \n",
      "Train Epoch: 1 [45596/60000 (76%)]\tLoss: 2009.250732 \n",
      "Train Epoch: 1 [45996/60000 (77%)]\tLoss: 2009.249756 \n",
      "Train Epoch: 1 [46396/60000 (77%)]\tLoss: 2009.284668 \n",
      "Train Epoch: 1 [46796/60000 (78%)]\tLoss: 2009.282227 \n",
      "Train Epoch: 1 [47196/60000 (79%)]\tLoss: 2009.242920 \n",
      "Train Epoch: 1 [47596/60000 (79%)]\tLoss: 2009.259033 \n",
      "Train Epoch: 1 [47996/60000 (80%)]\tLoss: 2009.233154 \n",
      "Train Epoch: 1 [48396/60000 (81%)]\tLoss: 2009.280762 \n",
      "Train Epoch: 1 [48796/60000 (81%)]\tLoss: 2009.228516 \n",
      "Train Epoch: 1 [49196/60000 (82%)]\tLoss: 2009.289062 \n",
      "Train Epoch: 1 [49596/60000 (83%)]\tLoss: 2009.241943 \n",
      "Train Epoch: 1 [49996/60000 (83%)]\tLoss: 2009.279541 \n",
      "Train Epoch: 1 [50396/60000 (84%)]\tLoss: 2009.197021 \n",
      "Train Epoch: 1 [50796/60000 (85%)]\tLoss: 2009.276855 \n",
      "Train Epoch: 1 [51196/60000 (85%)]\tLoss: 2009.243652 \n",
      "Train Epoch: 1 [51596/60000 (86%)]\tLoss: 2009.235352 \n",
      "Train Epoch: 1 [51996/60000 (87%)]\tLoss: 2009.265381 \n",
      "Train Epoch: 1 [52396/60000 (87%)]\tLoss: 2009.244385 \n",
      "Train Epoch: 1 [52796/60000 (88%)]\tLoss: 2009.337402 \n",
      "Train Epoch: 1 [53196/60000 (89%)]\tLoss: 2009.254883 \n",
      "Train Epoch: 1 [53596/60000 (89%)]\tLoss: 2009.249756 \n",
      "Train Epoch: 1 [53996/60000 (90%)]\tLoss: 2009.250244 \n",
      "Train Epoch: 1 [54396/60000 (91%)]\tLoss: 2009.241455 \n",
      "Train Epoch: 1 [54796/60000 (91%)]\tLoss: 2009.250732 \n",
      "Train Epoch: 1 [55196/60000 (92%)]\tLoss: 2009.241211 \n",
      "Train Epoch: 1 [55596/60000 (93%)]\tLoss: 2009.234619 \n",
      "Train Epoch: 1 [55996/60000 (93%)]\tLoss: 2009.256592 \n",
      "Train Epoch: 1 [56396/60000 (94%)]\tLoss: 2009.277832 \n",
      "Train Epoch: 1 [56796/60000 (95%)]\tLoss: 2009.298828 \n",
      "Train Epoch: 1 [57196/60000 (95%)]\tLoss: 2009.242188 \n",
      "Train Epoch: 1 [57596/60000 (96%)]\tLoss: 2009.296143 \n",
      "Train Epoch: 1 [57996/60000 (97%)]\tLoss: 2009.254395 \n",
      "Train Epoch: 1 [58396/60000 (97%)]\tLoss: 2009.293945 \n",
      "Train Epoch: 1 [58796/60000 (98%)]\tLoss: 2009.304688 \n",
      "Train Epoch: 1 [59196/60000 (99%)]\tLoss: 2009.242188 \n",
      "Train Epoch: 1 [59596/60000 (99%)]\tLoss: 2009.272461 \n",
      "Train Epoch: 1 [59996/60000 (100%)]\tLoss: 2009.284180 \n",
      "Train Epoch: 2 [396/60000 (1%)]\tLoss: 2009.263916 \n",
      "Train Epoch: 2 [796/60000 (1%)]\tLoss: 2009.264893 \n",
      "Train Epoch: 2 [1196/60000 (2%)]\tLoss: 2009.260254 \n",
      "Train Epoch: 2 [1596/60000 (3%)]\tLoss: 2009.272705 \n",
      "Train Epoch: 2 [1996/60000 (3%)]\tLoss: 2009.271729 \n",
      "Train Epoch: 2 [2396/60000 (4%)]\tLoss: 2009.207764 \n",
      "Train Epoch: 2 [2796/60000 (5%)]\tLoss: 2009.270996 \n",
      "Train Epoch: 2 [3196/60000 (5%)]\tLoss: 2009.252441 \n",
      "Train Epoch: 2 [3596/60000 (6%)]\tLoss: 2009.272461 \n",
      "Train Epoch: 2 [3996/60000 (7%)]\tLoss: 2009.268066 \n",
      "Train Epoch: 2 [4396/60000 (7%)]\tLoss: 2009.261475 \n",
      "Train Epoch: 2 [4796/60000 (8%)]\tLoss: 2009.279541 \n",
      "Train Epoch: 2 [5196/60000 (9%)]\tLoss: 2009.270508 \n",
      "Train Epoch: 2 [5596/60000 (9%)]\tLoss: 2009.212646 \n",
      "Train Epoch: 2 [5996/60000 (10%)]\tLoss: 2009.278564 \n",
      "Train Epoch: 2 [6396/60000 (11%)]\tLoss: 2009.260010 \n",
      "Train Epoch: 2 [6796/60000 (11%)]\tLoss: 2009.270996 \n",
      "Train Epoch: 2 [7196/60000 (12%)]\tLoss: 2009.229248 \n",
      "Train Epoch: 2 [7596/60000 (13%)]\tLoss: 2009.265137 \n",
      "Train Epoch: 2 [7996/60000 (13%)]\tLoss: 2009.253174 \n",
      "Train Epoch: 2 [8396/60000 (14%)]\tLoss: 2009.249756 \n",
      "Train Epoch: 2 [8796/60000 (15%)]\tLoss: 2009.272949 \n",
      "Train Epoch: 2 [9196/60000 (15%)]\tLoss: 2009.216309 \n",
      "Train Epoch: 2 [9596/60000 (16%)]\tLoss: 2009.277100 \n",
      "Train Epoch: 2 [9996/60000 (17%)]\tLoss: 2009.295898 \n",
      "Train Epoch: 2 [10396/60000 (17%)]\tLoss: 2009.200928 \n",
      "Train Epoch: 2 [10796/60000 (18%)]\tLoss: 2009.243164 \n",
      "Train Epoch: 2 [11196/60000 (19%)]\tLoss: 2009.267090 \n",
      "Train Epoch: 2 [11596/60000 (19%)]\tLoss: 2009.280273 \n",
      "Train Epoch: 2 [11996/60000 (20%)]\tLoss: 2009.275391 \n",
      "Train Epoch: 2 [12396/60000 (21%)]\tLoss: 2009.257324 \n",
      "Train Epoch: 2 [12796/60000 (21%)]\tLoss: 2009.253418 \n",
      "Train Epoch: 2 [13196/60000 (22%)]\tLoss: 2009.273682 \n",
      "Train Epoch: 2 [13596/60000 (23%)]\tLoss: 2009.250977 \n",
      "Train Epoch: 2 [13996/60000 (23%)]\tLoss: 2009.254883 \n",
      "Train Epoch: 2 [14396/60000 (24%)]\tLoss: 2009.259521 \n",
      "Train Epoch: 2 [14796/60000 (25%)]\tLoss: 2009.291260 \n",
      "Train Epoch: 2 [15196/60000 (25%)]\tLoss: 2009.216797 \n",
      "Train Epoch: 2 [15596/60000 (26%)]\tLoss: 2009.272705 \n",
      "Train Epoch: 2 [15996/60000 (27%)]\tLoss: 2009.268066 \n",
      "Train Epoch: 2 [16396/60000 (27%)]\tLoss: 2009.294189 \n",
      "Train Epoch: 2 [16796/60000 (28%)]\tLoss: 2009.311035 \n",
      "Train Epoch: 2 [17196/60000 (29%)]\tLoss: 2009.235596 \n",
      "Train Epoch: 2 [17596/60000 (29%)]\tLoss: 2009.266846 \n",
      "Train Epoch: 2 [17996/60000 (30%)]\tLoss: 2009.253418 \n",
      "Train Epoch: 2 [18396/60000 (31%)]\tLoss: 2009.282471 \n",
      "Train Epoch: 2 [18796/60000 (31%)]\tLoss: 2009.243408 \n",
      "Train Epoch: 2 [19196/60000 (32%)]\tLoss: 2009.239502 \n",
      "Train Epoch: 2 [19596/60000 (33%)]\tLoss: 2009.302002 \n",
      "Train Epoch: 2 [19996/60000 (33%)]\tLoss: 2009.241943 \n",
      "Train Epoch: 2 [20396/60000 (34%)]\tLoss: 2009.291992 \n",
      "Train Epoch: 2 [20796/60000 (35%)]\tLoss: 2009.253174 \n",
      "Train Epoch: 2 [21196/60000 (35%)]\tLoss: 2009.296631 \n",
      "Train Epoch: 2 [21596/60000 (36%)]\tLoss: 2009.255859 \n",
      "Train Epoch: 2 [21996/60000 (37%)]\tLoss: 2009.220215 \n",
      "Train Epoch: 2 [22396/60000 (37%)]\tLoss: 2009.246094 \n",
      "Train Epoch: 2 [22796/60000 (38%)]\tLoss: 2009.258545 \n",
      "Train Epoch: 2 [23196/60000 (39%)]\tLoss: 2009.266357 \n",
      "Train Epoch: 2 [23596/60000 (39%)]\tLoss: 2009.249512 \n",
      "Train Epoch: 2 [23996/60000 (40%)]\tLoss: 2009.250977 \n",
      "Train Epoch: 2 [24396/60000 (41%)]\tLoss: 2009.273926 \n",
      "Train Epoch: 2 [24796/60000 (41%)]\tLoss: 2009.246826 \n",
      "Train Epoch: 2 [25196/60000 (42%)]\tLoss: 2009.285400 \n",
      "Train Epoch: 2 [25596/60000 (43%)]\tLoss: 2009.236572 \n",
      "Train Epoch: 2 [25996/60000 (43%)]\tLoss: 2009.237305 \n",
      "Train Epoch: 2 [26396/60000 (44%)]\tLoss: 2009.201172 \n",
      "Train Epoch: 2 [26796/60000 (45%)]\tLoss: 2009.292969 \n",
      "Train Epoch: 2 [27196/60000 (45%)]\tLoss: 2009.264404 \n",
      "Train Epoch: 2 [27596/60000 (46%)]\tLoss: 2009.319824 \n",
      "Train Epoch: 2 [27996/60000 (47%)]\tLoss: 2009.226074 \n",
      "Train Epoch: 2 [28396/60000 (47%)]\tLoss: 2009.263672 \n",
      "Train Epoch: 2 [28796/60000 (48%)]\tLoss: 2009.262695 \n",
      "Train Epoch: 2 [29196/60000 (49%)]\tLoss: 2009.309814 \n",
      "Train Epoch: 2 [29596/60000 (49%)]\tLoss: 2009.261719 \n",
      "Train Epoch: 2 [29996/60000 (50%)]\tLoss: 2009.262207 \n",
      "Train Epoch: 2 [30396/60000 (51%)]\tLoss: 2009.278076 \n",
      "Train Epoch: 2 [30796/60000 (51%)]\tLoss: 2009.280029 \n",
      "Train Epoch: 2 [31196/60000 (52%)]\tLoss: 2009.259521 \n",
      "Train Epoch: 2 [31596/60000 (53%)]\tLoss: 2009.295166 \n",
      "Train Epoch: 2 [31996/60000 (53%)]\tLoss: 2009.247314 \n",
      "Train Epoch: 2 [32396/60000 (54%)]\tLoss: 2009.206299 \n",
      "Train Epoch: 2 [32796/60000 (55%)]\tLoss: 2009.245361 \n",
      "Train Epoch: 2 [33196/60000 (55%)]\tLoss: 2009.270264 \n",
      "Train Epoch: 2 [33596/60000 (56%)]\tLoss: 2009.247314 \n",
      "Train Epoch: 2 [33996/60000 (57%)]\tLoss: 2009.249023 \n",
      "Train Epoch: 2 [34396/60000 (57%)]\tLoss: 2009.306396 \n",
      "Train Epoch: 2 [34796/60000 (58%)]\tLoss: 2009.300049 \n",
      "Train Epoch: 2 [35196/60000 (59%)]\tLoss: 2009.269043 \n",
      "Train Epoch: 2 [35596/60000 (59%)]\tLoss: 2009.302734 \n",
      "Train Epoch: 2 [35996/60000 (60%)]\tLoss: 2009.300537 \n",
      "Train Epoch: 2 [36396/60000 (61%)]\tLoss: 2009.276611 \n",
      "Train Epoch: 2 [36796/60000 (61%)]\tLoss: 2009.209229 \n",
      "Train Epoch: 2 [37196/60000 (62%)]\tLoss: 2009.262207 \n",
      "Train Epoch: 2 [37596/60000 (63%)]\tLoss: 2009.216797 \n",
      "Train Epoch: 2 [37996/60000 (63%)]\tLoss: 2009.300781 \n",
      "Train Epoch: 2 [38396/60000 (64%)]\tLoss: 2009.277832 \n",
      "Train Epoch: 2 [38796/60000 (65%)]\tLoss: 2009.296387 \n",
      "Train Epoch: 2 [39196/60000 (65%)]\tLoss: 2009.320312 \n",
      "Train Epoch: 2 [39596/60000 (66%)]\tLoss: 2009.303955 \n",
      "Train Epoch: 2 [39996/60000 (67%)]\tLoss: 2009.278809 \n",
      "Train Epoch: 2 [40396/60000 (67%)]\tLoss: 2009.252441 \n",
      "Train Epoch: 2 [40796/60000 (68%)]\tLoss: 2009.287109 \n",
      "Train Epoch: 2 [41196/60000 (69%)]\tLoss: 2009.241211 \n",
      "Train Epoch: 2 [41596/60000 (69%)]\tLoss: 2009.273438 \n",
      "Train Epoch: 2 [41996/60000 (70%)]\tLoss: 2009.243164 \n",
      "Train Epoch: 2 [42396/60000 (71%)]\tLoss: 2009.244629 \n",
      "Train Epoch: 2 [42796/60000 (71%)]\tLoss: 2009.190918 \n",
      "Train Epoch: 2 [43196/60000 (72%)]\tLoss: 2009.242920 \n",
      "Train Epoch: 2 [43596/60000 (73%)]\tLoss: 2009.279053 \n",
      "Train Epoch: 2 [43996/60000 (73%)]\tLoss: 2009.292480 \n",
      "Train Epoch: 2 [44396/60000 (74%)]\tLoss: 2009.244873 \n",
      "Train Epoch: 2 [44796/60000 (75%)]\tLoss: 2009.268799 \n",
      "Train Epoch: 2 [45196/60000 (75%)]\tLoss: 2009.257568 \n",
      "Train Epoch: 2 [45596/60000 (76%)]\tLoss: 2009.281738 \n",
      "Train Epoch: 2 [45996/60000 (77%)]\tLoss: 2009.254395 \n",
      "Train Epoch: 2 [46396/60000 (77%)]\tLoss: 2009.281006 \n",
      "Train Epoch: 2 [46796/60000 (78%)]\tLoss: 2009.276123 \n",
      "Train Epoch: 2 [47196/60000 (79%)]\tLoss: 2009.290527 \n",
      "Train Epoch: 2 [47596/60000 (79%)]\tLoss: 2009.233154 \n",
      "Train Epoch: 2 [47996/60000 (80%)]\tLoss: 2009.248535 \n",
      "Train Epoch: 2 [48396/60000 (81%)]\tLoss: 2009.258789 \n",
      "Train Epoch: 2 [48796/60000 (81%)]\tLoss: 2009.253418 \n",
      "Train Epoch: 2 [49196/60000 (82%)]\tLoss: 2009.298584 \n",
      "Train Epoch: 2 [49596/60000 (83%)]\tLoss: 2009.296387 \n",
      "Train Epoch: 2 [49996/60000 (83%)]\tLoss: 2009.236572 \n",
      "Train Epoch: 2 [50396/60000 (84%)]\tLoss: 2009.280273 \n",
      "Train Epoch: 2 [50796/60000 (85%)]\tLoss: 2009.265625 \n",
      "Train Epoch: 2 [51196/60000 (85%)]\tLoss: 2009.248047 \n",
      "Train Epoch: 2 [51596/60000 (86%)]\tLoss: 2009.295410 \n",
      "Train Epoch: 2 [51996/60000 (87%)]\tLoss: 2009.323730 \n",
      "Train Epoch: 2 [52396/60000 (87%)]\tLoss: 2009.250000 \n",
      "Train Epoch: 2 [52796/60000 (88%)]\tLoss: 2009.237549 \n",
      "Train Epoch: 2 [53196/60000 (89%)]\tLoss: 2009.256104 \n",
      "Train Epoch: 2 [53596/60000 (89%)]\tLoss: 2009.230225 \n",
      "Train Epoch: 2 [53996/60000 (90%)]\tLoss: 2009.296631 \n",
      "Train Epoch: 2 [54396/60000 (91%)]\tLoss: 2009.281494 \n",
      "Train Epoch: 2 [54796/60000 (91%)]\tLoss: 2009.299805 \n",
      "Train Epoch: 2 [55196/60000 (92%)]\tLoss: 2009.237061 \n",
      "Train Epoch: 2 [55596/60000 (93%)]\tLoss: 2009.225098 \n",
      "Train Epoch: 2 [55996/60000 (93%)]\tLoss: 2009.248291 \n",
      "Train Epoch: 2 [56396/60000 (94%)]\tLoss: 2009.252441 \n",
      "Train Epoch: 2 [56796/60000 (95%)]\tLoss: 2009.258301 \n",
      "Train Epoch: 2 [57196/60000 (95%)]\tLoss: 2009.250244 \n",
      "Train Epoch: 2 [57596/60000 (96%)]\tLoss: 2009.233643 \n",
      "Train Epoch: 2 [57996/60000 (97%)]\tLoss: 2009.326172 \n",
      "Train Epoch: 2 [58396/60000 (97%)]\tLoss: 2009.257812 \n",
      "Train Epoch: 2 [58796/60000 (98%)]\tLoss: 2009.273682 \n",
      "Train Epoch: 2 [59196/60000 (99%)]\tLoss: 2009.259277 \n",
      "Train Epoch: 2 [59596/60000 (99%)]\tLoss: 2009.257568 \n",
      "Train Epoch: 2 [59996/60000 (100%)]\tLoss: 2009.275391 \n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "model.train()\n",
    "step = 0 \n",
    "loss_history = []\n",
    "for epoch in range(1, 3):\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_0 = criterion(output, target)\n",
    "        loss_1 = call_pr_loss(list(model.l1.weight) , m_784)\n",
    "        loss_2 = call_pr_loss(list(model.l2.weight) , m_200)\n",
    "        loss_3 = call_pr_loss(list(model.l3.weight) , m_200)\n",
    "        loss_4 = call_pr_loss(list(model.l4.weight) , m_200)\n",
    "        loss_5 = call_pr_loss(list(model.l5.weight) , m_200)\n",
    "        loss_6 = call_pr_loss(list(model.l6.weight) , m_200)\n",
    "        loss_7 = call_pr_loss(list(model.l7.weight) , m_200)\n",
    "        loss_f = call_pr_loss(list(model.lf.weight) , m_200)\n",
    "        \n",
    "        \n",
    "        loss = loss_0 + loss_1+ loss_2+ loss_3 + loss_4 + loss_f + loss_5 + loss_6 + loss_7\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        loss_history.append(loss.item())\n",
    "        if step % 100  == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} '.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.item()))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5753, Accuracy: 6742/60000 (11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in trainloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(trainloader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(trainloader.dataset),\n",
    "    100. * correct / len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (l1): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (r1): ReLU()\n",
       "  (l2): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r2): ReLU()\n",
       "  (l3): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r3): ReLU()\n",
       "  (l4): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r4): ReLU()\n",
       "  (l5): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r5): ReLU()\n",
       "  (l6): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r6): ReLU()\n",
       "  (l7): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r7): ReLU()\n",
       "  (l8): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (r8): ReLU()\n",
       "  (lf): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('l1', nn.Linear(784, 200))\n",
    "model.add_module('r1', nn.ReLU())\n",
    "model.add_module('l2', nn.Linear(200, 200))\n",
    "model.add_module('r2', nn.ReLU())\n",
    "model.add_module('l3', nn.Linear(200, 200))\n",
    "model.add_module('r3', nn.ReLU())\n",
    "model.add_module('l4', nn.Linear(200, 200))\n",
    "model.add_module('r4', nn.ReLU())\n",
    "model.add_module('l5', nn.Linear(200, 200))\n",
    "model.add_module('r5', nn.ReLU())\n",
    "model.add_module('l6', nn.Linear(200, 200))\n",
    "model.add_module('r6', nn.ReLU())\n",
    "model.add_module('l7', nn.Linear(200, 200))\n",
    "model.add_module('r7', nn.ReLU())\n",
    "model.add_module('l8', nn.Linear(200, 200))\n",
    "model.add_module('r8', nn.ReLU())\n",
    "model.add_module('lf', nn.Linear(200, 10))\n",
    "\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [396/60000 (1%)]\tLoss: 2193.086914 \n",
      "Train Epoch: 1 [796/60000 (1%)]\tLoss: 2193.056885 \n",
      "Train Epoch: 1 [1196/60000 (2%)]\tLoss: 2193.042480 \n",
      "Train Epoch: 1 [1596/60000 (3%)]\tLoss: 2193.043945 \n",
      "Train Epoch: 1 [1996/60000 (3%)]\tLoss: 2193.061035 \n",
      "Train Epoch: 1 [2396/60000 (4%)]\tLoss: 2193.016113 \n",
      "Train Epoch: 1 [2796/60000 (5%)]\tLoss: 2193.069824 \n",
      "Train Epoch: 1 [3196/60000 (5%)]\tLoss: 2192.997559 \n",
      "Train Epoch: 1 [3596/60000 (6%)]\tLoss: 2193.079346 \n",
      "Train Epoch: 1 [3996/60000 (7%)]\tLoss: 2193.056885 \n",
      "Train Epoch: 1 [4396/60000 (7%)]\tLoss: 2193.061279 \n",
      "Train Epoch: 1 [4796/60000 (8%)]\tLoss: 2193.034424 \n",
      "Train Epoch: 1 [5196/60000 (9%)]\tLoss: 2193.079102 \n",
      "Train Epoch: 1 [5596/60000 (9%)]\tLoss: 2193.038330 \n",
      "Train Epoch: 1 [5996/60000 (10%)]\tLoss: 2193.031982 \n",
      "Train Epoch: 1 [6396/60000 (11%)]\tLoss: 2193.011719 \n",
      "Train Epoch: 1 [6796/60000 (11%)]\tLoss: 2193.046143 \n",
      "Train Epoch: 1 [7196/60000 (12%)]\tLoss: 2193.062988 \n",
      "Train Epoch: 1 [7596/60000 (13%)]\tLoss: 2193.039795 \n",
      "Train Epoch: 1 [7996/60000 (13%)]\tLoss: 2193.051025 \n",
      "Train Epoch: 1 [8396/60000 (14%)]\tLoss: 2193.032471 \n",
      "Train Epoch: 1 [8796/60000 (15%)]\tLoss: 2193.075684 \n",
      "Train Epoch: 1 [9196/60000 (15%)]\tLoss: 2193.072021 \n",
      "Train Epoch: 1 [9596/60000 (16%)]\tLoss: 2193.036621 \n",
      "Train Epoch: 1 [9996/60000 (17%)]\tLoss: 2193.047607 \n",
      "Train Epoch: 1 [10396/60000 (17%)]\tLoss: 2193.052734 \n",
      "Train Epoch: 1 [10796/60000 (18%)]\tLoss: 2193.041260 \n",
      "Train Epoch: 1 [11196/60000 (19%)]\tLoss: 2193.041016 \n",
      "Train Epoch: 1 [11596/60000 (19%)]\tLoss: 2193.035645 \n",
      "Train Epoch: 1 [11996/60000 (20%)]\tLoss: 2193.076660 \n",
      "Train Epoch: 1 [12396/60000 (21%)]\tLoss: 2193.050293 \n",
      "Train Epoch: 1 [12796/60000 (21%)]\tLoss: 2193.035645 \n",
      "Train Epoch: 1 [13196/60000 (22%)]\tLoss: 2193.016846 \n",
      "Train Epoch: 1 [13596/60000 (23%)]\tLoss: 2193.063965 \n",
      "Train Epoch: 1 [13996/60000 (23%)]\tLoss: 2193.067383 \n",
      "Train Epoch: 1 [14396/60000 (24%)]\tLoss: 2193.043213 \n",
      "Train Epoch: 1 [14796/60000 (25%)]\tLoss: 2193.114502 \n",
      "Train Epoch: 1 [15196/60000 (25%)]\tLoss: 2193.095703 \n",
      "Train Epoch: 1 [15596/60000 (26%)]\tLoss: 2193.022949 \n",
      "Train Epoch: 1 [15996/60000 (27%)]\tLoss: 2193.028809 \n",
      "Train Epoch: 1 [16396/60000 (27%)]\tLoss: 2193.016113 \n",
      "Train Epoch: 1 [16796/60000 (28%)]\tLoss: 2193.042236 \n",
      "Train Epoch: 1 [17196/60000 (29%)]\tLoss: 2193.067139 \n",
      "Train Epoch: 1 [17596/60000 (29%)]\tLoss: 2193.026123 \n",
      "Train Epoch: 1 [17996/60000 (30%)]\tLoss: 2193.027588 \n",
      "Train Epoch: 1 [18396/60000 (31%)]\tLoss: 2193.054199 \n",
      "Train Epoch: 1 [18796/60000 (31%)]\tLoss: 2193.040039 \n",
      "Train Epoch: 1 [19196/60000 (32%)]\tLoss: 2193.067871 \n",
      "Train Epoch: 1 [19596/60000 (33%)]\tLoss: 2193.032471 \n",
      "Train Epoch: 1 [19996/60000 (33%)]\tLoss: 2193.053223 \n",
      "Train Epoch: 1 [20396/60000 (34%)]\tLoss: 2193.032227 \n",
      "Train Epoch: 1 [20796/60000 (35%)]\tLoss: 2193.130371 \n",
      "Train Epoch: 1 [21196/60000 (35%)]\tLoss: 2193.081787 \n",
      "Train Epoch: 1 [21596/60000 (36%)]\tLoss: 2192.983643 \n",
      "Train Epoch: 1 [21996/60000 (37%)]\tLoss: 2193.006836 \n",
      "Train Epoch: 1 [22396/60000 (37%)]\tLoss: 2193.072754 \n",
      "Train Epoch: 1 [22796/60000 (38%)]\tLoss: 2193.048828 \n",
      "Train Epoch: 1 [23196/60000 (39%)]\tLoss: 2193.057617 \n",
      "Train Epoch: 1 [23596/60000 (39%)]\tLoss: 2193.103027 \n",
      "Train Epoch: 1 [23996/60000 (40%)]\tLoss: 2193.043701 \n",
      "Train Epoch: 1 [24396/60000 (41%)]\tLoss: 2193.058594 \n",
      "Train Epoch: 1 [24796/60000 (41%)]\tLoss: 2193.023438 \n",
      "Train Epoch: 1 [25196/60000 (42%)]\tLoss: 2193.015137 \n",
      "Train Epoch: 1 [25596/60000 (43%)]\tLoss: 2193.064697 \n",
      "Train Epoch: 1 [25996/60000 (43%)]\tLoss: 2193.062988 \n",
      "Train Epoch: 1 [26396/60000 (44%)]\tLoss: 2193.009033 \n",
      "Train Epoch: 1 [26796/60000 (45%)]\tLoss: 2193.041504 \n",
      "Train Epoch: 1 [27196/60000 (45%)]\tLoss: 2193.036621 \n",
      "Train Epoch: 1 [27596/60000 (46%)]\tLoss: 2193.048584 \n",
      "Train Epoch: 1 [27996/60000 (47%)]\tLoss: 2193.013428 \n",
      "Train Epoch: 1 [28396/60000 (47%)]\tLoss: 2193.056396 \n",
      "Train Epoch: 1 [28796/60000 (48%)]\tLoss: 2193.094727 \n",
      "Train Epoch: 1 [29196/60000 (49%)]\tLoss: 2193.062012 \n",
      "Train Epoch: 1 [29596/60000 (49%)]\tLoss: 2193.027344 \n",
      "Train Epoch: 1 [29996/60000 (50%)]\tLoss: 2193.030029 \n",
      "Train Epoch: 1 [30396/60000 (51%)]\tLoss: 2193.003174 \n",
      "Train Epoch: 1 [30796/60000 (51%)]\tLoss: 2193.052490 \n",
      "Train Epoch: 1 [31196/60000 (52%)]\tLoss: 2193.091797 \n",
      "Train Epoch: 1 [31596/60000 (53%)]\tLoss: 2193.021240 \n",
      "Train Epoch: 1 [31996/60000 (53%)]\tLoss: 2193.104004 \n",
      "Train Epoch: 1 [32396/60000 (54%)]\tLoss: 2192.989746 \n",
      "Train Epoch: 1 [32796/60000 (55%)]\tLoss: 2193.008301 \n",
      "Train Epoch: 1 [33196/60000 (55%)]\tLoss: 2193.052979 \n",
      "Train Epoch: 1 [33596/60000 (56%)]\tLoss: 2192.972656 \n",
      "Train Epoch: 1 [33996/60000 (57%)]\tLoss: 2193.072998 \n",
      "Train Epoch: 1 [34396/60000 (57%)]\tLoss: 2193.063721 \n",
      "Train Epoch: 1 [34796/60000 (58%)]\tLoss: 2193.073486 \n",
      "Train Epoch: 1 [35196/60000 (59%)]\tLoss: 2193.081055 \n",
      "Train Epoch: 1 [35596/60000 (59%)]\tLoss: 2193.029053 \n",
      "Train Epoch: 1 [35996/60000 (60%)]\tLoss: 2193.080811 \n",
      "Train Epoch: 1 [36396/60000 (61%)]\tLoss: 2193.068115 \n",
      "Train Epoch: 1 [36796/60000 (61%)]\tLoss: 2193.041992 \n",
      "Train Epoch: 1 [37196/60000 (62%)]\tLoss: 2193.010254 \n",
      "Train Epoch: 1 [37596/60000 (63%)]\tLoss: 2193.085205 \n",
      "Train Epoch: 1 [37996/60000 (63%)]\tLoss: 2193.045654 \n",
      "Train Epoch: 1 [38396/60000 (64%)]\tLoss: 2193.025879 \n",
      "Train Epoch: 1 [38796/60000 (65%)]\tLoss: 2193.068359 \n",
      "Train Epoch: 1 [39196/60000 (65%)]\tLoss: 2193.028564 \n",
      "Train Epoch: 1 [39596/60000 (66%)]\tLoss: 2193.028564 \n",
      "Train Epoch: 1 [39996/60000 (67%)]\tLoss: 2193.087158 \n",
      "Train Epoch: 1 [40396/60000 (67%)]\tLoss: 2193.018799 \n",
      "Train Epoch: 1 [40796/60000 (68%)]\tLoss: 2193.016357 \n",
      "Train Epoch: 1 [41196/60000 (69%)]\tLoss: 2193.049072 \n",
      "Train Epoch: 1 [41596/60000 (69%)]\tLoss: 2193.036133 \n",
      "Train Epoch: 1 [41996/60000 (70%)]\tLoss: 2193.035400 \n",
      "Train Epoch: 1 [42396/60000 (71%)]\tLoss: 2193.040283 \n",
      "Train Epoch: 1 [42796/60000 (71%)]\tLoss: 2193.034424 \n",
      "Train Epoch: 1 [43196/60000 (72%)]\tLoss: 2193.027100 \n",
      "Train Epoch: 1 [43596/60000 (73%)]\tLoss: 2192.991211 \n",
      "Train Epoch: 1 [43996/60000 (73%)]\tLoss: 2193.000000 \n",
      "Train Epoch: 1 [44396/60000 (74%)]\tLoss: 2193.083008 \n",
      "Train Epoch: 1 [44796/60000 (75%)]\tLoss: 2193.095703 \n",
      "Train Epoch: 1 [45196/60000 (75%)]\tLoss: 2193.033691 \n",
      "Train Epoch: 1 [45596/60000 (76%)]\tLoss: 2193.068604 \n",
      "Train Epoch: 1 [45996/60000 (77%)]\tLoss: 2193.038086 \n",
      "Train Epoch: 1 [46396/60000 (77%)]\tLoss: 2193.052490 \n",
      "Train Epoch: 1 [46796/60000 (78%)]\tLoss: 2193.040527 \n",
      "Train Epoch: 1 [47196/60000 (79%)]\tLoss: 2193.023438 \n",
      "Train Epoch: 1 [47596/60000 (79%)]\tLoss: 2193.022461 \n",
      "Train Epoch: 1 [47996/60000 (80%)]\tLoss: 2193.033936 \n",
      "Train Epoch: 1 [48396/60000 (81%)]\tLoss: 2193.029785 \n",
      "Train Epoch: 1 [48796/60000 (81%)]\tLoss: 2193.116211 \n",
      "Train Epoch: 1 [49196/60000 (82%)]\tLoss: 2193.026611 \n",
      "Train Epoch: 1 [49596/60000 (83%)]\tLoss: 2193.110352 \n",
      "Train Epoch: 1 [49996/60000 (83%)]\tLoss: 2193.021484 \n",
      "Train Epoch: 1 [50396/60000 (84%)]\tLoss: 2193.026367 \n",
      "Train Epoch: 1 [50796/60000 (85%)]\tLoss: 2193.056641 \n",
      "Train Epoch: 1 [51196/60000 (85%)]\tLoss: 2193.062012 \n",
      "Train Epoch: 1 [51596/60000 (86%)]\tLoss: 2193.050049 \n",
      "Train Epoch: 1 [51996/60000 (87%)]\tLoss: 2193.056152 \n",
      "Train Epoch: 1 [52396/60000 (87%)]\tLoss: 2193.028076 \n",
      "Train Epoch: 1 [52796/60000 (88%)]\tLoss: 2193.077393 \n",
      "Train Epoch: 1 [53196/60000 (89%)]\tLoss: 2193.056641 \n",
      "Train Epoch: 1 [53596/60000 (89%)]\tLoss: 2193.040283 \n",
      "Train Epoch: 1 [53996/60000 (90%)]\tLoss: 2193.047119 \n",
      "Train Epoch: 1 [54396/60000 (91%)]\tLoss: 2193.016846 \n",
      "Train Epoch: 1 [54796/60000 (91%)]\tLoss: 2193.009277 \n",
      "Train Epoch: 1 [55196/60000 (92%)]\tLoss: 2193.037842 \n",
      "Train Epoch: 1 [55596/60000 (93%)]\tLoss: 2193.069824 \n",
      "Train Epoch: 1 [55996/60000 (93%)]\tLoss: 2193.029785 \n",
      "Train Epoch: 1 [56396/60000 (94%)]\tLoss: 2193.019775 \n",
      "Train Epoch: 1 [56796/60000 (95%)]\tLoss: 2193.023193 \n",
      "Train Epoch: 1 [57196/60000 (95%)]\tLoss: 2193.115723 \n",
      "Train Epoch: 1 [57596/60000 (96%)]\tLoss: 2193.076904 \n",
      "Train Epoch: 1 [57996/60000 (97%)]\tLoss: 2193.065674 \n",
      "Train Epoch: 1 [58396/60000 (97%)]\tLoss: 2193.013184 \n",
      "Train Epoch: 1 [58796/60000 (98%)]\tLoss: 2193.057617 \n",
      "Train Epoch: 1 [59196/60000 (99%)]\tLoss: 2193.071777 \n",
      "Train Epoch: 1 [59596/60000 (99%)]\tLoss: 2193.076172 \n",
      "Train Epoch: 1 [59996/60000 (100%)]\tLoss: 2193.081055 \n",
      "Train Epoch: 2 [396/60000 (1%)]\tLoss: 2193.007080 \n",
      "Train Epoch: 2 [796/60000 (1%)]\tLoss: 2193.081299 \n",
      "Train Epoch: 2 [1196/60000 (2%)]\tLoss: 2193.066650 \n",
      "Train Epoch: 2 [1596/60000 (3%)]\tLoss: 2193.035400 \n",
      "Train Epoch: 2 [1996/60000 (3%)]\tLoss: 2193.063721 \n",
      "Train Epoch: 2 [2396/60000 (4%)]\tLoss: 2193.069336 \n",
      "Train Epoch: 2 [2796/60000 (5%)]\tLoss: 2193.058105 \n",
      "Train Epoch: 2 [3196/60000 (5%)]\tLoss: 2193.006836 \n",
      "Train Epoch: 2 [3596/60000 (6%)]\tLoss: 2193.120361 \n",
      "Train Epoch: 2 [3996/60000 (7%)]\tLoss: 2193.080078 \n",
      "Train Epoch: 2 [4396/60000 (7%)]\tLoss: 2193.091309 \n",
      "Train Epoch: 2 [4796/60000 (8%)]\tLoss: 2193.085693 \n",
      "Train Epoch: 2 [5196/60000 (9%)]\tLoss: 2193.089600 \n",
      "Train Epoch: 2 [5596/60000 (9%)]\tLoss: 2193.049561 \n",
      "Train Epoch: 2 [5996/60000 (10%)]\tLoss: 2193.067627 \n",
      "Train Epoch: 2 [6396/60000 (11%)]\tLoss: 2193.057861 \n",
      "Train Epoch: 2 [6796/60000 (11%)]\tLoss: 2192.981689 \n",
      "Train Epoch: 2 [7196/60000 (12%)]\tLoss: 2193.056396 \n",
      "Train Epoch: 2 [7596/60000 (13%)]\tLoss: 2193.066650 \n",
      "Train Epoch: 2 [7996/60000 (13%)]\tLoss: 2193.046875 \n",
      "Train Epoch: 2 [8396/60000 (14%)]\tLoss: 2193.002930 \n",
      "Train Epoch: 2 [8796/60000 (15%)]\tLoss: 2193.049561 \n",
      "Train Epoch: 2 [9196/60000 (15%)]\tLoss: 2193.090088 \n",
      "Train Epoch: 2 [9596/60000 (16%)]\tLoss: 2193.050537 \n",
      "Train Epoch: 2 [9996/60000 (17%)]\tLoss: 2193.037842 \n",
      "Train Epoch: 2 [10396/60000 (17%)]\tLoss: 2193.092529 \n",
      "Train Epoch: 2 [10796/60000 (18%)]\tLoss: 2193.058594 \n",
      "Train Epoch: 2 [11196/60000 (19%)]\tLoss: 2193.039551 \n",
      "Train Epoch: 2 [11596/60000 (19%)]\tLoss: 2193.101807 \n",
      "Train Epoch: 2 [11996/60000 (20%)]\tLoss: 2193.078857 \n",
      "Train Epoch: 2 [12396/60000 (21%)]\tLoss: 2193.098877 \n",
      "Train Epoch: 2 [12796/60000 (21%)]\tLoss: 2193.040283 \n",
      "Train Epoch: 2 [13196/60000 (22%)]\tLoss: 2193.083740 \n",
      "Train Epoch: 2 [13596/60000 (23%)]\tLoss: 2193.038574 \n",
      "Train Epoch: 2 [13996/60000 (23%)]\tLoss: 2193.083252 \n",
      "Train Epoch: 2 [14396/60000 (24%)]\tLoss: 2193.074707 \n",
      "Train Epoch: 2 [14796/60000 (25%)]\tLoss: 2193.054688 \n",
      "Train Epoch: 2 [15196/60000 (25%)]\tLoss: 2193.025391 \n",
      "Train Epoch: 2 [15596/60000 (26%)]\tLoss: 2193.066650 \n",
      "Train Epoch: 2 [15996/60000 (27%)]\tLoss: 2193.064941 \n",
      "Train Epoch: 2 [16396/60000 (27%)]\tLoss: 2193.036133 \n",
      "Train Epoch: 2 [16796/60000 (28%)]\tLoss: 2193.029053 \n",
      "Train Epoch: 2 [17196/60000 (29%)]\tLoss: 2193.036377 \n",
      "Train Epoch: 2 [17596/60000 (29%)]\tLoss: 2193.048340 \n",
      "Train Epoch: 2 [17996/60000 (30%)]\tLoss: 2193.061035 \n",
      "Train Epoch: 2 [18396/60000 (31%)]\tLoss: 2192.979004 \n",
      "Train Epoch: 2 [18796/60000 (31%)]\tLoss: 2193.052490 \n",
      "Train Epoch: 2 [19196/60000 (32%)]\tLoss: 2193.074951 \n",
      "Train Epoch: 2 [19596/60000 (33%)]\tLoss: 2193.064697 \n",
      "Train Epoch: 2 [19996/60000 (33%)]\tLoss: 2193.071777 \n",
      "Train Epoch: 2 [20396/60000 (34%)]\tLoss: 2193.090576 \n",
      "Train Epoch: 2 [20796/60000 (35%)]\tLoss: 2193.005127 \n",
      "Train Epoch: 2 [21196/60000 (35%)]\tLoss: 2193.065674 \n",
      "Train Epoch: 2 [21596/60000 (36%)]\tLoss: 2193.062500 \n",
      "Train Epoch: 2 [21996/60000 (37%)]\tLoss: 2193.016357 \n",
      "Train Epoch: 2 [22396/60000 (37%)]\tLoss: 2193.101074 \n",
      "Train Epoch: 2 [22796/60000 (38%)]\tLoss: 2193.040283 \n",
      "Train Epoch: 2 [23196/60000 (39%)]\tLoss: 2193.062256 \n",
      "Train Epoch: 2 [23596/60000 (39%)]\tLoss: 2193.073486 \n",
      "Train Epoch: 2 [23996/60000 (40%)]\tLoss: 2193.120117 \n",
      "Train Epoch: 2 [24396/60000 (41%)]\tLoss: 2193.015625 \n",
      "Train Epoch: 2 [24796/60000 (41%)]\tLoss: 2193.004639 \n",
      "Train Epoch: 2 [25196/60000 (42%)]\tLoss: 2193.039551 \n",
      "Train Epoch: 2 [25596/60000 (43%)]\tLoss: 2193.060059 \n",
      "Train Epoch: 2 [25996/60000 (43%)]\tLoss: 2193.040283 \n",
      "Train Epoch: 2 [26396/60000 (44%)]\tLoss: 2193.075684 \n",
      "Train Epoch: 2 [26796/60000 (45%)]\tLoss: 2193.072998 \n",
      "Train Epoch: 2 [27196/60000 (45%)]\tLoss: 2193.088623 \n",
      "Train Epoch: 2 [27596/60000 (46%)]\tLoss: 2193.054199 \n",
      "Train Epoch: 2 [27996/60000 (47%)]\tLoss: 2193.084961 \n",
      "Train Epoch: 2 [28396/60000 (47%)]\tLoss: 2193.053467 \n",
      "Train Epoch: 2 [28796/60000 (48%)]\tLoss: 2193.031006 \n",
      "Train Epoch: 2 [29196/60000 (49%)]\tLoss: 2193.040283 \n",
      "Train Epoch: 2 [29596/60000 (49%)]\tLoss: 2193.057373 \n",
      "Train Epoch: 2 [29996/60000 (50%)]\tLoss: 2193.012207 \n",
      "Train Epoch: 2 [30396/60000 (51%)]\tLoss: 2193.039062 \n",
      "Train Epoch: 2 [30796/60000 (51%)]\tLoss: 2193.119385 \n",
      "Train Epoch: 2 [31196/60000 (52%)]\tLoss: 2193.062744 \n",
      "Train Epoch: 2 [31596/60000 (53%)]\tLoss: 2193.095215 \n",
      "Train Epoch: 2 [31996/60000 (53%)]\tLoss: 2193.039795 \n",
      "Train Epoch: 2 [32396/60000 (54%)]\tLoss: 2193.051270 \n",
      "Train Epoch: 2 [32796/60000 (55%)]\tLoss: 2193.075928 \n",
      "Train Epoch: 2 [33196/60000 (55%)]\tLoss: 2193.064453 \n",
      "Train Epoch: 2 [33596/60000 (56%)]\tLoss: 2193.055420 \n",
      "Train Epoch: 2 [33996/60000 (57%)]\tLoss: 2193.037109 \n",
      "Train Epoch: 2 [34396/60000 (57%)]\tLoss: 2193.078369 \n",
      "Train Epoch: 2 [34796/60000 (58%)]\tLoss: 2193.101074 \n",
      "Train Epoch: 2 [35196/60000 (59%)]\tLoss: 2193.077148 \n",
      "Train Epoch: 2 [35596/60000 (59%)]\tLoss: 2193.006104 \n",
      "Train Epoch: 2 [35996/60000 (60%)]\tLoss: 2193.076172 \n",
      "Train Epoch: 2 [36396/60000 (61%)]\tLoss: 2193.073486 \n",
      "Train Epoch: 2 [36796/60000 (61%)]\tLoss: 2193.039795 \n",
      "Train Epoch: 2 [37196/60000 (62%)]\tLoss: 2193.018066 \n",
      "Train Epoch: 2 [37596/60000 (63%)]\tLoss: 2193.085693 \n",
      "Train Epoch: 2 [37996/60000 (63%)]\tLoss: 2193.036377 \n",
      "Train Epoch: 2 [38396/60000 (64%)]\tLoss: 2193.073975 \n",
      "Train Epoch: 2 [38796/60000 (65%)]\tLoss: 2193.035645 \n",
      "Train Epoch: 2 [39196/60000 (65%)]\tLoss: 2193.052979 \n",
      "Train Epoch: 2 [39596/60000 (66%)]\tLoss: 2193.092285 \n",
      "Train Epoch: 2 [39996/60000 (67%)]\tLoss: 2193.042480 \n",
      "Train Epoch: 2 [40396/60000 (67%)]\tLoss: 2193.057861 \n",
      "Train Epoch: 2 [40796/60000 (68%)]\tLoss: 2193.021240 \n",
      "Train Epoch: 2 [41196/60000 (69%)]\tLoss: 2193.059326 \n",
      "Train Epoch: 2 [41596/60000 (69%)]\tLoss: 2193.028809 \n",
      "Train Epoch: 2 [41996/60000 (70%)]\tLoss: 2193.071289 \n",
      "Train Epoch: 2 [42396/60000 (71%)]\tLoss: 2193.034668 \n",
      "Train Epoch: 2 [42796/60000 (71%)]\tLoss: 2193.060547 \n",
      "Train Epoch: 2 [43196/60000 (72%)]\tLoss: 2193.037354 \n",
      "Train Epoch: 2 [43596/60000 (73%)]\tLoss: 2193.022949 \n",
      "Train Epoch: 2 [43996/60000 (73%)]\tLoss: 2193.027344 \n",
      "Train Epoch: 2 [44396/60000 (74%)]\tLoss: 2193.039795 \n",
      "Train Epoch: 2 [44796/60000 (75%)]\tLoss: 2193.035400 \n",
      "Train Epoch: 2 [45196/60000 (75%)]\tLoss: 2193.035645 \n",
      "Train Epoch: 2 [45596/60000 (76%)]\tLoss: 2193.056641 \n",
      "Train Epoch: 2 [45996/60000 (77%)]\tLoss: 2193.020996 \n",
      "Train Epoch: 2 [46396/60000 (77%)]\tLoss: 2193.080566 \n",
      "Train Epoch: 2 [46796/60000 (78%)]\tLoss: 2193.019287 \n",
      "Train Epoch: 2 [47196/60000 (79%)]\tLoss: 2193.059082 \n",
      "Train Epoch: 2 [47596/60000 (79%)]\tLoss: 2193.006592 \n",
      "Train Epoch: 2 [47996/60000 (80%)]\tLoss: 2193.005127 \n",
      "Train Epoch: 2 [48396/60000 (81%)]\tLoss: 2193.056152 \n",
      "Train Epoch: 2 [48796/60000 (81%)]\tLoss: 2193.045410 \n",
      "Train Epoch: 2 [49196/60000 (82%)]\tLoss: 2193.018066 \n",
      "Train Epoch: 2 [49596/60000 (83%)]\tLoss: 2193.020752 \n",
      "Train Epoch: 2 [49996/60000 (83%)]\tLoss: 2193.037109 \n",
      "Train Epoch: 2 [50396/60000 (84%)]\tLoss: 2193.038086 \n",
      "Train Epoch: 2 [50796/60000 (85%)]\tLoss: 2193.095459 \n",
      "Train Epoch: 2 [51196/60000 (85%)]\tLoss: 2193.033936 \n",
      "Train Epoch: 2 [51596/60000 (86%)]\tLoss: 2193.034912 \n",
      "Train Epoch: 2 [51996/60000 (87%)]\tLoss: 2193.042480 \n",
      "Train Epoch: 2 [52396/60000 (87%)]\tLoss: 2193.111816 \n",
      "Train Epoch: 2 [52796/60000 (88%)]\tLoss: 2193.071045 \n",
      "Train Epoch: 2 [53196/60000 (89%)]\tLoss: 2193.036621 \n",
      "Train Epoch: 2 [53596/60000 (89%)]\tLoss: 2193.042236 \n",
      "Train Epoch: 2 [53996/60000 (90%)]\tLoss: 2193.101562 \n",
      "Train Epoch: 2 [54396/60000 (91%)]\tLoss: 2192.989258 \n",
      "Train Epoch: 2 [54796/60000 (91%)]\tLoss: 2193.058594 \n",
      "Train Epoch: 2 [55196/60000 (92%)]\tLoss: 2193.090820 \n",
      "Train Epoch: 2 [55596/60000 (93%)]\tLoss: 2193.057617 \n",
      "Train Epoch: 2 [55996/60000 (93%)]\tLoss: 2193.040527 \n",
      "Train Epoch: 2 [56396/60000 (94%)]\tLoss: 2193.100830 \n",
      "Train Epoch: 2 [56796/60000 (95%)]\tLoss: 2193.018311 \n",
      "Train Epoch: 2 [57196/60000 (95%)]\tLoss: 2193.073975 \n",
      "Train Epoch: 2 [57596/60000 (96%)]\tLoss: 2193.063721 \n",
      "Train Epoch: 2 [57996/60000 (97%)]\tLoss: 2193.036133 \n",
      "Train Epoch: 2 [58396/60000 (97%)]\tLoss: 2193.071533 \n",
      "Train Epoch: 2 [58796/60000 (98%)]\tLoss: 2193.061035 \n",
      "Train Epoch: 2 [59196/60000 (99%)]\tLoss: 2193.051270 \n",
      "Train Epoch: 2 [59596/60000 (99%)]\tLoss: 2193.054932 \n",
      "Train Epoch: 2 [59996/60000 (100%)]\tLoss: 2193.071777 \n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "model.train()\n",
    "step = 0 \n",
    "loss_history = []\n",
    "for epoch in range(1, 3):\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_0 = criterion(output, target)\n",
    "        loss_1 = call_pr_loss(list(model.l1.weight) , m_784)\n",
    "        loss_2 = call_pr_loss(list(model.l2.weight) , m_200)\n",
    "        loss_3 = call_pr_loss(list(model.l3.weight) , m_200)\n",
    "        loss_4 = call_pr_loss(list(model.l4.weight) , m_200)\n",
    "        loss_5 = call_pr_loss(list(model.l5.weight) , m_200)\n",
    "        loss_6 = call_pr_loss(list(model.l6.weight) , m_200)\n",
    "        loss_7 = call_pr_loss(list(model.l7.weight) , m_200)\n",
    "        loss_8 = call_pr_loss(list(model.l8.weight) , m_200)\n",
    "        loss_f = call_pr_loss(list(model.lf.weight) , m_200)\n",
    "        \n",
    "        \n",
    "        loss = loss_0 + loss_1+ loss_2+ loss_3 + loss_4 + loss_f + loss_5 + loss_6 + loss_7 + loss_8\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        loss_history.append(loss.item())\n",
    "        if step % 100  == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} '.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.item()))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5753, Accuracy: 6742/60000 (11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in trainloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(trainloader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(trainloader.dataset),\n",
    "    100. * correct / len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
