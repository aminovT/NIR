{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX separate import of libraries from actual code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score \n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX use separate cells to define functions\n",
    "def load_cancer_data(): \n",
    "  cancer = load_breast_cancer()     \n",
    "  data = np.c_[cancer.data, cancer.target]\n",
    "  columns = np.append(cancer.feature_names, [\"target\"])\n",
    "  return pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = load_cancer_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX use meaningfull names e.g. validation_data, training_data instead of delayed_data (отложенные данные?) and all_data\n",
    "validation_data = all_data[int(0.8 * len(all_data)):]\n",
    "training_data = all_data[:int(0.8 * len(all_data))]\n",
    "\n",
    "# FIXIT bad assumption that targets are always at the end (better specify target column name and \n",
    "# split the data according to the name of this variable)\n",
    "training_labels = training_data[training_data.columns[-1]].values\n",
    "training_feature_matrix = training_data[training_data.columns[:-1]].values\n",
    "\n",
    "validation_labels = validation_data[validation_data.columns[-1]].values\n",
    "validation_feature_matrix = validation_data[validation_data.columns[:-1]].values\n",
    "\n",
    "\n",
    "train_feature_matrix, test_feature_matrix, train_labels, test_labels = train_test_split(\n",
    "    training_feature_matrix, training_labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 22 candidates, totalling 110 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1892s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1344s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done  36 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=4)]: Done  62 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=4)]: Done 110 out of 110 | elapsed:    7.9s finished\n",
      "C:\\Users\\a.zaytsev\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=4,\n",
       "       param_grid=[{'penalty': ['l1', 'l2'], 'C': array([6.73795e-03, 1.83156e-02, 4.97871e-02, 1.35335e-01, 3.67879e-01,\n",
       "       1.00000e+00, 2.71828e+00, 7.38906e+00, 2.00855e+01, 5.45982e+01,\n",
       "       1.48413e+02])}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp = np.exp(np.linspace(-5, 5, 11))\n",
    "\n",
    "logistic_regression_classifier = LogisticRegression(max_iter=1000)\n",
    "param_grid = [{'penalty': ['l1', 'l2'],\n",
    "               'C': lp       \n",
    "              }]\n",
    "\n",
    "searcher = GridSearchCV(logistic_regression_classifier, param_grid, cv=5,  verbose=10, n_jobs = 4)\n",
    "searcher.fit(training_feature_matrix, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9714285714285714"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 20.085536923187668, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98        65\n",
      "         1.0       0.99      0.99      0.99        86\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       151\n",
      "   macro avg       0.99      0.99      0.99       151\n",
      "weighted avg       0.99      0.99      0.99       151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predicted_labels = searcher.best_estimator_.predict(test_feature_matrix)\n",
    "print(classification_report(test_labels, test_predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.96      0.85        26\n",
      "         1.0       0.99      0.91      0.95        88\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       114\n",
      "   macro avg       0.87      0.94      0.90       114\n",
      "weighted avg       0.94      0.92      0.92       114\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.zaytsev\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "classifier.fit(train_feature_matrix, train_labels)\n",
    "validation_predicted_labels = classifier.predict(validation_feature_matrix)\n",
    "print(classification_report(validation_labels , validation_predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = feature_matrix.reshape(-1, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8730, 1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([291, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "from torch.optim import Adam , Adadelta, SGD\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.zaytsev\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train = training_feature_matrix.reshape(-1, 1).astype('float32')\n",
    "y_train = training_labels.reshape(-1, 1).astype('float32')\n",
    "\n",
    "tensor_x = torch.stack([torch.Tensor(i) for i in training_feature_matrix.astype('float64')])\n",
    "# FIXIT if you use one hot encoding, you should pass labels on One-Hot-Encoding format\n",
    "tensor_y = torch.stack([torch.Tensor(i) for i in \n",
    "                        OneHotEncoder(sparse=False).fit_transform((training_labels).reshape(-1, 1)).astype('float32')]) \n",
    "\n",
    "my_dataset = utils.TensorDataset(tensor_x, tensor_y) \n",
    "batch_size = 50\n",
    "my_dataloader = utils.DataLoader(my_dataset, batch_size=batch_size,\n",
    "                                 shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        # out = nn.Sigmoid()(out) # avoid this step, we need only \n",
    "        return out\n",
    "\n",
    "# FIXIT why do we need this function?\n",
    "def get_param_values():\n",
    "    return w.data[0][0], b.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(30, 2)\n",
    "criterion = nn.CrossEntropyLoss() # This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class!\n",
    "# categorical cross entropy\n",
    "optimizer = Adam(model.parameters(), 0.01)\n",
    "\n",
    "[w, b] = model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 0 = 64.1955135345459\n",
      "loss on epoch 1 = 31.68964672088623\n",
      "loss on epoch 2 = 15.890056228637695\n",
      "loss on epoch 3 = 9.68368957042694\n",
      "loss on epoch 4 = 4.351440572738648\n",
      "loss on epoch 5 = 1.991914439201355\n",
      "loss on epoch 6 = 1.1975868850946427\n",
      "loss on epoch 7 = 1.119466695189476\n",
      "loss on epoch 8 = 0.9694336377084255\n",
      "loss on epoch 9 = 1.0040420204401017\n",
      "loss on epoch 10 = 0.815358966588974\n",
      "loss on epoch 11 = 0.9001766711473465\n",
      "loss on epoch 12 = 0.6263689577579499\n",
      "loss on epoch 13 = 0.5624534502625466\n",
      "loss on epoch 14 = 0.5720000777393579\n",
      "loss on epoch 15 = 0.4342592859175056\n",
      "loss on epoch 16 = 0.4412039041519165\n",
      "loss on epoch 17 = 0.5433797866106034\n",
      "loss on epoch 18 = 0.3401086062192917\n",
      "loss on epoch 19 = 0.305942979734391\n",
      "loss on epoch 20 = 0.27461996615165846\n",
      "loss on epoch 21 = 0.23795140013098717\n",
      "loss on epoch 22 = 0.26984078623354435\n",
      "loss on epoch 23 = 0.27948376489803195\n",
      "loss on epoch 24 = 0.21755223721265793\n",
      "loss on epoch 25 = 0.15225974165368825\n",
      "loss on epoch 26 = 0.21412945315241813\n",
      "loss on epoch 27 = 0.3198778659105301\n",
      "loss on epoch 28 = 0.31502505391836166\n",
      "loss on epoch 29 = 0.3033848971128464\n"
     ]
    }
   ],
   "source": [
    "epoch_number = 30\n",
    "for epoch in range(epoch_number):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    batch_number = 0\n",
    "    for batch_id, batch_sample in enumerate(my_dataloader):       \n",
    "        #print(x[0].shape)\n",
    "        #print(x[1])\n",
    "        optimizer.zero_grad()\n",
    "        points = batch_sample[0]\n",
    "        \n",
    "        predicted_probabilities = model(points)\n",
    "        #print(trans.shape)\n",
    "        #print(x[1].shape)\n",
    "        loss = criterion(predicted_probabilities, torch.max(batch_sample[1], 1)[1])\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        batch_number += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"loss on epoch {} = {}\".format(epoch, epoch_loss / batch_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(train_feature_matrix, train_labels)\n",
    "y_ans = clf.predict(delayed_feature_matrix)\n",
    "print(classification_report(delayed_labels , y_ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_delayed = torch.stack([torch.Tensor(i) for i in validation_feature_matrix.astype('float64')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([114, 30])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_delayed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_results = model(x_delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_labels = torch.max(torch_results, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_scoretorch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
